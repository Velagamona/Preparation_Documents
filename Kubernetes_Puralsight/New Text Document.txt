Course Overview
Course Overview
(Music) Hi everyone. My name is Anthony Nocentino, enterprise architect and founder of Centino Systems. Welcome to my course, Kubernetes Installation and Configuration Fundamentals. Are you a systems administrator that needs to build and administer a Kubernetes cluster? If you do, then this is the course for you. First, we'll start off by introducing you to the Kubernetes and the world of deploying container‑based applications. We'll look closely at Kubernetes architecture, including cluster components and networking fundamentals. Then we'll look at how to build your own Kubernetes cluster, and we'll look at installation considerations, such as deploying on premises or deploying in the cloud. Together, we'll deploy clusters in both scenarios in some demonstrations. And we'll wrap it up with learning how we can interact with our cluster at the command line using kubectl and the basics of application and service deployments in Kubernetes. At the end of this course, you'll know how to build a Kubernetes cluster, both in local virtual machines and as a managed service with some of the major cloud providers. You'll learn what it takes to get an application up and running in your Kubernetes cluster. Before beginning this course, you should be familiar with the Linux operating system and administering it at the command line. You should have a firm understanding of TCP/IP‑based networking and also understand the fundamental concepts of containers. I hope you'll join me on this journey to learn how to build a Kubernetes cluster and deploying container‑based applications in Kubernetes in the course, Kubernetes Installation and Configuration Fundamentals.

Exploring the Kubernetes Architecture
Introduction, Course, and Module Overview
Hello, this is Anthony Nocentino with Centino Systems. Welcome to my course, Kubernetes Installation and Configuration Fundamentals. This module is the Introduction and Exploring Kubernetes Architecture module. In this course, we're going to get you started on your journey into building and configuring your own Kubernetes cluster, whether it's on‑prem or in the cloud. So let's start off with a course overview. We're going to kick it off with an introduction, and then we're going to look at the theory and the concepts behind Kubernetes that makes, well, Kubernetes, Kubernetes in the module, Exploring Kubernetes Architecture. Once we have that theory in our brains, we're going to move on into installing and configuring Kubernetes. We're going to look at both on‑prem and cloud scenarios, doing a deep dive into how that really all pieces together in building our first cluster together. And then once we have that cluster up and running, I don't want to leave you hanging. We're going to go ahead and learn how to work with our Kubernetes cluster and do some basic operations, like deploying a service and an application into our newly built Kubernetes cluster. All right, In this module, we're going to kick it off with the discussion to answer the question, what is Kubernetes? And once we know that, then we're going to move on to exploring the Kubernetes architecture. We're going to zoom in very closely on the cluster's components and also some basic networking fundamentals.

What Is Kubernetes? Kubernetes Benefits and Operating Principles
Let's start our discussion with answering the question, what is Kubernetes? At its core, Kubernetes is a container orchestrator. What this means is it's Kubernetes job to start and stop container‑based applications based on the requirements of the systems administrator or developer. Now, one of the key facets of Kubernetes is workload placement. If I need to deploy a container‑based application into a cluster, how do I deploy it? On which servers does it physically live on? Does it need to be co‑resident with other services or containers inside of the cluster? If that's the case, we can define that in our code that describes our application, and Kubernetes can manage that for us. Now, speaking of Kubernetes managing things for us, Kubernetes also provides an infrastructure abstraction. As a developer, if I need to deploy an application into production, I really don't want to have to care about which server it lives on or have to go configure a load balancer to send traffic to my application. That's all going to be handled for me under the hood by Kubernetes. One of the other core ideas behind Kubernetes is this concept called desired state. We can define what our applications or our services look like in code, and it's the job of Kubernetes to make sure that our system meets that defined desired state. And so perhaps our application or our system is composed of some collection of web application containers and database containers, maybe some middleware or even a caching tier. We can write the code that describes what our system looks like, hand it off to Kubernetes for deployment, and it's Kubernetes job to make that happen, to make sure that our system is in that defined desired state. Let's look at some of the key benefits of using Kubernetes, and first off is speed of deployment. Kubernetes gives us the ability to deploy container‑based applications very, very quickly. And what this enables us actually to do is to get code from a developer's workstation into production fast, and that gives us the ability to absorb change quickly. The speed of deployment really allows you to iterate quickly and get new versions of code out, enabling new key capabilities for your organization's applications. Next up is Kubernetes' ability to recover very quickly. When we define our system and code and we define that desired state, so perhaps a collection of web app containers, and something causes our system to no longer be in that desired state, so perhaps a container crashed or a server failed, well, it's Kubernetes' job to ensure that our application comes back to that defined desired state by deploying new containers supporting our applications, making sure that we have a collection of web app containers up or really whatever resource it is that defines our application or our system's desired state. And then finally, Kubernetes allows us to hide infrastructure complexity in the cluster. And so things like storage, networking configuration, and workload placement are core functions of Kubernetes, and developers don't have to worry about these things when deploying container‑based applications into Kubernetes. So , and now that we know what Kubernetes is and the benefits of Kubernetes, let's look at some of the basic operating principles behind Kubernetes. And first up is desired state or declarative configuration. This is where we define our application, or really, our deployment's state and code. We define what we want to deploy, and Kubernetes makes that happen for us. It'll go and pull the specified container images and starts them up and possibly even allocates load balancers and public IPs if that's what's defined in our deployment's code. We write the code describing the deployment. Kubernetes does the work to bring it online in the desired state. Next, controllers or control loops have the responsibility of constantly monitoring the running state of the system to make sure that the system is in that desired state. And if it's not, a controller will try to bring the system into the desired state. And so, for example, if we've defined that we want three web app containers online, it's Kubernetes job, more specifically, a controller's job, to ensure that three web app containers are online. A controller will start the three replicas up, and later, if one of those fails or goes offline, a controller will create a new web app container, replacing the one that failed. Now there are many different types of controllers available in Kubernetes for various scenarios, and we'll cover them throughout this series of courses. But the key concept here is controllers are what make changes to the system to ensure the desired state. Another core principle is the Kubernetes API. The Kubernetes API provides a collection of objects that we can use to build and define the systems that we want to deploy in code. The objects defined in our code define the desired state of our applications or the systems that we want to deploy in Kubernetes. The Kubernetes' API is implemented and available via the API Server. The API Server is the central communication hub for information in a Kubernetes cluster. This is where we, as administrators and developers, interact with Kubernetes to deploy and manage workloads. And that's also where the components of a Kubernetes cluster interact with each other to understand the current state of the system and to make changes to that state, if needed to ensure the desired state.

Introducing the Kubernetes API - Objects and API Server
So let's look more closely at the Kubernetes API. Behind the scenes, you'll find they're a collection of API objects. These objects are primitives that represent the state of the system, so things like Pods, which represent a deployed container, or nodes, which represent servers that do work in your system. These API objects allow us to configure the state of the system, and we could do that, both declaratively and imperatively, where declaratively, we learned, means we can describe the implementation that we want to have, or we can describe that deployment and how we want our system to look. Imperatively means we sat down at the command line and executed a sequence of commands to get the system to be in the state that we want it to be in. So as we continue along learning about the API server, the API server is a RESTful API that runs over HTTP or HTTPS using JSON. And this is going to be the sole way that we as administrators interact with our cluster, but it's also the only way that Kubernetes interacts with the cluster as well. There are other facets of the system that will be exchanging information, and all of that has to go through the API server. Now when we're communicating to the API server and we're telling it to create things or to change objects into different configuration states, that information is serialized and then persisted into the cluster data store. Now let's look at some of those core primitives, the Kubernetes API objects. These are going to be the building blocks for our deployments inside of Kubernetes. And first up's Pods. What Pods are, are a single or a collection of containers that we deploy as a single unit. This essentially is our container‑based application. Then we have controllers. These are the things that keep our system in the desired state. So things like ReplicaSets and deployments are going to become core tools in our toolbox as we build deployments in Kubernetes. Now, services provide a persistent access point to the applications that we deploy in Pods because, as things change under the hood and our Pods get redeployed perhaps by our controllers as they come up and down, those things will be constantly changing. Well, it's the service's responsibility to provide a persistent access point to the applications provided by our Pods. and finally, there's storage. Of course, we'll need to be able to store data somewhere, and Kubernetes gives us some storage objects so we can have persistent storage in our applications. Now this certainly is not an exhaustive list of all the API objects available in Kubernetes, but these are the key players, the core things that we'll build our deployments from.

Understanding API Objects - Pods
So let's look a little closer at each of these core API objects that we just introduced, and we're going to start the conversation with Pods. What a Pod is is a construct that represents one or more containers inside of a Kubernetes cluster. What it really is is those container‑based applications or services that you need to deploy into your Kubernetes cluster or your production environment. From a Kubernetes standpoint, it's the most basic unit of work, in fact, it's the unit of scheduling. When we define a Pod, we also can define some of the resources that it requires inside of the manifest that describes our deployment. It's up to Kubernetes to ensure that those resources are available, and schedules that Pods onto those resources in our cluster, and brings our application up and running. Another key attribute of Pods is that they're ephemeral. What this means is that no Pod is ever redeployed. If I deploy an application as a Pod based off of a container today, and that Pod dies, if I go ahead and redeploy that Pod again based off of that same container image, no state is maintained between those two deployments. Once that Pod goes away, it never ever comes back. That new Pod is going to be deployed and provide those services of that container‑based application in that Pod deployment. Another key attribute of Pods is the atomicity of a Pod, they're either there or they're not. Now, in a single‑container Pod, this kind of makes sense, like a Pod's up and running, if the container dies, well, yeah, that Pod's no longer available. In a multi‑container Pod, if I have more than one container deployed inside of a Pod, if one of those containers dies, the entire Pod becomes unavailable, and that Pod is no longer providing the services that it should. Continuing the conversation about Pods, what I want to introduce now is that it's Kubernetes' job to keep your Pods up and running using controllers, but more specifically, it's about keeping your application or your deployments in the desired state. And so to do that, to make sure that things are in the desired state, Kubernetes tracks the state of a Pod. Is our Pod and the containers inside of that Pod up and running? Now of course our Pod could be up and running, but the applications inside of that Pod could be throwing errors, so Kubernetes also tracks the health of a Pod. Is the application inside the Pod up and running? And so Kubernetes can check the health of an application running inside of a Pod with probes. In our deployment code, we can define a probe to check the health of our applications. And so, for example, if we've deployed a web application, we could write a probe that checks the URL and tests to see if the application is responding in an appropriate way. And so if that probe fails, then Kubernetes could understand that and make the appropriate adjustments to that individual Pod that might not be responding in a healthy way, and this could be deleting the Pod and replacing it with a new one. We're going to look much more closely at Pod internals and probes in an upcoming course, but for now, what I want you to understand is what they are and the role that they play inside of deploying applications in Kubernetes. We're going to look at them very closely later on.

Understanding API Objects - Controllers
So now that we know what Pods are and that they can come and go based on status and health, how does Kubernetes manage my Pod state? What's the thing that makes sure that our Pod is up, running, and healthy? Well, that's where controllers come in. A controller defines the desired state for your cluster and the applications deployed in it. It's the job of a controller to ensure that things stay in the desired state. Controllers are exposed to you as workload resource API objects and are most commonly used to create and manage Pod‑based applications deployed in Kubernetes. These API objects will create and configure the controllers needed to create and manage your Pods for you. And they ensure your applications stay in the desired state. So, for controllers to define and manage the desired state of applications deployed as Pods, controllers monitor and respond to the state and health of Pods deployed, ensuring the desired number of Pods are up, running, and healthy in a cluster. And if the state or the health changes, the controller will respond accordingly, trying to make sure that the system stays in the desired state. The first controller that we're going to look at today is the ReplicaSet. You've heard me describe this scenario a couple of times so far in the course where I want to have a collection of web application containers up and running in Pods. Well, that's what a replica set models. It allows us to define the number of replicas for a particular Pod that we want to have up and running at all times. And so if I want to have three Pods of this particular web app running at all times, well, it's the replica set's job to make sure that those three Pods are up and running at all times. If one of those Pods becomes unavailable or unhealthy, for whatever reason, it's Kubernetes' job to delete that failed Pod and deploy a new Pod with the hopes of that new Pod returning the system back to the desired healthy state. When it comes to deploying applications in Kubernetes, you generally won't create replica sets directly, you will create deployments. On the creation of a deployment, it creates a replica set based on what's defined in the deployment. And the deployment manages the state of the replica set, so things like which container image to run and also the number of Pods to create. But more interestingly, the deployment controller manages the transition between two replica sets, and a practical example of this is moving between two versions of an application. When we define our initial deployment, it creates a replica set, let's say running version 1.1 of a container image for our application. We can use a deployment to control the transition to a new collection of Pods, perhaps running version 1.2 of our application. A deployment controller controls the transition and even gives us the ability to roll back if needed. Deployments are core to the success of deploying applications in Kubernetes, and we'll look at them in more detail in this course and in much more detail in upcoming courses. Now, there are many more types of controllers available in Kubernetes, not just ones based on Pods. In future courses, as we continue along in our Kubernetes journey, we'll look at things like nodes, services, and other controllers that provide value to our Kubernetes applications and infrastructure.

Understanding API Objects - Services
So we just learned that Kubernetes controllers can start and stop Pods based on the health of the Pod and changes in application state. Well, what we need to know now is, how does Kubernetes add some persistency to all of that ephemerality? Well, that's where services come in. Services add persistency to the ephemerality of Pods. What a service is is the network abstraction for access to the services that Pods actually provide. And so what Kubernetes does for us is it persistently allocates an IP and DNS name for the application services that are provided by the collection of Pods that we want to front‑end with a service. And so, as Pods come and go based on their desired state, under the hood, Kubernetes is going to dynamically update the service with new information for those particular Pods. And so our users or other applications will simply access that front‑end IP address or DNS name, and Kubernetes will maintain the plumbing, or the infrastructure underneath, as Pods come and go based on their lifecycle and desired state. And so, as users or applications access that persistent IP, what Kubernetes will do is update the routing information to make sure that traffic comes in on that persistent IP and is routed directly to the Pods that are and healthy supporting that particular services. We can also leverage services to scale our application by adding and removing Pods based on the demands of that application, and services also provide a load balancing to distribute application load across the Pods providing that application services.

Understanding API Objects - Storage
So, the next question you might have in your head is what about my data? Where does Kubernetes store data for persistent storage in the cluster? And so let's talk about some storage constructs that are available inside of Kubernetes. Initially, Kubernetes had the concept of a volume, which was storage backed by physical media that was directly accessible to a pod. And so as we deployed pods, we defined that we wanted this volume of this type of storage, and those two things were tightly coupled together in a deployment. And so we lost some flexibility in how we could administer pods and how we could administer storage. And so, after that, Kubernetes came up with the concept of a persistent volume. What a persistent volume is is pod independent storage that's defined by the administrator at the cluster level. And so when a pod wants access to that storage, it defines what's called a persistent volume claim. So, in the pod definition, we basically say I want 10 GB of this type of storage, and the pod claims that storage from the underlying cluster. This technique effectively decouples the pod from the storage inside of the cluster.

Kubernetes Cluster Components Overview and Control Plane
So now that we've introduced the basic principles of Kubernetes, let's move on into exploring the Kubernetes cluster architecture and look at what a cluster is actually composed of. And so let's look at cluster components. The first cluster component is the control plane node. The control plane node implements the major control functions of a cluster. It coordinates cluster operations, monitoring, Pod scheduling, and is the primary access point for cluster administration. Next up is the Node, sometimes called the worker Node. This is where our application pods actually run. The Node has the responsibility of starting up pods and the containers supporting those pods. Nodes implement networking, ensuring the network reachability to the pods and services running on worker nodes. Each node in the cluster contributes to the compute capacity of the cluster, and clusters are generally composed of multiple nodes based on the scalability requirements of the workloads deployed. And finally, nodes can be either virtual or physical machines. The control plane node used to be called the master node, and so in some documentation and resources on the web, you will see the term master node used. The modern name for this is control plane node. I'm in the process of updating this series of courses, and we'll update this to control plane node in each course, updating the videos, code samples, and demos. So if you get to a course that I haven't updated yet, you will see this referred to as the master node. This is conceptually the same as the control plane node, and it will be updated as soon as I can get to that course. Zooming in on the cluster's control plane node. The control plane node, which implements the major control functions of a cluster, is comprised of several components, and the first is the API server. The API server is the primary access point for cluster and administrative operations. This is essentially the communication hub of our Kubernetes cluster. Now, the API server itself is stateless, and so we need a place to be able to store the state of the system, and that's where the cluster store etcd comes in. This has the responsibility of persisting the state of our Kubernetes objects. Next is the scheduler. The scheduler tells Kubernetes which nodes to start pods on based on the pod's resource requirements and other attributes such as administrative policies. And then there's the controller manager. The controller manager has the job of implementing the lifecycle functions of the controllers that execute and monitor the state of Kubernetes objects such as pods. Basically, it has the responsibility of keeping things in the desired state. And we're also going to introduce kubectl here, not really part of the control plane, but it's how we're going to interact with the API server for administrative functions. When we work with kubectl, it's going to interact with the API server for us to be able to retrieve information and also to make those changing operations to help get our system into the state that we want it to be in, such as deploying workloads. It's our primary administrative command line tool for operating our Kubernetes cluster. And now we're going to deal with this right now. I call it kubectl. You can call it kube control, kube cuddle, whatever you want to do, but for this course, we're going to go with kubectl. Now, within the control plane node, let's look even closer at those individual control plane components that we just introduced. We're going to look at the API server, the cluster store etcd, the scheduler, and the controller manager. Now, the API server is central to the control of your cluster. It's core to all operations. All configuration changes pass through the API server. It's a very simple interface leveraging a RESTful API, exposing RESTful operations like GET, PUSH, POST, and so on. Based on the operation that's going into the API server, it's the responsibility of the API server to validate that operation and persist that object state into the Cluster data store, which is etcd, and as we've introduced, etcd is the cluster data store and is used to persist the state of those API objects as key value pairs. Next up is the scheduler. It's the job of the scheduler to watch the API server for unscheduled pods, and then schedule those pods on nodes. When scheduling pods, the scheduler will evaluate the resources required by that Pod in terms of things like CPU, memory, and storage requirements to ensure their availability when placing a Pod on a specific node in our cluster. Further, the scheduler has the responsibility of respecting any constraints that we defined administratively, and so perhaps you want to keep two pods on the same node at all times. That's called Pod affinity. Or perhaps we want to do the opposite, where we want to ensure that two pods are never on the same node. That's called Pod anti‑infinity, and the Scheduler will handle that for us. The Scheduler is capable of scheduling pods based on many different other attributes, and we'll look at this more closely in an upcoming course. And then finally, the controller manager. It's the job of the controller manager to execute those controller loops. Those controller loops implement the lifecycle functions of pods ,and thus defined the desired state of the system. It's the job of the controller manager to watch the current state of the system and update the API server to ensure that its heading towards the desired state. The replica set API object is an example of a controller that we will be working with a bunch in this course. This is the controller that is used to ensure that the correct number of pods or replicas are up and running in our deployments. And so it's the responsibility of the replica set controller to communicate to the API server changes in its desired state. And so this could be operations like creating new pods or deleting pods, based on what the running state of the cluster is. And as we've introduced, there are many different other controllers available in Kubernetes, and we'll introduce more in this course and in upcoming courses as we continue our Kubernetes studies.

Nodes
Now that we've introduced the control plane node and its core components, let's take some time to look at what a node does in our cluster. A node is where our application Pods run. The node starts up the Pods and ensures that the containers in those Pods are up and running. Nodes also implement networking to ensure the reachability of the Pods running on the nodes in our cluster. We could have many nodes in our cluster based on the scalability of requirements of the applications that we're deploying inside of Kubernetes, and nodes can consist of either physical or virtual machines. Now, within a node, there are some components that I want to introduce you to, and first up is the kubelet. The kubelet has the responsibility for starting up Pods on the node, then there's the kube‑proxy, which has the responsibility for Pod networking and implementing our services abstraction on the node itself, and then there's the container runtime. This is the actual runtime environment for our containers. The container runtime has the responsibility of pulling the container image from our container registry and providing an execution environment for that container image and the Pod abstraction. Now, both the kubelet and the kube‑proxy communicate directly with the API server, monitoring it for changes to the environment. And so as Pods are scheduled to this individual node, the kubelet, it will monitor the API server for that information. And if it does have a scheduled Pod, it'll start up the containers needed to support that Pod on the node. Similarly, if there's a networking topology change that needs to be implemented, such as adding a newly‑created Pod's IP to a service for load balancing, it's the responsibility of the kube‑proxy to monitor the API server and make that modification on the node. Let's look at those individual components of a node in more detail. We're going to look at the kubelet, the kube‑proxy, and the container runtime. Now, I do want to call out that all of these components actually will run on all of the nodes in the cluster. What this means is that even on the control plane node, these three services, the kubelet, the kube‑proxy, and the container runtime, will run because there are going to be special purpose Pods deployed onto the control plane node providing the control plane services. There's also going to be a kube‑proxy providing networking services, and a container runtime as these execution environment for those Pods on the control plane node, and so all of these services will exist on all nodes in our cluster. Looking more closely at the kubelet first, as we've discussed, it monitors to API server for changes. And so, as Pods are scheduled onto individual nodes, it's the kubelet that goes and asks the API server, hey, do you have any work for me? And the API server answers that question, yes or no. The kubelet is also responsible for Pod lifecycle, meaning starting and stopping Pods and its containers in reaction to those modifications or those state changes that are being watched on the API server. In addition to monitoring the API server for changes, it's also the responsibility of the kubelet to report on both node and Pod state. So if a node is reachable and reports its status as ready, then it's available for scheduling of new Pods in our cluster. The kubelets also have the responsibility of monitoring Pod state. Is the Pod up and running? That's all reported back to the API server via the kubelet. If there is a probe used for determining Pod health, it's the responsibility of the kubelet to execute that probe. Now, the kube‑proxy has the responsibility of all networking components for nodes, and it's most commonly implemented in iptables. There are other modes for kube‑proxy, but for this course, we're going to focus on iptables. Kube‑proxy has the responsibility of implementing that services abstraction that we introduced a little bit ago, and the kube‑proxy is also responsible for routing traffic to Pods. And so as application requests come into the cluster, kube‑proxy has the responsibility of ensuring that those requests land on the correct Pods executing on the nodes in our cluster. And the kube‑proxy also has the responsibility of load balancing, making sure that traffic that's coming into multiple Pods is distributed in an even fashion. The container runtime has the responsibility of downloading container images and starting and running containers. The container runtime in Kubernetes is wrapped up in what's called the Container Runtime Interface, or CRI, and this gives us the luxury of being able to swap out the container runtime, choosing from several different supported container runtimes. Out of the box, we're going to be using containerd. This is the default container runtime In today's version of Kubernetes. There are many other container runtimes available to you, and you can use those in Kubernetes as long as they're CRI, or Container Runtime Interface, compliant. Before Kubernetes version 1.20, Docker was the container runtime used. In version 1.20, Docker was deprecated and will be removed in 1.22, or later. In practice, though, you can still use container images built with Docker in your Kubernetes cluster when using other container runtime as long as they're CRI compliant. In our cluster in this course, we'll be using containerd.

Cluster Add-on Pods
Cluster add‑on Pods are Pods that provide special services to the cluster itself, and the primary example of this is DNS. These special‑purpose Pods for DNS provide DNS services inside the cluster using the CoreDNS DNS server. The IPs for the Kubernetes service front ending these DNS server Pods and the search suffix of the domain is placed into the networking configuration of any Pods created by the cluster's API server, and so Pods and nodes and services will register their addresses with the DNS server when they're created. And since this is the DNS server used inside of the cluster for its services, it's commonly used for service discovery for applications deployed inside of a cluster. You'll find DNS Pods deployed in nearly every Kubernetes cluster. Other cluster add‑ons include ingress controllers, which are essentially advanced HTTP or Layer 7 load balancers and content routers. Also, another example of an add‑on Pod is the Kubernetes dashboard for web‑based administration of your Kubernetes cluster. Ingress and dashboard Pods are optional, and can be easily added on to any cluster if needed. Network overlays are also classified as cluster add‑on Pods. We're going to talk about those in more detail in the next module.

Pod Operations
So, now that we know the basic principles behind Kubernetes in terms of theory and we looked at the components that a Kubernetes cluster is made of, let's see Kubernetes in action and see how this all pieces together. And we're going to start off with some pod operations. Let's say we have a cluster and it's composed of a control plane node and a couple of worker nodes. Using kubectl, we submit code to instruct Kubernetes to create a deployment. And in that deployment, we've defined that we want three replicas of our pod. That request is going to be submitted to the API server, and it's the responsibility of the API server to store that information persistently in etcd. With that then, it's the responsibility of the controller manager to spin up those three requested replicas in that replica set. And so that's going to create those three pods. That request is then going to be submitted to the scheduler. The scheduler is then going to tell the API server that these pods need to be scheduled on nodes that it selects, and that scheduling information is persisted back to etcd. Which nodes did those pods get scheduled on? Well that's dependent upon the resources requested in the pods and the resources available in the nodes in the cluster.. The kubelets on the nodes will ask the API server, hey, do you have any work for me, and then it's going to start spinning up those pods that were requested by the replica set. Now everything is going along happy, happy, and the controller manager is monitoring the state of the replicas. And everyone's reporting that they're in a desired state. But what happens if a node goes down? Well, that node is no longer reporting state, and the controller manager understands that we're now outside of the desired state in the terms of the number of replicas required for our replica set. So the controller manager is going to submit for a new pod to be created and scheduled, and it's the responsibility of the scheduler to find a node to put that pod on. And so it schedules that new pod to be deployed on a node to bring our replica set back into compliance with the desired state of three replicas. Now, you might be wondering by looking at this picture, why didn't the scheduler schedule that pod onto the control plane node to maybe even out the workload a little bit between the control plane node and the worker nodes? Well, in the default configuration of Kubernetes, the scheduler taints the control plane node. And what this means is is that the control plane node is only going to run pods that are system pod, and so in this case the API server, etcd, the scheduler, and the controller manager. And a user workload is going to be scheduled onto regular worker nodes in the cluster. We can certainly adjust the configuration of the control plane node to untaint the control plane node so that user pods can be started up on it, but that's usually only good for test labs, not good for production systems.

Service Operations
With the basics of pod operations under our belt, let's see how we can extend that into services and provide a network abstraction or a single access point to our end users into the applications running inside of our cluster. Zooming out a little bit, we're not going to focus on things at the node level right now, we're going to look at things from the cluster level. And let's say we're running a deployment that creates a replica set which creates pods inside of our cluster. In this case, we're going to have four web application pods running inside of the cluster. We can expose access to these pods with a service. And in this case our service is going to be HTTP running on TCP port 80. With the service up and running, users in other applications can come along and access are web application pods via this fixed, persistent service endpoint, which is going to be an IP address or a DNS name. And as requests come in, they're going to be load balanced to all of the pods supporting that service. Now, the value of services, aside from simply having a persistent access endpoint into the application when combined with the functionality of a replica set, if a pod becomes unresponsive or is no longer available it's the responsibility of the replica set controller to remove that pod and deploy a new one. Now our users knew nothing of this because behind the scenes that service is going to stop load balancing requests to the failed, deleted pod and will continue load balancing requests across the remaining pods. Once that new pod is up and ready, requests will be load balanced to it as well. This functionality is core to building self‑healing applications in Kubernetes.

Kubernetes Networking Fundamentals
We just introduced services and accessing applications on Pods. Now is an appropriate time to take a look at some Kubernetes networking fundamentals. In Kubernetes, every Pod deployed will get assigned its own unique address. And to facilitate for this, there are some networking requirements, or rules, that we have to live by when we build our networked topologies for supporting our Kubernetes infrastructures. And the first rule is, Pods on a node can communicate with all Pods on all nodes in a cluster without network address translation, or NAT. The next rule is, agents running on a node, so things like system daemons and the kubelet, have the ability to communicate with all running Pods on that node. And so, essentially, what we need to be able to to do is to design our network infrastructures such that all Pods on all nodes in a cluster have reachability to each other with the real IP addresses that are on the nodes and the Pods themselves. And we'll look at an implementation of this in an upcoming module when we build a cluster together. So let's look more closely at the networking fundamentals, or how really Pods and nodes can communicate to each other in some various scenarios. And so let's say we have a cluster up and running, and we deploy a multi‑container Pod inside of our cluster. Well, those two containers inside that Pod, if they need to communicate to each other, they're going to do that over localhost using namespaces. Now, let's say we deploy some additional Pods. Those Pods aren't self‑contained, so they can't communicate over localhost. So they'll communicate to each other over a layer 2 software bridge on the node using the real IP addresses of the Pods themselves, and so that's how they can communicate to each other. Now, let's say we have to reach out onto a Pod onto another node. Let's say the Pods in our first node there at the top need to communicate to that Pod on the bottom. Well, that's going to happen on the real IP address of that Pod, so we'll need to facilitate for layer 2 or layer 3 connectivity between the Pods on the top node there and the Pod on the bottom node there. And so that's going to be really dependent upon our network infrastructure, so we might have to work with our network engineering team to ensure that we have the ability to have layer 2 or layer 3 reachability between the Pods on these nodes. Another common scenario is, if you don't control the underlying network infrastructure is to deploy what's called an overlay network, and that overlay network gives the ability of all of these things to seem like they're on the same layer 3 network and communicating together on an individual Pod overlay network. And the final case I want to look at with you today is external services. Let's say we have some service in our cluster, and we want to expose that to the world. Well, we learned that that's going to be implemented on the kube‑proxy, and we stand up that HTTP service, in this case, and we want to expose that to the world. And so that's how we can get external users into our applications on those services, and it's a responsibility for the service to then communicate to the individual Pods that are front‑ends. So these are the four core networking scenarios that I wanted to introduce to you today, within a Pod, between Pods on the same node, between Pods on different nodes, and external services accessing things running on Pods inside of your Kubernetes cluster. Now, as you might expect, this is only scratching the surface, and we are going to have a dedicated course just to networking coming up in this Kubernetes series.

Module Summary and What's Next!
Alright, so here we are at the end of the module. We covered a lot. I hope I got you started on your journey of learning Kubernetes well. We learned what is Kubernetes and the basic principles behind its operations. We explored the Kubernetes architecture. We learned a lot about the individual cluster components and how they interoperate to provide those services that Kubernetes provides to us as a platform. And then we looked at some basic networking fundamentals so we can learn how information moves between resources inside of Kubernetes. Well, I think we're off to a good start, and I hope you do too. Now, please join me in the next module where we're going to install and configure our first Kubernetes cluster together.

Installing and Configuring Kubernetes
Module Overview
Hello, this is Anthony Nocentino, enterprise architect with Centino Systems. Welcome to my course, Kubernetes Installation and Configuration Fundamentals. This module is Installing and Configuring Kubernetes. So far, we've looked at exploring the Kubernetes architecture, getting those basic principles underneath our belt. Now it's time to roll up our sleeves and learn how to install and configure our first Kubernetes cluster. First up, in this module, we're going to start off with some installation considerations. Basically, what do you need to know and do before you install Kubernetes? Then we're going to look at the installation overview. We're going to discuss the process of how we actually install a Kubernetes cluster. Then we'll look at where we can get Kubernetes from because, well, we need software to be able to install. Once we have all of that, we'll sit down and we will learn how to install our first Kubernetes cluster together with kubeadm. And then we'll wrap up the module with creating a Kubernetes cluster in the cloud, looking at some managed service offerings from several cloud providers.

Installation Considerations
Let's start this module off with discussing some installation considerations. What this boils down to is where are you going to install Kubernetes? Where are you going to get the service from to deploy your applications? And first up is cloud. And here there are really two major use cases when deploying Kubernetes in the cloud. You can deploy infrastructure‑as‑a service, virtual machines and deploy a Kubernetes cluster on top of that. With IaaS‑based deployments, the networking, the hypervisor, and all of the infrastructure. plumbing isn't something that you have to worry about, but you do have to worry about the operating system, patching it, and installing and maintaining Kubernetes itself as software on those virtual machines. And then there's PaaS offerings, or platform‑as‑a service offerings. Kubernetes is available as a managed service, and all the big cloud providers provide this. And you can simply go ahead and consume Kubernetes as a service. You don't have to worry about any of the underlying infrastructure or redundancy. So one of the things that you do have to think about when you are going with a PaaS offering, or a managed service offering of Kubernetes, is you lose a little flexibility in the versioning and other features that are available inside of Kubernetes, so there's kind of a tradeoff there. When you're building your own Kubernetes cluster using virtual machines in a cloud, well, you get to control exactly which version of Kubernetes that you install. But when you do that, you will take on more responsibility in administering the cluster when compared to a manage service or a PaaS offering. So another solution to getting Kubernetes is on premises. You can install Kubernetes on bare metal physical machines, or you can get Kubernetes on prem using virtual machines, and that seems to be a pretty common deployment scenario. And now that leaves us with one giant question. Which one do you choose? Well, that's really dependent on a couple of things. And what I like to think is that it's dependent a lot upon the skill set of the organization and the strategy of the organization. If you and your teams are cloud ready and you're already in a cloud and you're moving forward as fast as you can, well, yeah, it certainly makes sense to deploy in a cloud offering. Now if you want to stay on prem, well, that's fine, too. Again, it goes to the skills and the strategy of the organization. Which one do you choose? Well, take all of these things into consideration when you're working out your installation considerations for Kubernetes. This course is about the installation and configuration of Kubernetes, so we're going to focus a lot on installing Kubernetes in virtual machines. But we are also going to look at some cloud scenarios together at the end of this module. Now let's discuss some additional installation considerations that we want to take into account before we get started deploying Kubernetes. In an earlier module, we looked at Pod networking and how Pods need to communicate with each other. So we do have to take that into account before we install our cluster networking. Do we want to use an overlay network, or do we want to work with a network engineering team to make sure that we have the correct layer 2 and layer 3 connectivity between the Pods and the nodes in our cluster? And we also want to ensure that there aren't any network IP range overlaps with what we're doing in our cluster and in the rest of our network infrastructure. Then there's the question of scalability. Do we have enough nodes in our cluster with the appropriate amount of resources on each node in terms of CPU and memory to support the workloads that we want to deploy into Kubernetes? We also need to take into account that we have enough nodes in our cluster to support the workload in the event of a node failure. And then there's questions about high availability and disaster recovery. If you've noticed so far, we've discussed a single control plane node cluster. There's one control plane node doing all of the work in controlling everything in our cluster. That's certainly not a highly available solution. We'll learn in later courses in this series how we can take that control plane node and have replicas of the API server backed with multiple copies of etcd to provide redundancy for both the API server and the etcd data store. Additionally, we need to take into account for disaster recovery scenarios, ensure that we have backup and recovery of that etcd data store in the event of a catastrophic failure.

Installation Methods
Now, let's look at some installation methods for how we can get Kubernetes up and running, and first up is desktop installations. A desktop installation of Kubernetes is great for development environments or just playing around to learn how the Kubernetes platform works. Primarily, I use Docker Desktop for Mac when I just want to test various things and kick the tires inside of a Kubernetes platform rather than using a full‑blown installation or some managed service in the cloud. There is also a desktop version of Kubernetes for Docker for Windows as well. The installation method that's become the standard for installing and bootstrapping a Kubernetes cluster is a tool called kubeadm, or kubeadmin. This is a package that you can install, and with kubeadm you can bootstrap a cluster and get it up and running in a very fast way. This is the installation method that we will use in this course. And finally, as we introduced a second ago, there are cloud scenarios where you can deploy both IaaS and PaaS offerings In the various cloud providers. There are lots of different ways to get access to Kubernetes and get a cluster up and running. So go ahead and experiment one of these, but in this course we're going to focus on installing on virtual machines in a local on‑premises cluster using kubeadm. And we're also going to look at some clouds scenarios in this course as well.

Installation Requirements
When you're installing Kubernetes on bare metal or, like we're going to do in this course, in virtual machines, we have some system requirements that we need to sort out before we get things up and running. And, well, the first thing is you need Linux. We're going to focus on Ubuntu virtual machines in this course, but certainly Kubernetes is compatible with CentOS, RHEL, and many of the other major Linux distributions. Windows worker nodes are available, but that topic is out of scope for this course. You'll need 2 CPUs in your system and also 2 GB of RAM. Now this is the bare minimum to bring up a cluster for a lab. If you're sizing the cluster for a production workload, you will need an appropriate amount of resources to support your workload. Next, you need to ensure that the swap is disabled on your system, and I'll show you how to do that in an upcoming demonstration. In addition to the base system requirements that we just looked at, you'll also need a container runtime. Your container runtime will need to be Container RuntimeInterface, or CRI, compatible. Currently supported is containerd, Docker, and CRI‑O. Now, as of Kubernetes version 1.20, Docker has been deprecated and will be removed in version 1.23 or later. For this course, we're going to walk through an installation of containerd and build a cluster with that as our container runtime in our demos. But I'm also going to provide example code for building a cluster with Docker in the course downloads. From a networking standpoint, we've introduced so far the networking requirements for Kubernetes and pod networks, and so we already know that we need network connectivity between all of the nodes in the cluster. Additionally, though, we also need to ensure that each system has a unique hostname and a unique MAC address.

Understanding Cluster Networking Ports
Now I want to introduce you to the ports that are required to run Kubernetes in the event that you need to build firewalls or other security perimeters around these resources. Now, we know that this is what our cluster looks like, and we know that the control plane node provides a collection of services such as the API server, etcd, and so on. We also know that worker nodes need to access the API server. Specifically, the kubelet and the kube‑proxy on worker nodes will talk to the API server over TCP/IP. And so now, let's go through each of these cluster components together and discuss the ports that are required and who uses those ports so that you can develop the firewall rules to help secure your Kubernetes platform. The API server, by default, runs on port 6443. Now that's configurable to any port number, but that's the default port. Who needs to talk to that? Well, pretty much anything in the cluster and anything that needs to talk to it externally, whether it's you administratively with kubectl at the command line or any pipeline tools that need to have access to the system. Etcd runs on 2379 and 2380. Who needs to talk to etcd? The API server does because etcd is where the API server persists its data. If you're running a redundant configuration of etcd, the various replicas of etcd will need to communicate with each other over these ports. And so if you start scaling that out for redundancy purposes, these are the ports that are required for each of those additional etcd replicas. Next is the scheduler, which runs on 10251, and it's used by, well, itself. If you go look and see at what IP and port it's listening on, it's listening on 10251, but only on localhost. It's not exposed to the outside world. And the same goes for the controller manager, which runs on port 10252, and it also is just listening on localhost. And then finally, on the control plane node is the kubelet. This runs on port 10250, and all of the control plane components will need access to it inside of our Kubernetes cluster. Now on the worker node side of the house, there's a kubelet running on each worker node, and it runs on port 10250. And the control plane elements will need access to this kubelet. And now, for something that we haven't introduced yet, the NodePort service. It's a type of service that exposes our services and ports on each individual node in our cluster, and those port ranges are going to be allocated from 30000 to 32767. Who needs access to these ports? Well, anything that would need access to the services published on those ports. We're going to cover NodePort services in much detail in our networking course later in the series.

Getting Kubernetes
All right, so we've gone over the high‑level installation scenarios, we've gone through the system requirements, but what we haven't talked about is where do we actually get the Kubernetes software from. Well, Kubernetes is maintained on GitHub, so if you go to github.com/kubernetes/kubernetes, that's where you'll find the living, breathing project that is the Kubernetes software. So, honestly, if you're up for it, you can go ahead and contribute to this project as well. It's a very, very active and vibrant community. In addition to just the raw software that's available on this website, there is a ton, I mean a ton, of deep‑dive documentation available for you to learn about how things work at the deepest level. In fact, if you're up for it, you can go read the code itself. Now, for this course, and for most of the production systems that I support, I use Linux distribution repositories, both yum or apt depending on which Linux distribution I'm using. In this series of courses, we're going to use Ubuntu, so we're going to get Kubernetes from an apt repository.

Building Your Own Cluster
So with the software in our hands, it's now time to learn the steps that we need to take to build our first Kubernetes cluster together. And, well, step one is install and configure a container runtime and the Kubernetes packages. Today, in this course, we're going to learn how to install Kubernetes from packages and use containerd as our container runtime. Then once the packages are installed, we need to create our cluster. What this means is, using kubeadm, we're going to bootstrap that first control‑plane node and get those critical cluster components up and running, the API server, the controller manager, etcd, and so on. With the control plane up and running, then we need to configure our Pod networking environment. In this series of courses, we're going to use an overlay network for Pod networking in our cluster. Once we have our Pod network up and running, we can then join additional nodes to our cluster for worker nodes that we can use for our application workloads. Now, let's look at some of the required software packages to get started working with our first Kubernetes cluster. Again, we're going to be using Ubuntu in this series of courses, so this means we'll be using the app package manager for package installation. First up, we'll need a container runtime. We're going to use containerd in this series of courses. But as we discussed a moment ago, I'm going to give you code for both containerd and Docker installations. In our upcoming demo, we'll walk through the process of installing and configuring containerd together. And in the course downloads, I'll provide examples for both containerd and Docker since Docker is going to be around until version 1.23, or later. Next is the kubelet. The kubelet is the thing that's going to drive the work on individual nodes in our cluster. That comes from a package named kubelet. There's also a package named kubeadm, or kube Adam, or kube admin, whatever you want to call it, but it's the tool responsible for bootstrapping our cluster and getting the cluster components up, running, and configured. It's also the tool that we're going to use to join additional nodes to our cluster. And then finally, there's kubectl, or kube control, or kube cuddle, whatever you want to call it again, but it's the primary command‑line tool that we're going to use to administer the workloads in our cluster. Now, I do want to call out that you want to install all four of these packages on all nodes in a cluster, regardless of if they're a control plane node or a worker node.

Installing Kubernetes on VMs
Now, let's go ahead and look at the sequence of commands that we need to use to get an installed Kubernetes on some Ubuntu virtual machines. And I do want to point out here that we need to do this on all the nodes that we're going to have in our cluster, both control plane and worker nodes. First up, we need to install our container runtime. And on an Ubuntu system we'll do that with apt‑get install containerd. When we get into the demos, we'll have some additional configuration steps that are needed to configure containerd to use the systemd cgroup driver. The next thing that we're going to do here is we're going to add the GPG key for the apt repository where the Kubernetes packages live, and then we're going to execute this series of commands to add the Kubernetes apt repository to our local repositories list. Now, don't worry about writing this all down or taking screenshots, these will be available to you in the course downloads. And we're going to go through this process together in our upcoming demonstration. Now, with that new repository installed, we need to tell apt to update its package list, and we can do that with apt‑get update. And then we'll use apt‑get install to install the three remaining required packages, the kubelet, kubeadm, and kubectl. Now, here's where we're going to deviate from the normal Linux package installation. What we're going to do here is we're going to mark these packages with apt‑mark hold for the four packages that we need to install. And the reason for this is we no longer want apt to maintain the upgrading of these packages. We're going to service these packages outside of the normal security updates for the system so that we can control when we move between versions of Kubernetes independent of security patches being applied. And this is needed because Kubernetes has a defined upgrade process when moving between versions. We're going to cover that process in a course later in this series.

Lab Environment Overview
So we just looked at the code‑level detail to install Kubernetes on some Ubuntu virtual machines. Now, let's zoom out a little bit and look at the lab environment that we're going to use in this course. So, here you can see we have one control plane node and three worker nodes, and so that's the topology of the cluster that we're going to construct together in the upcoming demonstrations and use throughout this series of courses. We're also going to have kubectl installed on the control plane node. And so we'll drive all of our work from that particular node when we're working with our cluster. The version of Ubuntu that we're using in the lab virtual machines is Ubuntu 18.04. I've tested all the labs with the current version of Kubernetes 1.21 on Ubuntu 18. As versions of Kubernetes change, I will continue to update the code in the course downloads to ensure that they work with the latest version of Kubernetes. So if you experience an issue, please reach out to me via email or in the course discussion board. I'll check things out and update the code accordingly. Now as for our lab VMs, my virtual machines are VMware Fusion. You can use pretty much any hypervisor you want, as long as you have full network connectivity between all of the nodes in your cluster. Minimum CP requirements stand, so I have two virtual CPUs per virtual machine in this lab environment. And I also have 2 GB of RAM in each virtual machine. I'm a little generous with the virtual machines from a disk space standpoint because honestly, it's kind of a harder thing to change after the virtual machine is configured, and so I'm allocating 100 GB for each virtual machine in this cluster, and we also want to make sure that Swap is disabled on each virtual machine that we're using. Now I have all of the host names set for each of these individual nodes, and each of these node names is in a host file on each virtual machine in the cluster with the corresponding IP address, so that I don't have to rely on DNS for hosting resolution between any of the virtual machines, that's going to be hardcoded inside a host file on each node. Now as for host names, here we go. For our control plane node, it's going to be c1‑cp1 for cluster 1‑control plane 1, and its IP address in my lab is going to be 172.16.94.10. You can use whatever IP in your lab that you want to use. Now if you're coming from a previous version of this course, this system used to be called c1‑master1, so please be aware and update any code accordingly. Any where you see c1‑master1 in any future course, go ahead and swap in c1‑cp1. I'll be updating the videos and the code in each course in this series as I get to them. Next up is c1‑node1, so a pretty easy naming convention there, its IP address ending in 1.11, c1‑node2 ending in .12, c1‑node3 ending in 13. And so that's our cluster, one control plane node and three worker nodes.

Demo: Installing and Configuring containerd
All right, it's taken us a long time to get here, but it's time for our first demo together. In this demo, what we're going to do is we're going to install some packages together, containerd, kubelet, kubeadm, and kubectl. And with all those packages installed, we're going to look at some systemd units for some of these packages to understand how they operate with the underlying systemd system. Alright, so here we are in VS Code, and let's get started with the process of installing the required packages to bootstrap our Kubernetes cluster. In VS Code here, what I'm going to do is highlight the code at the top, and I'll execute that code, and it's going to show the output at the bottom in our environment. So this way we're able to see both the code that I'm executing and the output at the same time. I've already gone ahead and deployed the four virtual machines that we discussed in the presentation portion of the course, so I have c1‑cp1, c1‑node1, 2, and 3 all up and running in my environment here. I also, right now, have an SSH connection open to c1‑cp1, as you can see at the bottom here. And so let's go ahead and start the process of installing and configuring the proper software for our Kubernetes environment. The first thing that we'll need to do, and we're going to need to do this on every node in our cluster, is to make sure that our swap is disabled. And I've already actually done that for this virtual machine, and so here you can see if I do a swapoff ‑a, I get no output because I've already disabled the swap file. And so let me go ahead and show you how I did that. I did that by editing the fstab. Highlight this here, run that code at the bottom, and there we can see the contents of our fstab. On the second line here, we can see that the swap is disabled because this line is commented out. And so what you'll want to do here is either delete that line, comment it out, save the file, and then reboot your system and your swap will be disabled. And with that configuration complete, let's move forward into the installation and configuration of containerd. Now I do want to call out that the containerd installation right now is a little bit more complicated than I think it should be, and I expect that this process is going to become more streamlined as containerd becomes kind of the centerpiece of the container runtimes for Kubernetes. And so what we're going to see here is we'll have to perform some extra steps to get this containerd installation configured for us to use Kubernetes on top of it. And what I expect is that this is going to become more simple, and so I promise to keep this code up to date with the latest method, but this is the most current method for configuring and installing containerd, and so let's walk through that process together. The first thing that we'll need to do is to load some modules, and with lines 28 and 29 here, I'm going to load both the overlay and the br_netfilter module. Now that's the runtime module that's right now in this configuration, but I also need to make sure that they're set on boot, and we can do that here on lines 31 through 34 with this here doc. And that's going to write the appropriate information into the modules‑load.d directory in etcd, so we'll run that code there. It'll create that file in the appropriate directory to ensure that these modules get loaded when the system reboots. We also need to configure some sysctl parameters, and again, we want to make sure that those persist across reboot. And so we'll grab this here doc here from lines 38 to 42, run that code, and it's going to write the appropriate sysctl parameters into /etc/sysctl.d in the file 99‑kubernetes‑cri.conf With that file created, we can then apply that configuration with the code on line 46 here, so sudo sysctl, space, ‑‑system. Now those configurations that were in that file are applied to the running system. With those configuration prerequisites complete, both the modules and the sysctl parameters set, it's now time to install containerd. We'll make sure we have an up‑to‑date package list, and we can do that with the code on line 50 here, sudo apt‑get update. Once we have the updated package list, we could install containerd. And the code to do that is on line 51, sudo apt‑get install ‑y containerd. Run that code, and that's going to install containerd for us on this system. Now with containerd installed, we need to apply some configuration specific to containerd, and we'll do that with the code here on line 55 and 56. First up, we'll create a directory for a containerd configuration file to live in, and on line 55 we have mkdir ‑p/etc/containerd to make that directory. Now on line 56 here, we can use the containerd command to generate a default configuration file, so there is a pseudo containerd config default. That's going to generate the default configuration file. We're going to pipe that output into sudo tee and write that output into /etc/containerd/config.toml. And inside that configuration file will be the configuration attributes for containerd. We're pretty good with the defaults except for one change. We need to change the Cgroup driver of containerd to systemd, which is required for our kubelet. And we'll look at how to configure the kubelet in the next demonstration a little bit later in the course. And what we're going to do here is grab the text here on lines 68 69, that's our secret driver configuration, and copy that into our clipboard, and then open up the config.toml file in Vim. And we're going to look for the string that we have on line 65 there, and it ends in containerd.runtimes.runc. And below that, we're going to paste in this text from 68 and 69 in that file. So let's go ahead and find that information in our config file. So jumping down to the bottom here, I'm going to look for containerd.runtimes.runc. Once I find that line, here we go. I'm going to add a new line and drop in that text that I copied from line 68 and 69. And so there we can see I added that configuration into the configuration file, and we're going to write that out and save it. With that written out, let's go ahead and restart containerd, and we can do that with sudo systemctl restart containerd to make that configuration change for containerd part of the running configuration for containerd.

Demo: Installing and Configuring Kubernetes Packages
With containerd installed and configured, now it's time to move forward and install the Kubernetes packages kubeadm, kubelet, and kubectl, and the first part of that process is to add Google's apt repository GPG key to our system so that we can trust that repository. The next step then is to add the Kubernetes apt repository to our local repositories list, and we can do that with the heredoc that's on lines 86 through 88. Run that code together, and that's going to configure that local apt repository on our local system here. With that new repository added, we're going to want to update the package metadata information for our system so that we can get the package information from that newly added repository, and we can do that with sudo apt‑get update. Now let's take a peek at what's available to us to install from that new repository, and I want to look at the kubelet packages to see the different versions of the kubelet that are available. And I could do that with apt‑cache policy and then specifying the package in kubelet. I want to pipe that into head and limit that to 20 lines of output. In the output at the bottom here, we can see the different versions of the kubelet that are available as packages in the repository. So there we see 1.20.2‑00, 1.20.1‑00. And so those are all the different versions of the kubelet that are available to us to install. And what we're going to do here is, we're actually, we're going to pin our installation to a specific version, and we're going to install 1.20.1‑00, and I have that set here as an environment variable on line 98. And what we'll do then is, on line 99, we'll specify the exact version of the packages that we want to install. So there we see sudo apt‑get install ‑y, and then the package name equals, and then we're referencing that environment variable that we just declared, version. We're going to repeat that pattern for kubeadm and also kubectl, specifying the version for each, making sure that they're the same, all on 1.20.1‑00. And the reason why I want to do that is because later on in this series, of course, we're going to run an upgrade to a newer version of Kubernetes. And so if you saw there was 1.20.2 is available, so that way I have a version to upgrade to when we get to that course. Let's go ahead and run this code together to get the kubelet, kubeadm, and kubectl installed. Now with our packages installed, let's go ahead and mark all four packages with hold, and we can do that with sudo apt‑mark hold, kubelet, kubeadm, kubectl, and containerd. And what that will do is prevent these packages from being updated when someone comes along and updates the system for security updates with apt. Now on lines 104 and 105 here, I have the code that'll allow you to install the latest and greatest version of Kubernetes. I held us back one version intentionally and also showed you how to pick a specific version. If you want to just install the latest version that's available, you can use the code that's on line 104 and 105. So now with the requisite packages installed, let's look at the systemd units for a couple of the services that were installed, and first up is the kubelet. I want to look at sudo systemctl status kubelet.service and look at the output that's generated for this particular system to unit. And we can see something interesting here, is that we see that the main process exited, code=exited, status=255 failed with result exit=code. And we also see that the systemd unit's status is activating, rather than active, which would mean that the service is up and running. What's happening right now is the kubelet's actually crash looping because there's no cluster configuration yet. We haven't told the kubelet to do anything specific. And so later on when we get into the next portion of the course we're going to learn how to bootstrap a cluster. And what that's going to do is write some information into a specific location the kubelet's looking for. Well, that information's not available there yet, and so the kubelet is going to be in what's called a crash loop, looking in that specific location for the configuration information that's generated by the bootstrapping process for our cluster. But we haven't done that yet. And so hang on to that thought right now, and a little bit later in the next demo when we bootstrap the cluster, we're going to revisit this and see when the kubelet is up and running and healthy because our cluster has been bootstrapped. The other service that I want to look at is also containerd, just to make sure that that's in good shape before we move forward with our demonstrations. So sudo systemctl status containerd.service, run that code there. We can see it's active and running, so it's up and loaded and in a proper state. So let's break out of this output here. And one final step is just to make sure that both of these services are set to start up when the system starts up. We can do that with sudo systemctl enable kubelet.service. So let's going ahead and run that code. And we'll do the same, sudo systemctl enable containerd.service to make sure that both of those services are set to start up when the system boots. So what we've done right now is, we've installed the required packages to start building, or bootstrapping, our cluster, what you're going to learn about now in the next portion of the course.

Bootstrapping a Cluster with kubeadm
With the software packages installed, it's now time to start the process of bootstrapping or creating a cluster with kubeadm. Kubeadm is a tool that we can use to manage the process of creating our cluster. It's going to walk through various phases, and build and configure our cluster for us, and right now we're going to walk through each one of those steps together so that you know what's really going on during the creation of a Kubernetes cluster. Now, to bootstrap a cluster at the command line, you'll type kubeadm init, and it's going to begin the process of creating your cluster. The first phase is the pre‑flight checks. What kubeadm is going to do here for you is execute a series of checks that will help ensure a successful cluster creation. So things like ensuring that you have the right permissions on the system that you're working with. It's also going to verify that you have the appropriate system resources on this box in terms of CPU and memory, and another important pre‑flight check that it executes is it checks to see if there's a compatible container runtime on this system, and if it's up and running. If any of these checks fails, kubeadm will report the error and stop the cluster creation process. The next thing that kubeadm does for you is it creates a certificate authority for you. Kubernetes uses certificates for both authentication and encryption. We're going to look at this in more detail in an upcoming slide. In the next phase, what kubeadm does is it generates kubeconfig files for you for the various cluster components of Kubernetes, so that they can locate and authenticate against the API server. We're going to look at these much more closely in an upcoming slide as well. Then the next phase is generating static pod manifests. Static pod manifests are going to be generated for each of the control plane pods, and what's going to happen here is they're going to be written to the file system and the kubelet is going to monitor that location. If it finds a pod manifest there, well the kubelet's going to start up the pods defined in that manifest, and we're going to look at this also in much more detail in a few moments. And then, after those static pod manifests are generated, kubeadm is going to wait for the control plane pods to start up. So the API server, etcd, and so on, will be started up as pods. Then kubeadm goes on to taint the control plane node. We discussed this a little bit ago when we looked at pod operations earlier in the course. What this means is when Kubernetes schedules pods, it will never schedule user pods onto the control plane node, it will only ever schedule system pods to run on that control plane node. Then, kubeadm generates a bootstrap token used for joining additional nodes to the cluster, and then finally, in its final phase, it will start up any add‑on pods, such as DNS and kube‑proxy pods. The process that we just described here is the default process if you use kubeadm init with no parameters. This bootstrapping process is very customizable, and you can build more complex or customized clusters using kubeadm command line parameters, or even a YAML manifest configuration file to describe your cluster's configuration. In this module, we're going to learn how to create a cluster with a configuration file, since our cluster is using containerd rather than Docker as its container runtime. In that cluster configuration file, we will specify that we're using containerd as our container runtime, and we'll also define the C group driver for the kubelet, which is systemd. The reason why we're using systemd as our C group driver is because Ubuntu is a systemd based system.

Understanding the Certificate Authority's Role in Your Cluster
It's time now to zoom in on those facets of kubeadm that I called out a second ago, and we're going to start that conversation with the certificate authority. Kubeadm init, by default, will create a self‑signed certificate authority for you. And if desired or required in your organization, you can tell kubeadm init to integrate with an external PKI, or public key infrastructure, if that's needed in your organization. The certificate authority that's created by kubeadm init is used to secure cluster communications. It's going to generate server certificates that are used by the API server to encrypt the HTTP stream that is the API server's communication protocol. So that's going to be HTTP over TLS, in other words, HTTPS. In addition to securing cluster communications, the certificate authority is used to generate certificates for the authentication of users and cluster components like the scheduler, controller manager, and kubelet to authenticate the request of the API server. And so we'll have strong authentication methods for users that have to operate the cluster, any of the cluster components, and also the nodes in our cluster since the kubelets will use certificates to authenticate and verify their identity. The certificate authority files live in /etc/kubernetes/pki and will be distributed to each node in your cluster when you join additional nodes to your cluster using kubeadm init. This distribution process ensures that the nodes trust the self‑signed CA. For more information on how to customize your kubeadm init process, check out this link here. This article includes the many customizable parameters for kubeadm init, including how to specify an external PKI configuration, amongst many more.

kubeadm Created kubeconfig Files and Static Pod Manifests
Next up is kubeadm‑created kubeconfig files. What is a kubeconfig file? A kubeconfig file is a configuration file that defines how to connect to your cluster API server. Inside a kubeconfig file, you're going to find the certificates used for client authentication. And you'll also find the cluster API server's network location. Usually it's going to be its IP address or DNS name. Often included in a kubeconfig file is the CA certificate of the CA that was used to sign the certificate of the API server. This way, the client can trust the certificate presented by the API server upon connection. Kubeadm creates a collection of kubeconfig files used by various cluster components, and those live in /etc/kubernetes. Let's take a look at each one of those now. The first kubeconfig file that's generated is admin.conf, which is the Kubernetes cluster administrator account, essentially a super user inside of our Kubernetes cluster. And we're going to use this to authenticate to our cluster today in our upcoming demo. Kubeadm also generates kubelet.conf. Kubelet.conf is used to help the kubelet locate the API server and present the correct client certificate for authentication. Similar kubeconfig files exist for the controller manager and the scheduler, again, to tell these components where the API server is on the network and also which client certificates to use for authentication to the API server. These kubeconfig files are generated by kubeadm during the cluster bootstrapping process and are used by these components to locate and authenticate the API server. Later on in this series of courses, we'll look at how to create kubeconfig files for individual users that will locate and authenticate to the cluster API server, and we'll also learn how to set appropriate permissions for users. The next stop on the list in our closer look at what kubeadm init does for us is static pod manifests. A manifest describes a configuration, and in this case it's going to describe the configuration of a pod. And so kubeadm init is going to generate static pod manifests for our cluster control plane components that it needs to bootstrap a cluster and get it up and running. Those static pod manifests are going to live in /etc/kubernetes/manifests. And it's the job of kubeadm init to generate a static pod manifest for each of the core cluster control plane components, and that includes etcd, the API server, the controller manager, and the scheduler. So, kubeadm init generates the static pod manifests for each of the control plane pods, and they live in that special directory. It's the job of the kubelet to watch this directory on the file system. And for any static pod manifests that it finds in this directory, it'll start up the pods described in those manifests. And in this process here, it's the control plane pods. This is how a cluster starts up after a reboot as well. The kubelet is started by the systemd daemon on the node on boot. And then on the cluster we'll find the static pod manifests in this directory, and it will start up any of the pods defined in the static pod manifests. And in this case, it's going to be the control plane pods. Now, you might be wondering, why do I need this special configuration to start up these pods? Well, this is what enables the startup of these core cluster components, well, without the cluster being up and running.

Pod Networking Fundamentals
Now, that wraps up the conversation about kubeadm init and a closer look at each one of those phases. Now, let's shift the focus over to the last thing that we need to talk about before we actually stand up our Kubernetes cluster. We're going to talk about Pod networking again. Let's say we're designing a cluster. We have the requirements of the Kubernetes networking model that we introduced earlier in this course. We need to ensure that we have a single un NATed IP address per Pod and that all Pods have reachability between each other Now, we could, of course, use direct routing, and in this case, we would have to configure the infrastructure underneath our Pods and nodes to support full IP reachability between the Pods and nodes without NAT using real IPs, and sometimes that's just not feasible. Perhaps you're in a cloud scenario or in a scenario where you don't have control over the underlying infrastructure, and that's where overlay networking comes in, sometimes called software‑defined networking. With overlay networking, what happens is we get the appearance of a single layer 3 network that the Pods can all communicate on. And some of the techniques that are used to make this happen are tunneling and encapsulation of the IP packets, and that's the responsibility of the overlay network to facilitate for those communications of those packets between the Pods in an unchanged way. Now, some of the overlay networks that are available to us include Flannel, Calico, and Weave Net. These are overly networks, each of which have different capabilities and features that may be interesting to you as you deploy your overline network. In our demos today, we'll be using Calico for our Pod network. When deploying a Pod network, it will provide the IP address management for the Pods deployed. Now, it's imperative that the network range must not overlap with other networks for your network infrastructure, so be sure to work with your network engineering teams to find an appropriate address range to use for your Pod network. For a full discussion of overlay networking, I encourage you to check out this link here. You'll find many more overlay networks available and other networking solutions that you can use for your Pods and nodes so that they can communicate to each other. This discussion here is really just scratching the surface for what you need to know about networking to get a cluster up and running. We have a full course dedicated the Kubernetes networking coming up later in this series.

Creating a Cluster Control Plane Node and Adding a Node
With all of the preliminaries out of the way, it's time to use kubeadm to bootstrap the control plane node in our Kubernetes cluster. The first thing that we need to do is download a YAML manifest that describes our Pod network that we want to deploy, and that's calico.yaml. This is the actual deployment manifest for our Pod network. Inside of that file is where you can specify the Pod network IP address range. We'll look at how to configure that upcoming demo. If this URL changes for the Calico Pod network, I will keep the course downloads updated with the new URL. Next, you'll need to create a cluster configuration file, and there's an easy way to do that with kubeadm. We can use kubeadm config print init‑default and then write that out to file. And so here we're using tee to write the output to console and also into a file manifest named ClusterConfiguration.yaml. Inside of that file is the configuration defaults for a Kubernetes cluster. In our upcoming demo, we're going to use this command to generate that cluster configuration file, and then we're going to make some required changes to that file in our lab environment together. Once we have that ClusterConfiguration file, the next thing that we need to do is to execute sudo kubeadm init, kicking off the kubeadm init process that we just walked through together. We need to then pass in our ClusterConfiguration file with the ‑‑config parameter and specifying the file location of the configuration file that we just created, ClusterConfiguration.yaml. We then need to specify the ‑‑cri‑socket of our container runtime, and we're going to be using containerd in this course, and so here you can see the path /run/containerd/containerd.sock, which is the file location of our ‑‑cri‑socket. If you don't specify a ‑‑cri‑socket in the current version of kubeadm, it will default to Docker and will error out. I expect this to change and be updated to autodetect the container runtime in future releases of kubeadm. When you get into the demos, if you decide to use Docker Azure container runtime for your lab, you'll find the kubeadm init process is a bit different. In fact, it's a lot simpler. The reason is a lot of the configurations are autodetected and don't require cluster configuration files or specifying the ‑‑cri‑socket at the command line with a parameter. But like I said a second ago, I expect the containerd implementation to get a lot simpler, and I'll update the course accordingly. Now, once kubeadm init finishes, all the control plane Pods will be up and running. Kubeadm init will also print out the commands and the parameters needed to join additional nodes to your cluster, and it will also print out how to configure a kubeconfig file for an administrative user. We'll review the join command in a moment, but now let's look at how to work with that kubeconfig file for an administrative user. The following steps will enable you're currently logged in user to perform administrative functions on your cluster. And first what we'll do is we'll create a directory for our kubeconfig file in our current HOME directory, and then what we'll do is we'll copy the admin.conf kubeconfig file from /etc/kubernetes into our HOME directory. And then what we'll do is adjust the permissions on that file so that we can access this with our current user. This kubeconfig file allows the current user to perform administrative functions against the cluster, and inside of that kubeconfig file you'll find the required certificates to authenticate to the cluster and also the network location of our cluster's API server. The final step that we need to take after creating our control plane node with kubeadm init is to deploy a Pod network, and we can do that with kubectl apply ‑f calico.yaml. This will read the deployment manifest file that we just downloaded, send it into the API server, and then create the Pod network in our cluster. Once the Pod network is finished, then the DNS add‑on Pods will be able to start up, and we'll have a fully functioning control plane node. The next thing that we need to do now is to add some additional nodes to our cluster for user workload. Let's check out how to do that. Nodes, or worker nodes, are where your user workloads will run in your cluster. To join a node to a cluster, you first need to install all the required software packages onto your soon‑to‑be node. So your container runtime, in our case, that's containerd, kubeadm, kubelet, and kubectl all need to be installed. We then used kubeadm with the join parameter to start the process of joining a note to a cluster. Kubeadm join takes some additional parameters, such as the network location of the API server, a bootstrap token used to join a node to a cluster, and a CA cert hash used to trust the certificate presented by the API server. We'll look at some example code for that in a second. When we execute kubeadm join on the node that we want to join to the cluster, what's going to happen is that node is going to download some cluster information and configuration metadata. It's then going to generate and submit a certificate signing request, or CSR, into the API server to generate a certificate that would be used by the kubelet on the node we're joined into the cluster, and this is used by the kubelet to authenticate to the API server. The CA is going to automatically sign that certificate signing request, and then kubeadm join is going to download that certificate and store that on the file system of the soon‑to‑be node. That certificate is going to live in /var/lib/kubelet/pki on the node. Kubeadm join is then going to generate a kubeconfig file named kubelet.conf, and that kubeconfig file is going to live in /etc/kubernetes on the node. In that kubeconfig file is going to be a reference to that new client certificate and also the network location of the API server that we want to authenticate against for the cluster that this node is joining. This process is called TLS bootstrapping. The code for kubeadm join looks like this: kubeadm join and then the IP address and port of the API server, which in our lab is 172.16.94.10 on port 6443. If you're using a different network address, please update that here. The next parameter is the bootstrap token. This effectively is a time ticket or password for joining a node to a cluster. And then the next parameter after that is the CA cert hash. The CA cert hash here is used to establish a certificate chain of trust between the node joining a cluster and the certificate presented by the API server for HTTP requests during the join process. Recall a moment ago I told you that the output of kubeadm init when we create our cluster will print this command and the required parameters to the screen so that you can use that to join additional nodes to your cluster. In our upcoming demos, I'll show you just that, and I will also show you how to print the bootstrap token, the CA cert hash, and also the entire join command on demand so that you can get that information when you need it outside of that initial cluster creation process.

Demo: Creating a Cluster Control Plane Node
All right, let's get into a demo and look at how we can create our first Kubernetes cluster together. Once that's up and running, we're going to deploy a Pod network, and then we're going to look at those systemd units again to see what changed from the last time when we installed the kubelet and how it's going to react differently now that we have a cluster up and running because of the static Pod manifests generated by kubeadm init. Then we're going to look at those static Pod manifests for each of our control plane Pods, and then we're going to join a couple of nodes to our new cluster. Here we are logged into c1‑cp1, and let's go ahead and begin the process of creating our first Kubernetes cluster together. Now this process that we're going to go through is for containerd. If you're interested and still need the Docker demonstrations, those are available to you in the course downloads here in the docker directory. This demo that we're going to walk through together will be using containerd. Now the first part that we need to go through for creating our cluster is to download the deployment manifest for the Calico Pod network. And have the code to do that here on line 11, and so we'll use wget to retrieve the calico.yaml file from the Calico website. Inside that manifest is a setting that describes the Pod network IP range, and that setting is CALICO_IPV4POOL_CIDR. So let's grab this string here and put that in our clipboard and then open up the file that we just downloaded, calico.yaml, and find that setting inside the manifest. Now what we see here is the value for that field is 192.168.0.0/16. All Pods are going to be allocated IPs from that network range, and so we want to make sure that that network range doesn't overlap with other networks in our infrastructure. If it does, you can set that value here. We're going to leave that as default. And so let's go ahead and break out of this file and get back to our console. Moving forward in the demo, the next part is creating a kubeconfig init configuration file. That configuration file is going to define the settings of the cluster that kubeadm is going to build for us. We can generate a default file with the code here on line 24, kubeadm config print init‑defaults. We're going to take the output of that and write that into a file named ClusterConfiguration.yaml. Let's run that code together and review the output of a default ClusterConfiguration file. Scrolling up a little bit, in this output here, we can see, I do want to show you that there is one error that's going to be thrown. In this current version of kubeadm that we're using for this demo, it's going to look for Docker, and, well, Docker is not installed, and so that's an active bug that will be fixed in a future kubeadm installation, so we can safely ignore that warning here. Looking inside of the output of this ClusterConfiguration document, we see LocalAPIEndpoint, advertiseAddress: 1.2.3.4 That's the IP address of the API server, and so we're going to want to update that to our IP address of our control plane node, which in our cluster is going to be 172.16.94.10 or whatever IP address you're using in your lab. The next thing I want to call out is in nodeRegistration criSocket. There we see /var/run/dockershim.sock. We're going to want to update that from the default, which is Docker, to the container runtime that we're using, which is containerd. One other element that we're going to add to this document that's not in the configuration yet is defining the cgroup driver for the kubelet and setting that to systemd, and so we're going to add some configuration there in a moment. Scrolling down a little bit further in this file, we can see Kubernetes version is v1.20.0, and we're going to want to update that to v1.20.1, which is the version of the Kubernetes packages that we installed in the previous demo. And so let's go ahead and begin the process of updating those four elements together. Now I've written some code here that will swap out those values we've set for use that we'd get through this process in a little more streamline of a fashion. And so on line 34, here I have sed that's going to swap out the advertiseAddress from 1.2.3.4 to the actual address of our control plane node, which is 172.16.94.10. Let's go ahead and run that code to make that change. Moving forward to the next update that we have to make, we're going to change the CRI socket to point to containerd, which is the contain a runtime on our node here. And so there on line 38 we have criSocket: \/var\/run\/dockershim\.sock, and we're going to change that to \/run\/containerd\/containerd.sock. Run that code there to make that update and move forward to the next change that we need to make to our cluster configuration, which is to set the cgroupDriver to systemd for the kubelet. Now this configuration doesn't exist in the default document that's generated by kubeadm, and so we're going to add that to our ClusterConfiguration file with this here doc here starting on line 42. And in that code there, you can see on line 46 cgroupDriver is set to systemd for the kubelet. Run that code there to append that to the file that we're working with. Now let's hop into that file to review those changes and make that one final change for our Kubernetes version. Scrolling down in the output here, we can see that the localAPIEndpoint advertiseAddress is now 172.16.94.10, which is the IP address of our control plane node. We also see that the criSocket is now /run/containerd/containerd.sock, and so that update was successful to the configuration document. Going down a little bit further, let's go ahead and change the Kubernetes version from v1.20.0 to v1.20.1. Now remember this is going to be whatever version of Kubernetes that you installed into the previous demonstration, and you want to make sure that those match. So if you move ahead in a version, please be sure to update that here. Scrolling down a little bit further, there at the bottom we can see our kubelet's configuration defining the cgroupDriver as systemd. Let's save this file out and move forward into the next part of our demo where we bootstrap our cluster together. And so on line 58 here I have sudo kubeadm init and then the parameter ‑‑config=ClusterConfiguration.yaml. That's the configuration document that we just built up together. We're also going to specify the CRI socket and point that at containerd. Let's highlight all of that code and run that together and begin the process of bootstrapping our cluster. Now I'm going to speed this process up a little bit so we don't have to wait for the whole deployment process, and then we're going to review the output together. All right, with that finished, we can see our command prompt is returned. Let's scroll to the top and review the output of that cluster initialization process together. So in the output here, we can see we're using Kubernetes version 1.20.1. The next phase is preflight checks where that goes through those preflight checks that we talked about during the presentation portion of the course. In the next phase, it creates the CA for us, or that certificate authority, and then goes through a series of commands to generate certificates for each one of the control plane Pods. Then it goes into the kubeconfig phase generating and writing out the kubeconfig files for admin.conf, kubelet.conf, controller‑manager.conf, and scheduler.conf. Moving forward, it then creates the static Pod manifests for each of the control plane Pods. There we see kube‑apiserver, kube‑controller‑manager, and kube‑scheduler. It writes those all into /etc/kubernetes/manifests. After the control plane Pod's static Pod manifests are generated, it also creates a static Pod manifest for etcd. And then at that point in time, once all the control plane Pod's static Pod manifests are in place, it goes into a wait state waiting for all those static Pods to start up, and so there we see wait for control plane. And then a few seconds later we see all control plane components are healthy after 24 seconds here in my case. Moving forward into the output here at the bottom, we can then see it creates the add‑on Pods. So there we see CoreDNS is created and kube‑proxy. Now the most crucial output in all of this here is seeing the output that says your Kubernetes control‑plane has initialized successfully! And so when you see that, you know your control plane node is up and running. Now we see some code here that describes how we can connect to our cluster as a user, and that's the code that we reviewed in the presentation portion that will copy the kubeconfig file for admin.conf from /etc/kubernetes into our HOME directory and then sets the permissions on that. And then moving forward, at the bottom there, we can see the join command that we'll use in an upcoming demonstration on how to join a node to the cluster, and we see kubeadm join and then the IP address of our API server, which now is populated with the correct address 172.16.94.10. And so with our control plane node up and running, let's move forward and execute those commands that we need to access our control plane node. And on line 68 here, we can see mkdr ‑p. In our HOME directory, we're going to create a hidden directory .kube. Run that code and then copy /etc/kubernetes/admin.conf into that directory that we just created and write that into a file named config. That's going to copy that admin.conf kubeconfig file into our user's home director so that we can use that to connect to our Kubernetes cluster. And then next, we're going to change the permissions on that so that our regular user can access that file. With our cluster up and running, we can now deploy our Pod network, and we can do that with kubectl apply ‑f calico.yaml. Run that code there. What that's going to go ahead and do is create a collection of resources that define the Pod network in our cluster forest. And so now what's happening is a bunch of Pods and resources are being created to implement that Pod network within our cluster. And now let's take a peek at what's happening inside of our cluster, and we can do that with kubectl get pods, and we're going to get pods across all namespaces. Well, really haven't discussed what a namespace is yet, but it's a way for Kubernetes to organize resources, and we're going to focus on some system Pods right now, which are things like the control plane Pods and the Calico Pods that we just deployed. In the output at the bottom here, we can see a collection of Pods that are in different statuses. Let's review some of the output together. And so we see our control plane Pods, etcd, apiserver, controller‑manager and scheduler Those are all up and running because we just completed that cluster initialization process. We also see kube‑proxy is up and running on this node. That's going to implement service networking on this individual node. We then see our coredns Pods are in the status of ContainerCreating. So what that means is those Pods are out pulling the container images to start those Pods up. And similarly, we see two Calico Pods that are in the status of PodInitializing, or creating the Pod, and then also ContainerCreating, most likely pulling the container image down again. If I use the up arrow and execute that command again, we can see that our Pods are still in that same state. And so rather than continuing to iterate and hit the up arrow and press Enter over and over again, I can add the ‑‑watch parameter to that command. And what that's going to do is output to the screen the current state of all the Pods in the system. And as the status of those Pods change, it'll be updated and written to console. And so this way, we kind of track the deployment of the remaining Pods that aren't quite up and running yet in our cluster, specifically our Pod network Pods, or Calico Pods, and also those coredns Pods. A few moments later, we'll find that all of the Pods are now up and running, including our Calico Pod network Pods and also our DNS Pods. We can check the status of our control plane node with kubectl get nodes, so let's go and run that code there, and we'll see that our control plane node, c1‑cp1 has a status of Ready. We can see that its roles are currently control‑plane and master and it's up and running on version v1.20.1. That's the version of Kubernetes that we deployed together. We have a functioning cluster up and running. Now we still have to join some nodes to the cluster, but before we do that, let's check out some systemd units again. In the demonstration where we installed the kubelet, we saw that the kubelet service was crash looping because there were no static Pod manifests for the kubelet to start. We hadn't initialized our cluster yet. Well, now that we have initialized our cluster, let's check out the status of the kubelet's systemd service one last time. And we can do that with sudo systemctl status kubelet.service. And in the output at the bottom there, we can see that the status of the kubelet is now active (running) because it has a static Pods to start up when the service starts. Let's take a peek at those static Pod manifests that live in /etc/kubernetes/manifests. Run that code there, and we can see we have a static Pod manifest for etcd, the apiserver, the controller‑manager, and the scheduler. If we looked inside each one of these static Pod manifests, these are going to describe what that Pod looks like that it needs to start up. And so in this case here, we have the static Pod manifest for etcd.yaml. Looking inside here at this code, we can see the various configuration elements for etcd. Break out of this output and do the same for the static Pod manifest for the API server, and we can see its configuration as well. And so later on in the series of courses, we'll dive deeper into the static Pod manifest configurations for the control plane Pods. One final thing that we're going to look at in this demonstration is where the kubeconfig files live for the control plane Pods on the control plane node, and those are in /etc/kubernetes. And so there we see admin.conf controller‑manager.conf, and scheduler.conf, and we also see the kubeconfig file for the kubelet on the control plane node, kubelet.conf. And so with that demo here, we have a fully functioning up and running cluster with a Pod network deployed. Now join me in the next demo where we'll join a worker node to the cluster for some user workload.

Demo: Adding a Node to Your Cluster
Now it's time to join a worker node to our cluster. You'll find a lot of the steps in this process are very similar to setting up a control plane node, in terms of getting and installing software. The key difference is, once everything is installed and configured, we'll use kubeadm join to join the node to our existing cluster. And so here I have a session open to c1‑node1, and let's get started joining this node to our cluster by ensuring that swap is disabled. And so on line 7 here, I have the code to do just that. Swapoff ‑a, if we get no output, we know the swap is off. And if we need to disable that swap, we'll hop into our fstab, and ensure that our swap entry is either commented out or removed from the file, and there we can see it's commented out. Let's hop back out into our script here, and move forward in configuring this particular node. We're going to start off with installing our container runtime, which is containerd, which is going to require us to load the overlay in br_netfilter modules at runtime here, and also execute this heredoc to make sure that those modules load when the system boots. Moving forward, let's go ahead and create the required sysctl parameters for our container runtime with the heredoc on lines 31 through 35. Run that code there to save that into the sysctl.d directory, so that's available on reboot, and then on line 39 here, we'll execute sysctl ‑‑system to load those settings now in our current running system. With those prerequisites done, we can now install containerd. So I'll issue an apt‑get update for that, and then execute sudo apt‑get install containerd, to install the containerd container runtime. With containerd installed, we'll go ahead and move forward with configuring containerd. So just like we did on the control plane node, we'll need to make a directory for the containerd configuration in /etc/containerd. And then we'll use containerd to generate a default configuration file with sudo containerd/config default, and then pipe that output to tee, and then create the configuration file in /etc/containerd/config.toml. Now we need to make a modification to that configuration file to ensure our container runtime is using the SystemdCgroup driver. I'm going to copy the code from lines 61 and 62, and then hop into the containerd configuration file, config.toml, and we're going to look for that line that ends in containerd.runtimes.runc, and paste in this code in that location. So let's go ahead and find that in our configuration file here. We'll add a new line, and paste our configuration in, and get that in a proper format, and write that configuration file out. And with that config, we will then need to restart containerd with sudo systemctl restart containerd to affect that change, to make sure that we're using the SystemDCgroup driver within containerd. And so now we can install our Kubernetes packages, and we start with adding the gpg key for Google's app repository, and then adding the app repository to our local repositories lists. Once that's added, we'll then update our package list with apt‑get update, and get a quick peek with apt‑cache policy kubelet to see all the different versions of the kubelet that are available. And we're going to go with 1.20.1, which is the version that we used on our control plane node. And with that, we can go ahead and install our Kubernetes packages. So I'm going to set the version environment variable, just like we did previously, and then we're going to install the kubelet, kubeadm, and kubectl, and we're going to pin that to the version 1.20.1‑00, which is the version of the package that we want to install. With the package installation finished, let's use apt‑mark hold to put a hold on the kubelet kubeadm, kubectl, and containerd. So I'll run that code on line 92 here to do that. If you want to install the latest version of Kubernetes, I have the code here on lines 96 and 97 to do just that, but make sure that you match the version that you're using on your control plane node. Moving forward, let's see the status of our kubelet with sudo systemctl status kubelet.service, and we'll find that the kubelet is crash looping right now, because we have not yet joined this node to our cluster. We'll break out of the output at the bottom, and we'll also check the service status for containerd with sudo systemctl status containerd.service. We can see that the containerd service is active and running, and so that's in good shape there to move forward. We'll want to make sure that both the kubelet and containerd are configured to start up when the system reboots, and we can do that with sudo systemctl enable kubelet.service, and we'll do the same for containerd. And so now it's time to join the node to the cluster. We have all of the software installed. We need to join the node to the cluster now, and let's hop back out onto c1‑cp1, to begin that process. To join a node to a cluster, you need both a Bootstrap token, and also the CA cert hash. You could have copied this information from the output of kubeadm init, and used that information, but let's say that you didn't. I'm going to show you where you can find that information in your cluster. First up is the Bootstrap token. The Bootstrap token is a timed ticket and has a 24‑hour lifecycle. So after 24 hours, you have to generate a new one. If we do a kubeadm token list, it will print any active Bootstrap tokens to screen, and you can see here, the only Bootstrap token, and that it has one hour left, as indicated here in the TTL column. If you have no Bootstrap tokens, you can generate a new one with kubeadm token create. And here's our new token in the output here. Again, we see that warning about Docker not being installed. This is safe to ignore, and it will be cleared up in future versions of kubeadm. The other key piece of information that you need to join a node to a cluster is the CA cert hash, and so on line 24 here, I have the code that goes and extracts the CA cert hash from actual CA certificate in /etc/kubernetes/pki. If I run this code here, it'll print that information to the console. So there we it begins with 073. Now we could take all of this information and piece it together, and make the join command with kubeadm joins, specifying the Bootstrap token, and then specifying the CA cert hash, or we could use the command on line 129 here, to generate that join command for us. And so let's do that with kubeadm token create ‑‑print‑join‑command. We run that code, and there at the bottom there, we'll get a well‑formed join command with the proper parameters and their values, including the location of the API server, the Bootstrap token, and the CA cert hash. So let's grab this text and throw that into our clipboard. And so, with that join command in our clipboard, let's hop back onto our individual worker node. So I'm going to SSH back into c1‑node1. Now that I'm on c1‑node1, you can see here, I have the text for the join command. I'm going to take that command that I just copied into my clipboard, and paste that over this existing text, because this is a previous token and a previous CA cert hash. And so let's get that onto our screen here, and we'll go ahead and format that so it's a little more readable. And now, let's walk through this join command together. We have sudo kubadm join, and then the IP address of our API server, 172.16.94.10, on port 6443. We then have the Bootstrap token specified with ‑‑token, and then the Bootstrap token that was generated when we used the print‑join‑command command. We then also have the CA cert hash defined there, so we see that beginning with 0735. And so with that, we can then highlight this code and run it, and that will then join c1‑node1 to the cluster on c1‑cp1. In the output at the bottom there, we could see it's executing the preflight checks, and then it's waiting for the kubelet to perform the TLS Bootstrap process, which we discussed during the presentation portion of the course. And so now, that's all we need to do on the individual worker node. Let's hop back out onto our control plane node, c1‑cp1, and check the status of things. If I do a kubectl get nodes now, we can see c1‑cp1 is up, running, and ready. We see that c1‑node1 is not ready, because what's happening right now is it's deploying the Calico pod network onto that new node that we just added to the cluster, and it's also deploying the kube‑proxy pod onto c1‑node1. If we do a kubectl get pods ‑‑all‑namespaces ‑‑watch, we can see that it's in the midst of deploying those pods onto the new node that we just joined to the cluster. Let's go ahead and break out of there, and get our console back, and now we should see that c1‑node1 is ready, because now the calico pod networking pods are deployed onto the node, as well as the kube‑proxy. And so there we can see that c1‑node1, its status is up and ready, and it's running version v1.20.1. Now we just installed c1‑node1 together. I encourage you, the viewer, to go ahead and install c1‑node2 and c1‑node3 to complete the build of your cluster. Just repeat the process of the script here in updating the node name as you go through the installation process to the node that you're working on.

Managed Cloud Deployment Scenarios: AKS, EKS, and GKE
So far we've discussed manually installing Kubernetes in virtual machines. That technique can be applied to both on‑prem virtual machines or IaaS VMs in the cloud. But what if you want to use them in service? And so I want to take a second here to walk you through some of the managed solutions that are available from the major cloud providers, and first up is Elastic Kubernetes Service, EKS. This is a managed service offering from Amazon Web Services, or AWS. Here is a link to a getting started guide so that you can go ahead and get started using Kubernetes in the cloud as a service right away. Google Kubernetes Engine, or GKE, is available from Google. And so you can go ahead and consume that service if you want to as well. And here is a link to a how‑to document to get you started there. And last but not least, of course, is Azure Kubernetes Service from Microsoft, AKS. Here is a link to a walkthrough document there at the bottom on building your own cluster in AKS. The process for deploying a cluster in a managed solution in any of these cloud providers is effectively the same. Using the command line tooling or web portals, you'll go ahead and authenticate against your cloud provider, you'll deploy a cluster, you'll then download a kubeconfig file to your local machine so that you can use kubectl with a client certificate to authenticate to the API server that's in the cloud for your cluster.

Demo: Creating a Cluster in the Cloud with Azure Kubernetes Service
All right, let's get into a demo. We're going to look at creating a managed service cluster in the cloud in Azure Kubernetes Service. All right, so here we are logged into c1‑cp1, and let's get started with the process of creating an Azure Kubernetes Service cluster. The first thing that we need to do is to ensure that we have the Azure CLI client installed. And so let's go ahead and walk through the steps of doing that together. The first part of that process will be adding the repository to our local repositories lists. And then next we will then add the GPG key for Microsoft's repository to our local keychain. Once we have the repo installed and it's trusted, we'll then update our package metadata with sudo apt‑get update and then install Azure CLI with sudo app‑get install azure‑cli. With Azure CLI installed, it's time to log into our Azure subscription. If you don't have an Azure subscription, I have a link here. We can sign up and get free access to a free Azure subscription. And so to log in, we'll use the command az login and run that code. That's going to then initialize what's called a device login. I'm going to log in to my Azure account off screen, and in a moment I'll get the command line back, and I'll know that I'm authenticated to my Azure subscription. Now that I'm successfully logged in, I'm going to make sure that I'm pointing at the right subscription in Azure. So I'll use az account set ‑‑subscription. I'm going to point it at my Demonstration Account subscription. And so now with the tooling installed and logged into our current Azure subscription, I can then begin the process of deploying an AKS cluster. And that starts off with creating a resource group, which is a way to organize resources in Azure. And we can do that with the command az group create. I'm going to give it a name, Kubernetes‑Cloud, and a location which is an Azure region, and that's going to be centralus for our demo here. So I'll run that code to create my Azure resource group. The provisioningState comes back as Succeeded, so we know that that was successful. Moving forward, let's take a look at the various versions of Kubernetes that are available to me in AKS. And I can find that out with the command az aks get‑versions, and then specify a location as centralus, and modify that output into a table format. And like we discussed in the presentation portion of the course, this kind of goes into your cloud selection process because the versions of Kubernetes that are in the cloud are defined by your cloud provider. And so here in the output, we can see the various versions of Kubernetes that are available for us to consume. And so, after that, it's then time to create our Azure Kubernetes cluster. Now you can do that with the command az aks create, specifying our resource group that we just created, Kubernetes‑Cloud. We're required to have the generate‑ssh‑keys switch. And then I'm going to give that cluster a name, which is CSCluster, and define the node‑count as 3. So, we'll highlight that code and run that code together to start the deployment process of our AKS cluster in the cloud. Now, I'm going to speed up the video here until the deployment is finished. All right, so our deployment's finished. We can see that the provisioningState is Succeeded. Let's move forward in the demo. If we didn't have kubectl installed on the system that we're working on, we can use the command az aks install‑cli to download kubectl and install that on the local system. But we're on c1‑cp1 which already has kubectl installed. And so the next part of the process is going to be getting the credentials from AKS onto our local system so that we can locate and authenticate to our cluster. And we can do that with az aks get‑credentials and then specifying the resource group, which is Kubernetes‑Cloud, and the cluster that we just created together, which is CSCluster. Now we run that code there. That's going to download the kubeconfig file for the cluster and then merge that into our local kubeconfig file in our current user's home directory. And what that will do is give us two different cluster configuration contexts in our local system. And so, to look at those we can use kubectl config get‑contexts. And at the bottom here now we have two cluster contexts available to us. And so we can use a cluster context to tell kubectl which cluster to send commands to. And so here you can see the current cluster context, as indicated by the asterisk, is our AKS cluster. And that second cluster in there is our local kubeadm‑based cluster. And so any commands that we execute right now will be sent to our AKS cluster. That AKS cluster context was added when I downloaded the credential from AKS. If we need to switch a cluster context, I can use the command kubectl config use‑context, and then specify the context name or the cluster name, which is CSCluster for our Azure cloud. But that's already been set when we merged that configuration in. Now, just to make sure that we're pointing at the right cluster, we can use a command like kubectl get nodes because we know that cluster topology is going to be slightly different than what we're pointing at locally. And so, if I do kubectl get nodes, we can see I have three worker nodes in this cluster. What we don't see here in the output is the control plane node. That's abstracted away for us in Azure Kubernetes Service. What we do see here is just the worker nodes that are supporting our user workload. And if we do a kubectl get pods ‑‑all‑namespaces, we also won't see any of the control plane pods, like the API server, etcd, the controller manager, and so on. We do see things like coredns, kube‑proxy, and some additional pods that are used by AKS to report metrics. When we're all done with our AKS cluster and we want to point back to our local kubeadm‑based cluster that we built together, we can switch that cluster context by specifying kubectl config use‑context, and then that context name, which is kubernetes‑admin@kubernetes. So we'll switch back to our local cluster, and to confirm that we'll do a kubectl get nodes. And there we see our local cluster's nodes. We see the control plane node and then the three worker nodes that are a member of that cluster. I also have commented out here the command to delete this AKS cluster if you need to get rid of it from your subscription. And you can do that with az aks delete, specifying the resource group name and then the cluster name.

Module Summary and What's Next!
All right. So here we are at the end of the module, and we certainly covered a lot. We've looked at some installation considerations and what you need to know before you install Kubernetes. Then we looked at an installation overview and talked about where to get Kubernetes and then installed our first cluster together with kubeadm. We then went on and discussed managed cloud solutions and did a deployment in Azure Kubernetes Service. Well I hope you enjoyed that module. Why don't you join me in the next module where we'll start working with our Kubernetes cluster?

Working with Your Kubernetes Cluster
Module Overview
Hello. This is Anthony Nocentino with Centino Systems. Welcome to my course, Kubernetes Installation and Configuration Fundamentals. This module is Working with your Kubernetes Cluster. So let's check out where we've been so far in the course. We started off with the introduction. We looked at what Kubernetes really is in the module, Exploring Kubernetes Architecture, we built our first cluster together in Installing and Configuring Kubernetes. Now it's time to sit down and roll up our sleeves and start working with our Kubernetes cluster. We're going to break down this module into two big chunks. We're going to start off with using kubectl to interact with our cluster, and then we're going to look at some basic application deployments into our cluster.

Introducing and Using kubectl
Kubectl, kube control, or kube cuddle, take your choice, it's the primary CLI tool for controlling workloads in your Kubernetes cluster. Now, what we're going to use is kubectl to perform operations against our cluster. Basically, we're going to create, read, update, or delete pretty much any kind of resource in Kubernetes. Now, remember, in Kubernetes, everything goes through the API server, and so kubectl is your primary way to interact with the API server. And so any time you need to make a new thing or query something that exists or make a modification to something, this is the primary CLI tool for doing that. You're going to perform operations on resources, the Kubernetes API objects. Using kubectl is how you're going to manipulate objects like Pods, deployments, services, and others. And so we're going to perform operations against resources. And then finally, the other facet that we're going to look at today in this module is output. If there's output from the commands that we're executing, then we can define the format that we're going to get that output in. And so perhaps we want to have more detailed output with additional attributes exposed or we want to print output as a particular type like JSON or YAML. We can do this all with kubectl. And we're going to start our deep dive into how kubectl works with the operations that you can perform, or really, what do you want to do? And so let's jump right in and talk about the core operations that you'll likely use at the command line with kubectl every day. First up is apply or create, Apply and create are the primary operations for sending deployments and the creation of resources to the API server. Run allows you to start a single or bare Pod when it's not managed by a controller and then also specifying the container image at the command line, so basically, bootstrapping the most basic Pod configuration. One of my favorite commands is explain. This gives you the documentation for API resources, and what this does is it shows you the documentation for a particular Kubernetes API object or resource listing the description and the fields needed to construct that API object. This is a very valuable command when you're working at the command line. Kubectl combined with delete will delete a specified resource. And then there's kubectl get. What kubectl get will do is display basic information about the specified resource type. And so far in this course, we've looked at nodes and Pods in our cluster, and we're going to use this plenty more as we get into more advanced topics. Next, let's look at describe. Kubectl describe is used to display very detailed information about a particular resource, and it is extremely valuable in troubleshooting scenarios with Kubernetes resources, and we'll look at it in great detail in an upcoming demo. Next up is kubectl exec. Exec allows you to run a command inside a container on a Pod, and so this is very similar to docker exec. And then finally, kubectl logs. Kubectl logs allows you to view the logs that are written to stdout from a container running inside of a Pod. And so this is very valuable for troubleshooting issues with your applications that are running inside of containers inside of Pods. Now this is a short list of what I think are the most critical kubectl operations to get you started. There is a huge list available to you here at the documentation page for kubectl on the Kubernetes website, and I strongly encourage you to check that out there. When working at the command line, we're going to combine kubectl with an operation like the ones that we just introduced with a resource. Basically, what do you want to perform that operation against? And we've introduced things like nodes and Pods and services. And honestly, there are many, many more objects available inside of Kubernetes that we can work with, and so that's how we go ahead and specify what type of resource we want to perform the operation against. Now, here you can see in parentheses an alias to represent that particular type of resource, so nodes, no, Pods, po, services, svc, and that's a good way so that we can get real quick at the command line executing these commands. Now there's a huge list of resources that are available, and you can certainly check that out at that link there, but I'm going to show you some techniques at the command line where you can discover the resource names and the resource aliases because I want you to get very proficient at the command line without having to refer back to the documentation to get things done, and we'll do that in an upcoming demonstration together. The final thing that I want to look at with kubectl today is modifying output. We can specify kubectl's output format by adding additional flags to our commands. The first format that I want to introduce you to today is wide. Using the wide option, we can specify that we want kubectl to output additional information about our Kubernetes objects that have been deployed in our cluster. We can also output our Kubernetes objects as YAML or JSON. YAML and JSON formats are at the core of how Kubernetes describes things declaratively, giving us the ability to describe our configurations in code. We can use kubectl to output YAML or JSON, and this is a very valuable way to get configuration data out of our cluster and describe the resources that have been deployed in our cluster. We can persist this to file and exchange it with other systems, down‑level environments, or developers, if we need to. One final option that I want to introduce you to today here is a dry‑run. When combined with the yaml output modifier, you can use this to generate YAML for resources that you want to create in your cluster. But dry‑run doesn't create the object in the API server; it just out puts the YAML for the object. And so this is a great tool for quickly generating YAML for resources that you want to create, so things like deployments, services, and more, and we'll see this in action later in the module. If you want to dive deeper into how the output options work, check out this link here.

A Closer Look at kubectl
So let's bring all that together and learn how we can use kubectl at the command line. When we're sitting at the command line, we use the command kubectl, we'll specify a command, in other words, an operation, right, that's the thing that we want to do. We'll specify a type or a resource, right, what do we want to do it to, and then, if we need to, we can specify an individual named object. And of course, there's the optional flags that we can append to any command, and so let's look at one command together. If we do something like kubectl get, right, that's the operation, pods, that's the resource, and what if we needed to get the information about a particular pod? Well, we would just append that on. So, kubectl get pods would list all of the pods in the default namespace. Kubectl get pods pod1 will give me the information about just that particular pod, and then I can append on some optional flags. What if I wanted to get that pod's information output as YAML? I would append ‑‑output=yaml to get that info. One other command I want to show you guys is kubectl create, right, again, the operation, what I want to create, the deployment, that's the resource. If I need to give it a name, of course I would, nginx, and then I specify an image name, and that's how we could do a basic deployment at the command line using kubectl. Now, I'm going to throw two links at you here, one for the reference documentation and one for a cheat sheet for using kubectl, and I strongly encourage you to check these out. They're fantastic resources, and you might find a small piece of information in there that can help you get through a scenario more efficiently or easily than you're doing it today. And so, go ahead and check out those two links, very good resources.

Demo: Using kubectl: Nodes, Pods, API Resources and bash Auto-Completion
So let's get into a demo where we're going to look at using kubectl. We're going to use kubectl and work with some nodes, pods, and other API resources, and I'm going to throw in a treat here for you. I'm going to show you how to configure bash auto‑completion so you don't have to remember all the syntax and shortcuts for kubectl or even resource names. Alright, so here we are logged into c1‑cp1. Let's get started with working with our Kubernetes cluster using kubectl. The first command that I want to show you here is kubectl cluster‑info, and this is useful for listing and inspecting which cluster you're pointing at in your current context. So I'm going to highlight that code there and run kubectl cluster‑info. And at the bottom here, we can see it says Kubernetes control plane is running at https://172.16.94.10 on port 6443. That is the local API server running on c1‑cp1 in our kubeadm‑based cluster. Now one of the most common operations that you'll use with kubectl is get, and so let's do that together with kubectl get nodes. And what that will do is then print out some critical information about the resource. And in this case, that's going to be a node. And at the bottom there, let's walk through this output together. We have a row of information for each node in our cluster, and let's walk through each one. We have c1‑cp1. We see its status is ready, so it's able to take on workload. In this case, on our control plane node, that's going to be control plane pods. We see its role is currently control plane and master. It's been up for about 24 hours, and the version 1.20.1. We see additional rows. For c1‑node1, 2, and 3, and those all have a status of ready, meaning that they can take user workload. Now, we can add the output modifier, ‑o wide, to get additional information about a resource. And so, in this case here, we're going to say kubectl get nodes ‑o wide, and what that will do is give me additional information about the resource, in this case, our nodes. And so, in addition to NAME, STATUS, ROLES, AGE, and VERSION, we also have additional fields. So we have the INTERNAL‑IP address of the node, so there we see 172.16.94.10 for c1‑cp1. In some cloud scenarios, we'll see the external IP populated. Since we're doing this on‑prem, that's set to none. We have our OS‑IMAGE, so that's going to be Ubuntu 18.04. The kernel that we're running and wrapping off the end of the screen there. We can also see the information about our container runtime. And in our lab here, that's containerd version 1.3.3, and we have a row of information for each node in our cluster. So let's look at the pods that are currently running in our cluster. And if I do kubectl get pods, we get the answer of No resources found in default namespace. Now remember, a namespace is a way to organize resources together. And when we run kubectl get pods, that's going to point at the default namespace. And, well, there's no workload up and running in the cluster yet, so we have no resources found in the default namespace. But there are some pods that are up and running in what's called the kube‑system namespace, and that's where the system pods were run in our cluster. And so I can say, kubectl get pods ‑‑namespace and then specify kube‑system. And then I can see all of the system pods that are running, including the control plane pods, our pod networking pods, and our DNS pods. And so in the output at the bottom here, we see the name of the pod, and then next, we see ready, which tells us if the containers defined in the pod are up and running. Then after that, we have status, which tells us the current state of the pod. Earlier, in a previous demo, when we deployed our pod network, we saw the statuses container creating and pod initializing based on the deployment state of the pod at that point in time and then a transition to running once everything was up running and ready. Next we see restarts, which is the number of times a container restarted inside of a pod, and that that pod was defined about 25 hours ago. We could also combine kubectl get pods with ‑o wide, and so we can get additional information about a pod. And so let's do that for our pods in our system namespace, so kubectl get pods ‑‑namespace kube‑system ‑o wide. Run that code together, and we'll get additional information about a pod. And so we see NAME, READY, STATUS, RESTARTS, and AGE, which is the regular information that we just walked through. Well, we have additional information now. We have IP, NODE, NOMINATED NODE, and READINESS GATES. Let's look closely at IP and NODE. So on the IPs, we see that some of the pods are on the pod network 192.168, and some pods are on our virtual machines network 172.16.94. Depending on the role that those pods play in our cluster, that's what network they'll be attached to. So, for example, our DNS pods will be servicing DNS requests inside the cluster on the pod networks, so those are deployed on the pod network 192.168.00, which we defined in our earlier module when we created our pod network together. Some other pods are on the actual network that our infrastructure is on, so 172.16.94. And so those are exposing services outside of the cluster. So, for example, the API server is available outside of the cluster. It's going to be listening on 172.16.94.10, which is the real address of the control plane node. Additionally, we see four kube‑proxy pods up and running. There's a kube‑proxy pod running on each node in a cluster. Now recall, kube‑proxy has the responsibility of implementing service networking on each individual node. And so there will be a kube‑proxy pod on each individual node. So there we see a kube‑proxy pod running on c1‑cp1, c1‑node1, c1‑node2, and c1‑node3 on the real network IP address 172.16.94. They're exposed to the real network so that they can receive those requests coming in from outside of the cluster and route that information to the correct services and pods running inside the cluster. Now, we can also combine kubectl with get all. And what get all will do is list all current resources that are running in a cluster, and I can also combine that with ‑‑all‑namespaces. And what that will do is give me every resource that's up and running in my cluster across all namespaces. And so this is a valuable command. They give you a quick view of what's going on in your entire cluster's space. And so the first part of the output here is pods, and we just walked through that together. And so let's skip forward into the remainder of the output, and we'll see some other API resources defined in our cluster. We can see some services, daemon sets, deployments and replica sets are defined. We'll be diving into each of these in much more detail in some upcoming courses. But for now, the key concept that I want to cover here is using kubectl to display all of the resources that are defined in our cluster. Now, moving forward, let's ask the Kubernetes API server in our cluster about all the types of API objects that it knows about, and I can do that with the command kubectl api‑resources. And I'm going to pipe that output into more because there is a large collection of API objects available for us to work with. Now in the output here, we see things like the name of the API object. We see short names or aliases. And so this is the way that we can address a particular object at the command line if we need to. So, for example, if you want to address nodes rather than typing the entire word nodes, we can type the alias or the short name no. After the short name, we see the API version, which is a way of grouping and versioning resources in the API, so we see all of those are on v1. We also can see if an object is namespace or not. So there we see true or false, depending on if that particular object can be in a namespace or not. And then the object KIND at the end in that last column there, we see the different object names. And so, if we need to work with an alias, let's get a quick, simple demonstration of that, kubectl get no, and I'll still get that same output as if I typed out nodes. And so, at the command line, you'll get used to using different various aliases, depending on what your favorites are. Now within that huge list of API resources, one of the quick ways to be able to filter down to find what you want is to type that output in the grep. And so here, we're going to look for the string pods from the output of kubectl api‑resources. And again, that's a quick way for me to kind of pare down that list to discover an API resource that I might want to work with. Now if I need to know more about a particular API resource, that's where the command explain comes in, and this becomes one of my more useful commands at the command line when I'm building workloads and constructing workloads. And so let's look at kubectl explain, and we're going to look at the object pod. Now we can put any object type in here from that listing of API objects that we just went through, but we're going to walk through the pod example together. So kubectl explain pod, and I'll pipe that into more. What explain gives you is the documentation about that API object and so that we can learn little bit more about what it takes to construct this type of object. And so here we see the KIND, we see the VERSION, so KIND is Pod, VERSION is v1. This is a v1 pod. We see a description that tells us what it is that we're working with. So a pod is a collection of containers that can run on a host. Then it goes down to the actual fields that are required to construct a pod. And so if we needed to deploy a pod, this is what will be required, the apiVersion, the kind, a metadata, and a spec. If I wanted to dive a little bit deeper to learn about what in the spec is required to describe a pod, well, let's look at that. I can do kubectl explain pod.spec and dive deeper into the description of a pod and learn about what I need to specify when I need to describe a pod in code. And so this, again, is useful for discovering those various attributes. And so if I go down in the output here, you can see that one of the required fields is a container, which makes sense because a pod runs containers, and that's a very useful way to discover how to build that. If I want to even go deeper, I can say kubectl explain pod.spec.containers and run that and dive deeper into that object. And here would be the fields used to describe a particular container. If I go down in this output here at the bottom, we can see things like an image, which makes sense. I'm going to run an image inside of a container to define what container image I want to be started up inside of a pod. So useful information there, again, diving into the documentation to learn about these things at the command line quickly without having to go look them up either on the web or using some other resources. Now this command, kubectl explain pod ‑‑recursive, is a valuable command because what it will do is it will output all of the fields that are part of an API object and march down recursively through all of the fields available in the API object, in this case, a pod. And so what this will do is it will give you the output of the fields for the particular API object, but without the description. So if, perhaps, you forgot a particular field name, this is a valuable way to go and just retrieve that information quickly. So let's break out of that output here and look at one of my other favorite commands that I use very, very frequently at the command line when I'm working with resources defined in my cluster, and that's kubectl describe. And so on line 60 here, I have kubectl describe. I'm going to describe what a node and then a particular node, c1‑cp1. Now, I'm describing a node here, but we could describe any other resource. It could be a pod, it could be a service. We're going to focus on nodes at this point in time. Kubectl describe nodes c1‑cp1. Let's go and pipe that into more and walk through this output together. So, describe gives you some very detailed information about an API object, and this is extremely valuable when it comes to troubleshooting things that are running in your cluster. And so in the output at the bottom here, we see the name of the resource, in this case, it's a node, which is c1‑cp1. The Roles, control‑plane, master, and we have some labels and annotations. Labels and annotations are a way for Kubernetes to track and monitor objects that are running in the cluster, and we'll look at those in much more detail in an upcoming course. In addition to that information, we have the creation timestamp when this API object was created. There we see the taint associated with the control plane node. That's going to have the taint of NoSchedule. This is the taint that prevents user pods from running on this node and allows only system pods to run on this node. Moving forward in the output, we see conditions, which describe the current state of the node in terms of things like network availability, MemoryPressure, DiskPressure, and PIDPressure. Moving forward in the output, we also see things like addresses. There's internal IP and then a hostname, the capacity of the node, so the amount of CPU that it's contributing to the cluster, its storage, memory, and so on. We have some additional system information with regards to things like the kernel version, OS image, the operating system that it's running, CPU architecture, and much more, so very valuable deep‑dive information about that. We have the current pods that are up and running on the node, and so there we can see the collection of system pods that are running on the control plane node. And so that was kubectl describe. For our control plane node c1‑cp1, we can do the same for c1‑node1 and dive deeper into this particular node's configuration at the command line here. So there we can see things like its conditions and its status, if it's up and ready, and all of the resources that are available on it, and so on. So at the command line, what we have the ability to do is interact with the API server. And one of the things that we want to be able to do when we're working with Kubernetes is to do these things quickly and discover these things at the command line, so that's one of the reasons why we walk through how to retrieve API objects and their documentation because I want to be able to do that quickly at the command line. Similarly, if I'm working with kubectl, I can just ask kubectl for help, and I can do that with kubectl ‑h. And then in the output, it has a well‑formed organized method of exposing the various operations that you can perform. So there we see basic commands like create, expose, and run, and then going down into more advanced commands for things like deployments and also cluster management. So very valuable information is available in the help. In addition to the basic output right off of kubectl ‑h, I can combine that with an operation, so kubectl get ‑h, and then I can get more detailed information about a particular operation that I want to execute. In this case, it's get. And so looking at this, in addition to the normal help that you would see, we also get a collection of very valuable examples. These examples here are very useful for helping you remember more advanced command lines and texts. The final operation that I want to call out here in the help is kubectl create. This is a command used to create resources in the cluster imperatively, and it's something that we'll be doing very frequently. And as you're working through future demos, be sure to remember that this is here for you as a resource. And now, last in the demo, but certainly not least, here's that treat that I want to share with you. I want to show you that enable bash auto completion for kubectl. And so, let's walk through that process together. On line 71, I have apt‑get install ‑y bash‑completion, and that's going to install bash completion on our system. And on my particular system here, we can see that it was already installed. I'm then going to, on line 72, echo in some configuration information into my local bashrc. I'm going to reread that with the source command, and then I'm going to show you at the command line at the bottom here what bash auto completion provides. Now, normal bash completion would do something like this where it would complete the command based on what you've typed at the command line. So I type kube, I hit double tab, I get the auto completion of the three commands that match that string. In that case, kubectl is the one that we want. Now, to extend that, we get auto completion in kubectl for operations. So here you can see if I type g and hit double tab now, it auto completes to get. In addition to operations, it also does that for resources. So if I type po and then double tab, you can see it'll auto complete the pod. If I double tab again, you'll see it auto completes to all of the different resources that are available in my API server that match that string, in this case, pods. And so we'll go ahead type pods there. Also, I can then add something like ‑‑ to see any of the different modifiers that are available to me at the command line, so if I double tab now, we can see all the various different modifiers that are available to me at this point in the command line string that I'm building. Let's go ahead and type something like all, and then I'll do a double tab on that, and we can see it's going to limit the list of modifiers down to the modifiers that match the string ‑‑all. And so I'll go ahead and auto complete that with ‑namespaces. Execute that code there. We can see how we can use that to quickly work at the command line to execute commands and discover commands and discover resources that are available in our cluster.

Application and Pod Deployment in Kubernetes and Working with YAML Manifests
So, now that we know how to interact with our cluster at the command line, let's move the conversation to application deployment in Kubernetes. Now, the first thing I want to discuss is imperative configuration. When you're using imperative configuration, you're generally going to be executing commands at the command line one at a time, and you're going to be operating on one object at a time. So, for example, if I want to create a deployment, I can use kubectl create deployment nginx, and specify the image as nginx. I punch this in at the command line, I hit Enter, the command is sent to the API server, and the object is created, but I'm only operating on one object at a time on the command line. Now, similarly, I can operate on other types of objects certainly. I can say kubectl run nginx, and it will create a pod for me running nginx. That's a fine way to manage a system, but if your application stack starts to grow and your configurations and your deployments become more complex, managing each individual object at the command line isn't really a sustainable way to manage or maintain your system. Were going to want to do things declaratively, and this is a core principle behind kubernetes, where we define our desired state in code. Earlier in the course, we introduced the concept of manifests, first when we deployed our pod network, and second when we bootstrapped our cluster with static pod manifests. Those manifests described those configurations in code, and Kubernetes was able to use them and bring the system up to the desired state as described in those manifest files. And we can do the same for our own applications. We can define our configurations in code using manifests written in YAML or JSON, and feed those into the API server with commands like kubectl apply. In this case here, you can see kubectl apply ‑f deployment.yaml, and the contents of deployment.yaml will have the description of the thing that I want to deploy inside of Kubernetes. Let's look at our first manifest together, and we're going to look at a deployment manifest. We could have manifests for many different types of API objects available in Kubernetes, but we're going to start our conversation off with deployments. The first thing that you're going to find in any manifest is the apiVersion, and in this case here, since we're using a deployment, it's going to be apps/v1. As the Kubernetes API develops and changes to give users stability as to how API objects are defined and behave, it's versioned. This way, we know a v1 deployment will always look and behave a particular way when we call that in our code. If a newer version of an API is released, and the API objects either change in definition or behavior, that can be a breaking change, and so in our manifest we specify the API version, and this gives us control over which version of an API object we're consuming, and thus adding stability to the objects defined in our manifests. The next thing that you're going to find in a manifest is kind, or the kind of object that we want to define. In this case, we're defining a deployment. Remember in an earlier demo when we did kubectl api resources to list all of the API resources available, those are the objects that we can use here. We'll need some metadata to describe what we're working with here, and in our case, we'll just give our deployment a simple name, and we're going to call it hello‑world. Next is the spec. This defines the implementation details of the deployment object. And the first thing that we're going to define is the number of replicas. This is the number of pods that we want up and running for this deployment, and we're going to start off with 1. The selector is a way for a deployment to know which pods are a member of this deployment, and we'll be diving into labels and selectors in much more detail in an upcoming course. And then there's the template. This is the section that's used to define the pods created by this deployment. You'll also hear this called the pod template. In here, we'll have another metadata section, and we'll define some labels. These are matched with a selector in the deployment spec above, and these are assigned to each pod created by this deployment, and that's how the deployment is able to track which pods are a member of this deployment. And then finally, there's another spec. Here's where we will define the containers started by the pods in this deployment, and we define an image to do that. Here we're going to be using a simple hello‑world application from the Google Container Registry, and we'll also need to give this container a name, and we're going to call it hello‑app. Now, to deploy this deployment, we can save this information into a file named deployment.yaml, and then we'll use kubectl apply ‑f, and then pass in that file name as a parameter. That will then read the file and feed that manifest into the API server, and then the API server will go and make that happen for us, affecting the desired state of the application in the cluster, and in this case, it's starting up one replica of our hello‑world application pod. Now you might be thinking, how am I going to remember all of this? Well, to build YAML manifests for deployments, or really any objects that I want to work with, well, remember in the last demo when we talked about kubectl explain, you can use kubectl explain to quickly look up the fields for an object to help you fill out the implementation details for your manifests. Additionally, we had previously discussed the dry‑run output modifier. We can use that to generate basic manifests like this very quickly. Let's look at dry‑run in more detail now. Now, we can certainly write the YAML for our deployment manifest by hand and ensure that we get all of the fields and all the spacing right, but what I want to show you now is a way to generate the YAML needed to create a deployment, or really any API object, on the fly at the command line quickly and correctly. A few slides back, I showed you how to create a deployment imperatively at the command line. We can take that code and its parameters and combine it with ‑‑dry‑run=client ‑o yaml, and what this will do is generate the YAML for the API object that you want to create, but it won't send it to the API server for creation. What dry‑run client ‑o yaml will do is write the object YAML to stdout. We can combine that with file redirection and write the output into a file, and so here you can see we're sending the output into a file named deployment.yaml. With this YAML written into the deployment.yaml file, we can use kubectl to send that deployment into our API server for creation or use this output as a starting template for a more complex deployment scenario. I can't stress enough how helpful the dry‑run parameter can be to quickly generate correct YAML representation of objects, nearly any object, not just deployments. So I encourage you to use it frequently at the command line to help you get proficient at generating manifests quickly and correctly at the command line. Now it's time to discuss what the API server is actually doing for us when creating a deployment, and we're going to discuss the application deployment process in Kubernetes. And at this point, we're going to bring together a lot of the theory and concepts that we've been going through throughout this course, and so really, this is the time when we're going to bring all of those elements together. And so let's say we have a cluster and we're sitting at the command line with kubectl and we want to deploy an application into Kubernetes and we say kubectl apply. We then pass in some sort of manifest describing the objects that we want to create. So let's say we want to create a deployment. That deployment will create a replica set, and that replicas set will create our pods based on the pod spec in the template, and kubectl is going to send our request into the API server. The API server is going to parse that information defined in the manifest and store those objects persistently in etcd. The controller manager is watching for any new objects that it needs to know about. Since we define a deployment, it's going to start up a controller for that deployment, and that will create a replica set. That replica set is going to create the required number of pods to support the configuration and write that information about the pods back to etcd. Now, the scheduler is watching etcd to see if etcd has any pods that haven't been assigned to nodes yet, and if it finds any unscheduled pods, it will schedule, or in other words, assign a pod to run on a particular node in a cluster. Each pod object is updated in etcd with the assigned node that it needs to run on. Now, no pods have started yet. What we have is the objects for the deployment, the replica set, and the pods with their scheduled node information all written into etcd. So how do the pods actually start? Well, the kubelets on the nodes are watching the API server asking, do you have any work? Do you have any work? And if it finds a pod scheduled for that node, it's then going to send a message to the container runtime on that node to pull down the appropriate container images specified in that pod spec, and start the pod up on that node. If that pod is a member of a service, then that service's information is updated in kube‑proxy on that node. This entire process is how pods get deployed inside of Kubernetes. In this example, we described a deployment. Similar processes exist for deploying other types of objects in Kubernetes where the workflow might vary slightly. I just wanted to highlight this example for you today to connect the dots of all the various things that we've discussed throughout this course.

Demo: Imperative Deployments and Working with Resources in Your Cluster
Alright, so let's get into a demo, and in this demo, we're going to deploy some resources using two different techniques. We're going to look at things imperatively and declaratively. Using those techniques, we'll learn how to deploy resources into our cluster, focusing on deploying deployments, pods, and services, and once we have things up and running, we'll look at how we can make changes to existing resources in our cluster, using both declarative and imperative techniques. So here we are, logged into c1‑cp1, and let's begin the process of deploying resources imperatively in our cluster. What we're going to do first here is to create a deployment together, and I have the code on line 8 to do just that. And so on line 8, we see kubectl create. What do we want to create? A deployment. where we'd give that deployment a name, hello‑world. And then we're going to specify the container image that we want to run in the deployment with the parameter, ‑‑image. And so here for the image, we're using a simple hello‑world app container image from Google Container Registry, hello‑app, with a tag of 1.0. And so when I highlight that code and run that there, and at the bottom we can see deployment.apps/hello‑world created. And so what this code is going to do is create a deployment, which will create a ReplicaSet with one pod in it. So it creates a one‑replica deployment. Moving forward, let's go ahead and create a bare pod, or a pod that's not managed by a controller. And to create a bare pod, we use the command kubectl run, and then we're going to give it a name, hello‑world‑pod, and then we need to specify the container image that we want to run inside of that pod, and we do that again with the parameter, ‑‑image, and we're going to use our simple hello‑world application again for this pod. We'll run that code, and we can see here at the bottom, pod/hello‑world‑pod created. And so let's check out the status of our deployment, and our bare pod, and I want to see if both of those pods are up and running, and I can do that with kubectl get pods. In the listing here, I have our pod that's associated with our deployment, that's the first one in the list there, and then our bare pod, which is hello‑world‑pod, which is the second one in the list there. We can see that both of those pods' status is running. Looking at the pod that's associated with our deployment, we can see in the name of the pod, the name of the deployment, which is hello‑world, and then we see the string, 5457b44555. That's what's called the pod template hash, and is unique amongst ReplicaSets within a deployment. The last part of the pod name there is a unique identifier for a pod within a ReplicaSet, so gnxsk is a unique name. And so all of that together, with the deployment name, hello‑world, the pod template hash, 5457b, and so on, plus the pod unique identifier, gnxsk, will give us the unique value for the pod name. So let's look at these pods from a different angle. Let's use kubectl get pods ‑o wide, to display some additional information about the pods that we have up and running, and I want to zoom in on IP and NODE. So we can see both of these pods have IPs that have been allocated from the pod network, 192.168.0.0/16. We can also see the nodes that these individual pods have been scheduled to. So the pod associated with our deployment, the first one there, was scheduled to c1‑node3, and then our bare pod was scheduled to c1‑node2. Now I want to point out that Kubernetes is a container orchestrator. It has the job of starting containers on nodes, and we can see that we have two pods up and running on two different nodes, and while those pods on those nodes started containers. And so what I want to do now is I want to SSH into an individual node, and show you how you can view the containers running on that node, even the ones that have been started by Kubernetes. And so we're going to go ahead and open an SSH connection into c1‑node3, so I want to copy and paste this code from the top, down to the bottom, and SSH into c1‑node3. That's the node where our pod that's running from our deployment, was scheduled to and is now running on. The way that we can get a listing of the containers running on a node that are running with container D is to use the utility called crictl. And so I have the code to do just that on line 30 here. So we have sudo crictl, and then the parameter, ‑‑runtime‑endpoint. And then we specify the cri socket for containerd, and then the command that we want to run there at the end, ps. And so if I highlight that code and run it, we can see that there are three containers running on this node in our cluster, the first one being our hello‑app, which is part of our deployment. There are two other containers running on this node, one for our pod network, there we see calico‑node, and we also see a container named kube‑proxy, which is supporting our kube proxy pod, which is also running on this node. If you are still running Docker, here is the command to do just the same. If you aren't running containerd, you can do sudo docker ps, to get a listing of the containers that are running on your node. So let's go ahead and exit back out onto c1‑cp1, and look at things from a couple other different angles. I want to show you some troubleshooting techniques that might be useful for working with your pods, first up, using kubectl logs. And so on line 41 here, I have the command kubectl logs hello‑world‑pod. We can use the command kubectl logs, which is very valuable to retrieve the logs from a container running inside of a pod in our cluster. And so any information written to standard out will be captured and available to you via kubectl logs. And so this is valuable when you have an application that's in trouble, or crashing, or a pod that won't start, so a very useful troubleshooting technique here. And so for our scenario, I have kubectl logs, and then a pod name. In this case, it's going to be hello‑world‑pod. So if I highlight that code, and run that, we can see the log from our container inside of our pod. In this case here, we have just one entry, which includes a date and timestamp, and then a string that says server listening on port 8080. Moving forward. I also want to show you that we can attach a shell to a running pod. And to do that, we can use a technique called kubectl exec. And I want to point out that you can use kubectl exec to start a process inside of a container, inside of a pod, and you can use this to launch any process, as long as that executable is available inside the container. In this scenario, we're going to attach a shell, so that I can show you how to have a shell to a container running inside of a pod. And so let's walk through that technique together. On line 47, I have kubectl exec ‑it. The parameter ‑it will allow you to attach an interactive terminal. We then specify the name of the pod that we want to attach it to. In this case, it's hello‑world‑pod. Then we have space minus minus, and then a space, which is a delimiter, and then after that, you specify the command that you want to run, in this case, /bin/sh, which is a shell. And so when I run that code, it's going to give me an interactive shell to the container running inside of that pod, and at the bottom here, you can see I have a root shell open. We can now execute any command that exists inside of the container. And so the first thing I want to show you is hostname. When I execute hostname, it'll print out the hostname of the pod stdout. And so here you can see the hostname, which by default will match the pod name, in this case, it's going to be hello‑world‑pod. If I do ip space addr, I can look at the network configuration of this individual container, running inside of this pod. Then we can see in the output, the IP of this pod is 192.168.131.62. When we're all done, we can use exit to exit out of the container, and get back onto our localhost, which is c1‑cp1. Now, remember that first kubectl create deployment command that we executed in this demo? What that did is it created a deployment for us, which then created a ReplicaSet, which then created a pod, and I want to show you how that all pieces together here inside of our cluster. And so we'll first off do kubectl get deployment hello‑world, and that's going to list the information about our deployment, hello‑world. And so at the bottom here, we see the name, hello‑world, we see that one of one pods is ready, and that our deployment was created just under 9 minutes ago. If I do a kubectl get replicaset, we can see the ReplicaSet supporting the deployment. So there we see hello‑world, and then the pod template hash, 5457b, and so on. We also can see the desired state and current state, and that one pod is up running, and ready, in our ReplicaSet. Now, if I do a kubectl get pods, we can see the pods that are up and running in the cluster, and now you can see where that name is built from, the deployment name, hello‑world, then the pod template hash, and then that pod's unique identifier, which is gnxsk. So let's look a little more closely at the deployment, at its ReplicaSet, and its pods, and we'll use one of my favorite Kubernetes commands, kubectl describe, to do that. And so on line 64 here, I have kubectl describe deployment hello‑world. I'm going to pipe that output into more, and here at the bottom we can see the the output of kubectl describe for our deployment. So we can see the name, hello‑world, and the namespace default, and that it was created just a few minutes ago. Moving forward in this output, here we see pod template, which is the pod template for each of the pods that is created by this deployment. So inside of there, we can see containers, hello‑app, and then the container image that we're working with, our hello‑app from gcr. Going down to Events, we can see in Events that this deployment has an event that says ScalingReplicaSet, because the deployment creates the ReplicaSet, and then a ReplicaSet will go on and create the pods. So in the Events output at the bottom, we see ScalingReplicaSet from deployment‑controller, Scaled up ReplicaSet hello‑world‑5457b44555 to 1. So that's creating that ReplicaSet, which will in turn create that one pod for us. Let's go ahead and look more closely at the ReplicaSet, and we can do that with kubectl describe replicaset hello‑world. In this output here, we see the name, hello‑world, and then the pod template hash. In Replicas, we can see the current state is 1, and desired is 1, and right below that, we see Pod Status one 1 is running. So we know currently we are in the desired state for this ReplicaSet. Moving forward, we see the pod template for this ReplicaSet. This matches the current state that is in our deployment. So there we see our containers, hello‑app, running the container image from gcr, hello‑app with the tag of 1.0. In the Events section for the ReplicaSet, we can see an event that created a pod, and so let's look at that output. We see SuccessfulCreate from replicaset‑controller, Created pod: hello‑world, and then our pod template hash, and then the pod identifier ending in gnxsk. Now let's go a level deeper and get kubectl describe information about our running pod. I want to show you a technique of how we can very quickly auto‑complete a pod name at the command line with Bash auto‑completion. So I'm going to take the highlighted text here on line 75, and thread it into my clipboard, and in the command line at the bottom, I'm going to paste that, and here we have kubectl describe pod hello‑world and a dash. And so if I do a double tab now, it'll try to auto‑complete to any pod that has the name that begins with hello‑world‑. And so we have two, right? We have the one associated with our deployment, and our bare pod, and so I'll just type a 5 there to give it enough information to auto‑complete to the full pod name. By hitting tab again, we can see it auto‑completes the full pod name for us. And so we have kubectl describe pod, and then our pod name, hello‑world‑5457b, and so on. I'm going to pipe that output into more, and here we can see the information associated with an individual pod. So there we have the pod name. We also can see the node that the pod is running on, so c1‑node3, and its node IP address. Going down a little bit further, we can see the pod IP address. There we have 192.168.206.127. We can see that this pod is controlled by the ReplicaSet, hello‑world‑5457b, and so on, and then we can see the runtime information about the container, running inside of this pod. Going down a little bit further, I want to jump to the Events section, and in the Events section, we have some really good information about the lifecycle of this pod, and so let's walk through each one of these records. First up, we see Scheduled from the default‑scheduler, and the message is, Successfully assigned default, which is the namespace/, and then that's our pod name, hello‑world‑5457b, ending in gnxsk to c1‑node3. So that's when the scheduling decision was made, and that's part of this pod's events. On the next line, we see Container image, and there's our container image, hello‑app, with the tag of 1.0. We can see that it was already present on this machine, and so we didn't have to re‑pull that down from the container registry; it already existed. So then we can jump right to creating the container image, and that's the third event in our output here, Created container hello‑app. The last record we have in the Events is, Started container hello‑app. And so that process of scheduling, pulling, creating, and starting, is all part of this pod's lifecycle. If you create a deployment and your pods don't come up, use kubectl describe on either the deployment, the ReplicaSet, or one of the pods in your deployment, and check out the events for those resources, looking for any events or errors that can help you understand what went wrong. This is my go‑to place to help me troubleshoot deployment failures and pods failing to start up. Now, if you want to dive deeper into deployments, check out my course, Managing Kubernetes Controllers and Deployments, coming up later in this path. I have a link there for that. In that course, we'll talk about rollouts, controlling rollouts, rollbacks, and updating our application, so deep‑dive stuff covered in that course.

Demo: Exposing and Accessing Services in Your Cluster
All right, so let's move forward into the next part of our demo where we're going to learn how to expose our deployment as a service and so we can access our application inside of our cluster. Now, to create a service for our deployment, we can do that with the code on line 89 here, kubectl expose deployment, and then the deployment name, hello‑world \. The ‑‑port parameter is the port that the service is going to listen on. In this case, it's going to be on port 80. We then have a parameter target port, which is the port that our Pod is listening on. So we saw earlier that our application is listening on port 8080, so I'm going to highlight this code and run that to create the service for this particular deployment. And at the bottom, we can see that service hello‑world is exposed. And so let's look at the information associated with that service, and we can do that with kubectl get service, specifying the service name hello‑world. At the bottom here in the output, we can see the name of the service, the type is ClusterIP, and then the IP address associated with the service, or this is where we're going to send user traffic to is 10.100.236.189. There's no external IP associated with this. In a cloud scenario, you might see a public IP address there. The port that this service is listening on is port 80 on TCP. Let's look at this from another angle using kubectl describe, with kubectl describe service hello‑world, and let's walk through this output together. We see a lot of the same information that we saw just a second ago, so we have the name, the type, and the IP address of the service. We see the port that we're listening on and also the target port. What I want to call out here is the Endpoints. Endpoints are the IP and port pairs for each of the Pods that are a member of the service. And currently there's only one Pod supporting this service, and so here we see the Pod IP and port 192.168.206.127 and port 8080. And so when traffic comes in on the service, it'll get routed to this endpoint. And so let's go ahead and do just that. Let's access the service inside of the cluster. And so what I'm going to do is I'm going to grab the IP address of the service and throw it into my clipboard and use curl to access our application, which is a simple Hello World web application, and curl is a command line web browser. If I type curl and then the protocol http://, and I'll paste in the service IP, and then type in :80 for the port that we're listening on. When I press Enter, this is going to send traffic to the service IP, which will then get routed into the Pod that's supporting this particular service. So let's go ahead and press Enter and look at our output. Our output, simple Hello, World application running on version 1.0. The application also prints out the hostname. So here we can see the actual Pod name is the hostname. So we see hello‑world, then our Pod template hash, and then the Pod ID ending in gnxsk. And so what happened there is we hit the service IP. The service then routes that traffic into the individual Pod supporting the service. If we scaled our application out, there would be additional endpoints registered into the service, and it would be up to kube‑proxy to load balance that traffic amongst all of the Pods supporting the service. Now in the next part of the demo, what I want to show you here is how to use kubectl to generate YAML or JSON for resources that you have deployed in your cluster. In this case, we're going to look at a deployment. And so on line 116 here I have kubectl get deployment hello‑world, and then the output modifier ‑o yaml, and I want to pipe that output into more. And so this will give us a YAML representation of the deployment object. This YAML representation also includes runtime information about the actual object itself. And so if we look inside of here, we see the YAML associated with the deployment, apiVersion apps/v1, kind is Deployment, but we also have a bunch of runtime information. This can be useful for monitoring in configuration management scenarios, but not so great as a source for manifest for declarative deployments. We'd have to remove all of this runtime information. I'm going to show you a technique in a few moments where we use dry run to help us generate these manifests very quickly and correctly at the command line without runtime information. If we also wanted to see this from a different angle, I can do the same, but for JSON. So kubectl get deployment hello‑world, then with the output modifier ‑o json, pipe that into more. And here we can see the JSON representation of this object, including its runtime information. And so let's go ahead and clear out of that. And before we move forward into our next demo, I want to delete the resources that we created imperatively and recreate all of those declaratively in our upcoming demo. So let's walk through the process of deleting some resources together. Before we delete anything, let's look at what we have. And so with kubectl get all, we can see I have my deployment, which created my replicaset, which created the Pod supporting that deployment. We also see that our bare Pod is in there, and I also have two services up and running, one that we just created, our hello‑world service, and a service for the API server that's available inside of the cluster. So let's delete our service, and we can do that with kubectl delete service hello‑world. I'm going to go ahead and delete our deployment with kubectl delete deployment hello‑world. What's going to happen when I do kubectl delete deployment hello‑world, that'll delete the deployment, which will then delete the replicaset, which will then delete all the Pods associated with that replicaset. So there's a cascading delete that happens there. We then need to delete our bare Pod since it's not associated with any controller, and we can do that with kubectl delete pod, specifying a Pod name, hello‑world‑pod. That's going to block for a moment until that Pod is actually deleted, and then we'll get our console back. With that Pod deleted, if I do a kubectl get all now, all that we have left is that single cluster service for Kubernetes available.

Demo: Declarative Deployments and Accessing and Modifying Existing Resources in Your Cluster
Moving forward, let's look at how we can deploy resources declaratively in our cluster. We just walked through how to create things imperatively at the command line, but we want to get to where we're deploying things declaratively in code in our cluster, and so let's start that process together. On line 136 here, I have an example of where we can use a dry‑run=client to help create a YAML manifest quickly and correctly, and so let's walk through the code to do just that. We have kubectl create deployment hello‑world, and then specifying the image that we want to run. So really, that's no different than we saw in the previous demo. We then add on dry‑run=client and ‑o yaml, and what that will do is create the YAML manifest for that deployment named hello‑world running that container image. And that'll give us that output, the standard out. I'm going to pipe that into more. And so let's go and run this code and look at what we get. In the code at the bottom, we have the YAML that describes what we want to do. We want to create a deployment with that container image. So let's break out of this output and then run that code one more time, but instead of writing it to console, we'll write it to file. And so here, the only difference from the previous command is on line 144, where we're redirecting that output into a file named deployment.yaml file. And so from here we could take this deployment.yaml and build on a more complex deployment, if we need to, adding and changing the configuration inside of the deployment.yaml file if we needed to. And so let's take a peek at that just to make sure that we have what we want, and there we can see our code inside of there. For our demonstration here, we're going to take that file as is and deploy that in our cluster, and so let's look at the code to do just that. On line 152 we have kubectl apply ‑f deployment.yaml. And so what this will do is read that deployment.yaml file and send it into the API server for creation. So let's run that code and see what we get. A few moments later at the bottom we can see deployment.apps/hello‑world is created. So we created our deployment declaratively in code. Now, we can do the same thing for our service. In a previous demo, we use the command kubectl expose deployment, and then specified the port as 80 and the target port as 8080. We can also add on dry‑run=client ‑o yaml, and do the same thing, converting that imperative code into declarative code. And here we see at the bottom, we have our YAML representation of that. We'll take that code and run that into file, just like we did a few moments ago. But this time we're going to take this output for our service creation and redirect that into a file named service.yaml. We'll take a peek inside of there and look at the code inside of service.yaml, and here we can see the YAML manifest for the service that we want to create. I can't stress enough how valuable this technique is to quickly and correctly create manifests that you can either deploy right away or make simple modifications to that text before you send it into the API server for creation. So let's go ahead and create that service with kubectl apply ‑f service.yaml. At the bottom here we can see service/hello‑world created. Let's check out the status of our deployment and our service, and we can do that with kubectl get all, run that code there, and here we can see all the resources deployed into our default namespace. In our default namespace we see our deployment hello‑world. That created the replicaset here at the bottom. That replicaset created the Pod that we have up and running. We see our hello‑world service and also our Kubernetes API service that are in the cluster. So next now, let's look at how we can make a change to an existing resource in our cluster. Since we have the code that describes our deployment, we can make a modification to that file and then just resubmit that into the API server and then make that change to our deployment. So let's do that together and edit deployment.yaml and make a modification to what we have running in our cluster. So in here is the code that describes our deployment. If we go down into the spec, I want to jump over in replicas and go from 1 to 20. And so what I want to do is to change the number of replicas supporting our deployment from 1 to 20, and we're changing that in the code. But that doesn't change the application yet. I have to feed that code into the API server to effect that change. And we do that with kubectl apply ‑f, and we're going to send in deployment.yaml. At the bottom we can see deployment.apps/hello‑world configured. We've effected that change to the desired state in our cluster. And now Kubernetes will go and spin up those 19 additional Pods to bring us up to 20 replicas supporting our deployment. And so if I do a kubectl get deployment now for hello‑world, we can see in the output I have 20 of 20 Pods that are up, running, and ready. And if I do a kubectl get pods and pipe that into more, we can see the the whole collection of Pods that are up and running and supporting our deployment now. Now what happened behind the scenes when I scaled that deployment out, each one of those Pods that was a member of that deployment also got registered as an endpoint in the service and will automatically start receiving workload. And so if I do a kubectl get service now, we'll go ahead and grab the IP address of this new service, since I had to delete the previous service that we created imperatively and created a new one declaratively, the IP address changed. And so want to grab that IP address and throw that in my clipboard. We're going to use curl again, http://, and paste in the new IP address associated with our hello‑world service. If I press Enter now, I'm going to access the service on the service IP and then get load balanced to 1 of the 20 Pods supporting this application. So there I got load balanced to a Pod ending in 7j8tc. Let's try this a few more times, and we can see each time that I access the pod, I get load balanced to a different Pod. So the next one we see is qvc6s, 5w5cg, we hit that one twice, and then we got load balanced to another Pod. And so kube‑proxy is distributing that workload amongst 20 Pods that are a member of this service. Now let's say I didn't have that deployment manifest and I needed to scale my application or even make any other change to my application, and I don't have that deployment manifest. Well, I can use the command kubectl edit to edit a resource that's available in the cluster that's already up and running. And so let's look at the code to do this. Kubectl edit, what do I want to edit? I want to edit the deployment named hello‑world. And so when I run this code, what it's going to do is retrieve the object via the API server and present that back to me in a local text editor. And so now I don't need to make a modification to the code and then send it in. What I'm going to do here is make the modification to this object in my text editor and then when I save this out, it'll send this code back into the API server and effect that change. And so here I'm going to change replicas from 20 to 30. And when I save this out, this is going to immediately be changed in our cluster effecting that change. So there we see deployment.apps/hello‑world was edited. And so now the desired state of our cluster changed from 20 Pods to 30 Pods, and Kubernetes will go and scale up and create 10 more Pods in our cluster at the point in time in which that object was saved and sent back into the cluster when I exited my text editor. If I do a kubectl get deployment now for hello‑world, we can see 30 of 30 Pods are up and running. I want to show you one other way to be able to scale an application at the command line, and we can do that with the command kubectl scale deployment. Which one? hello‑world and specifying the ‑‑replicas parameter. Here we're going to set our number of replicas to 40, and at the bottom there we can see deployment.apps/hello‑world is scaled. And if I check the status of the deployment with kubectl get deployment hello‑world we can see 40 of 40 Pods are ready. Now I do want to call out that I continue to scale up the number of Pods here. We certainly could have reduced the number of Pods in any of these demonstrations to change the desired state of our application. I'll leave that as an exercise to you, the viewer, to experiment with that. And so that's a wrap for this demo. Let's go ahead and clean up our resources that we deployed, and I'm going to use kubectl delete deployment hello‑world to delete that and then kubectl delete service hello‑world to delete our service. And then I'll do a kubectl get all, and here we can see we're kind of in an intermediate state, where those Pods are all getting terminated actively for us behind the scenes. And so we still have a couple that are up and running, and a few moments later, all of these Pods will be shut down and deleted from the cluster, and all that we'll be left with Is that Kubernetes service running in our cluster. Let's refresh kubectl get all one last time to see, and all that we have left is our Kubernetes cluster API service.

Module Summary and Thank You!
Here we are at the end of our module, and we introduced how we can interact with our cluster focusing on using kubectl to retrieve different types of information out of our cluster. And then we looked closely at how to deploy some applications and what is really happening behind the scenes. We discussed both imperative and declarative deployment methods, and hopefully I made the case for using declarative deployment methods. So here we are at the end of our course, and I really hope you enjoyed listening to this and that we laid the appropriate foundation for your Kubernetes studies. We covered a lot of ground together so far. We discussed the Kubernetes architecture, we did a deep dive into installing a Kubernetes cluster, both on‑prem and in the cloud, and we looked at how to interact with our cluster using kubectl and did some application deployments. It's truly been a pleasure recording this course, and I thank you so much for listening, and most importantly, learning with me. I hope you enjoyed the course. Join me again soon, here at Pluralsight.







Module 2:


Managing the Kubernetes API Server and Pods:

Course Overview
Hi everyone. My name is Anthony Nocentino, Enterprise Architect and Founder of Centino Systems. Welcome to my course, Managing the Kubernetes API Server and Pods. Are you a systems administrator or a developer that needs to deploy workloads in Kubernetes clusters? If you do, then this is the course for you. First, we'll start off by digging into the Kubernetes API server and learn how it works. We'll learn how to interact with it and its internals of its critical functions in our Kubernetes cluster. Then, we'll learn how to use labels, annotations, and namespaces to help get control over and organize workloads in our clusters. And to wrap it up, we'll look closely at the primary workload construct in Kubernetes, the Pod. We'll learn what they're made of, how they work, and how to keep them online in reporting our application health. By the end of this course, you'll have what it takes to debug issues with your API server and your cluster workloads. You'll learn how to organize and get your arms around your largest Kubernetes workloads and also how to craft the best Pods for your applications. Before this course, you should be familiar with the Linux operating system and administering it at the command line. You should have a firm understanding of TCP/IP‑based networking, and also the fundamental concepts of containers. You'll need to know the core foundations of Kubernetes, like what a cluster is and how to interact with it at the command line. I hope you'll join me on this journey to learn how to deploy and manage workloads in Kubernetes in the course, Managing the Kubernetes API Server and Pods.

Using the Kubernetes API
Introduction, Course, and Module Overview
Hello. This is Anthony Nocentino with Centino Systems. Welcome to my course, Managing the Kubernetes API Server and Pods. In this module, we're going to kick it off with a course introduction and then dive right into the content with Using the Kubernetes API. All right, so let's start the course off with a course overview, and we're going to kick things off with the module Using the Kubernetes API. We're going to look at how we can use the Kubernetes API to build and model systems to deploy into our Kubernetes cluster. And then we'll also take a close look at the API server itself and its internal functions. As our Kubernetes workloads grow, we'll need to use some techniques to help interact with and manage our deployed applications and services in our cluster, and we'll learn how to do that in the module Managing Objects with Labels, Annotations, and Namespaces. And then finally, we'll take a deep‑dive look at the primary workload element in Kubernetes, the Pod, in the module Running and Managing Pods. So let's take a look at an overview for this module. And first up, we're going to talk about the Kubernetes API and the API server where we'll take a closer look at the API itself, API objects, and the internals of the API server. Next up is we'll look at working with Kubernetes objects. We'll look at the types of objects available, how to use them, looking closely at how we define objects, Kubernetes API groups, and also how the API server itself is versioned. Then we'll wrap up the module with a deep dive into the anatomy of an API request where we'll look closely at what happens when we submit a request into the API server.

The Kubernetes API and API Server
The Kubernetes API is a single surface area over the resources that are available in our data center or, more specifically, inside of our Kubernetes cluster. The Kubernetes API gives us the ability to use API objects to help model our system and build the workloads that we want to deploy in our cluster. The API objects are a collection of primitives to help us represent our system state where we can define what that desired state is, enabling us to configure our system and model it as API objects deployed in our API server. The API server is going to be the sole way that we interact with our cluster, and it's also the sole way that Kubernetes interacts with our cluster when it needs to exchange data between the various components of the cluster itself. So let's take a closer look at the Kubernetes API server. The Kubernetes APIs server is a client/server architecture. It implements a RESTful API over HTTP using JSON. So we're going to exchange JSON objects between client and server using our RESTful API, so GET, SET, and POST operations and things like that. We'll look much more closely at those operations later on in this module. Now the client is going to submit requests to the server over HTTP or HTTPS. The server is going to respond to these requests based on the actions it had to take, whether there was a positive result and made the change that was requested or perhaps a negative result, and there was an error or resource that wasn't found. We're going to look much more closely at responses later on in this module as well. The API server itself is stateless. So what this means is any configuration changes that we've made in our cluster via API objects aren't stored in the API server. They're actually serialized and persisted into the cluster data store, which is traditionally etcd.

The Control Plane and API Objects
So let's take a look more practically at the control plane node in our cluster. Inside the control plane node, we have a collection of critical services that help us facilitate cluster operations, the API server, the cluster store, the scheduler, and the controller manager. The API server is the primary access point for administrative and cluster operations. This is the communication hub of the entire Kubernetes system. The cluster store is where the state of the system is stored and persisted in a key value data store, etcd. The scheduler tells Kubernetes which nodes to start Pods on based on the Pod's resource requirements in our workloads. And then finally, the controller manager has the job of implementing the lifecycle functions of controllers, basically keeping things in the desired state. In this module we're going to focus on the API server and its interactions with clients like kubectl and how they work together to create or make changes to resources deployed in our cluster with API objects. Kubernetes API objects are the persistent entities that allow us to build and deploy systems in Kubernetes. They are the things that we'll use in our code to model our systems. We'll send that code into the API server, and Kubernetes will persist that configuration in the cluster store through the API server and go about building our system in our desired state. So, the Kubernetes API objects are the things that represent the state of the system that we want to build. Kubernetes API objects are organized in three ways, and we're going to look at each one of these individually as we go through the next series of slides. And first up is kind. Kind is a string value representing the REST resource this object represents in the API server, so basically the things that we've been working with so far earlier in this series of courses, things like Pods, services, deployments, and so on, all the API objects that are available to us. Then there's the API group. The API group is a way to organize or group like objects together based on their function, things like core, which are the essential API components foundational to Kubernetes workloads, so things like Pods we'll find in the core API group. There's apps which includes the deployment object, which are used to build and model applications in our clusters. And then finally, the storage API object group, which is used to model storage inside of our cluster. There are many more API object groups, and we'll look at those more closely throughout this module. And then finally, the API is versioned. The API version defines the version schema of this representation of a particular API object in the API server. Again, we're going to look at each one of these individually in the upcoming slides. Okay, so with API groups and versioning under our belt, let's look at some of the core API objects that we'll actually use to model systems in our Kubernetes cluster, and first up is Pods. These are used to deploy our container‑based applications in Kubernetes. Then there's deployments, which allow us to declaratively deploy applications in our cluster and control our applications in terms of the container image or version of our applications and also scale our application if needed. There's services that provide persistent access point and load balancing services for our applications running on Pods. And then finally, persistent volumes. These are used to provide persistent storage to our container‑based applications. Now, in this course, we're going to have a module dedicated to Pods and instantiating workloads based on Pods. In subsequent courses, we'll have modules dedicated to each one of these fundamental resources, deployments, services, and persistent volumes. And I do want to mention that this isn't an exhaustive list of API objects. These are just the key players that we're going to use to build fundamental workloads and deploy in our Kubernetes cluster. So, now we have to discuss how we actually work with Kubernetes objects. We know what they are. Well, how do we get Kubernetes to do the things that we want it to do? And the first technique you can use is imperative configuration. When using imperative configuration, you're generally going to be executing a series of commands at the command line, operating on one object at a time, effecting the change that we want directly on our cluster. But there's a better way. We're going to use declarative configuration, and when we use declarative configuration, and this is really one of the core principles behind Kubernetes, we're going to define the desired state of our application in code using manifests. When we use manifests, we'll define our configurations in code using languages like YAML or JSON to represent the API objects that we want to feed into our API server. And then when we're ready, we'll take that code that we implemented in our manifest and use a command like kubectl apply to feed that information into our API server to effect that change of our desired state upon our Kubernetes cluster.

Defining a Basic Pod Manifest
So far in this course we've gone through what API objects are. And we've gone through some of the fundamental objects used in Kubernetes and learned how to work with objects and manifests. Let's take a minute to look at how we write the code to implement a basic workload in Kubernetes in a manifest, and in this case we're going to create a Pod together. And so, the first line of every manifest is going to look like this. We're going to specify an API version. We briefly introduced the API versioning a bit ago, and we're going to look at this topic in much more detail later in this module. In this case, for this resource, we're going to be using the API version v1. Next up is kind. This is the API object that we want to instantiate with this manifest. In this case, we're going to create a Pod. Next up is metadata. We'll need to give our API resource a name, and in this case we're going to create a Pod so I'm going to name this resource nginx‑pod. Now we get into the technical specification or the spec part of our manifest. And since we're creating a Pod, we're going to need to define a container. And our container's going to need a name, in this case it's going to be nginx, and we're going to use the nginx image. And so once we take this information, and we can save it into a file, we could take that file and feed it into our API server with this command, kubectl apply ‑f nginx.yaml, nginx.yaml being the manifest or the text file that represents the code that we just typed at the top here. Now these four elements are required for any Kubernetes manifest, the API version, the kind, the metadata, and the spec. Now how do I know which fields or elements I need to fill out that spec part of the manifest there? Well, that's where the documentation comes in. At this link here, you'll find the documentation for the Kubernetes API objects that are available. And in addition to that, in our upcoming demonstration we'll look at using kubectl at the command line to help explain to us the elements or the fields that are available in our API objects.

Using kubectl dry-run
So when working with API objects and writing code to represent those objects, we're going to need a way to ensure our code in these YAML manifests is syntactically correct and can be processed by the API server successfully, and that's where the dry‑run parameter for kubectl comes in. There are two types of dry‑run that we can use, and let's talk about server‑side dry‑run first. When making an API request that creates or changes resources in your cluster like kubectl create or kubectl apply, you can add the dry‑run parameter to the request, and the request submitted will be processed by the API server as if it was a typical request, but with one fundamental difference, the API request will not be persisted to storage in the cluster. Using dry‑run enables you to know if the request you're submitting is properly formatted and is able to be processed successfully by the API server, or if it is not. If there is an issue, a dry‑run will tell you what the issue is for the resource that you're trying to create or change, so for example, if an object already exists or if you had a syntax error in your manifest. On the client side, dry‑run will print the object that you want to create or change to stdout without sending the object to the API server. Client‑side dry‑run is useful to validate if a manifest is syntactically correct, but since the request is not getting sent to the API server, it is possible that the object creation or change could fail for another reason when it's submitted to the API server for real. The primary way that I use dry‑run client is to generate syntactically‑correct YAML manifests for objects. You can combine dry‑run client with imperative commands like kubectl create to output syntactically‑correct YAML manifests for objects like deployments, Pods, and others. And once you have that basic manifest, you can use that as a building block for more complex manifests. Let's look at a few examples of using kubectl dry‑run in code. First up, here is an example of server‑side dry‑run. We see kubectl apply ‑f deployment.yaml ‑‑dry‑run=server. This will send the manifest deployment.yaml to the API server for server‑side validation of the objects defined in the manifest. So this could be deployments, services, persistent volume claims, really any resources that we need to create or modify. The API server will validate if we can create or modify the resources in this manifest. Next, we see a similar example, but this time on the client side, and this will validate the syntax of the objects defined in the manifest, but will not send the manifest to the API server. Now, let's look at how we can combine dry‑run with imperative commands to generate syntactically‑correct YAML manifests. Using the command kubectl create deployment nginx, and then specifying the image's nginx, if we combine that with a dry‑run =client and the ‑o yaml output modifier, this will output a YAML representation of the object defined in the command, in this case here it's going be a deployment, and it'll write that YAML manifest to stdout by default. This is a fantastic way to quickly generate syntactically‑correct YAML manifests for objects, any objects, not just deployments. This could be Pods, StatefulSets, and so on. These manifests can be used as building blocks for more complex YAML manifests. And finally, you can take that command, and then you can combine it with I/O redirection. So here we see kubectl create deployment nginx, specifying the nginx image, add in dry‑run client ‑o yaml, and then redirect the output of that to deployment.new.yaml, and then we have a file containing the syntactically‑correct YAML for that deployment.

Using with kubectl diff
Now, let's say we want to make a change to a resource that's deployed in our cluster, and before we do that we want to know what the difference is between the current state of the resource running in the cluster and what's defined in a manifest that we want to deploy containing changes to that resource. That's where kubectl diff comes in. Kubectl diff is used to generate the differences between resources running in your cluster and resources defined in a manifest or ones passed in via stdin. When executed, it will output the differences to stdout, and so you can very easily determine what's going to change when you apply that manifest to your cluster. At the command line, you will use kubectl diff most commonly with the ‑f parameter and read in a manifest. In this example here, it's newdeployment.yaml. For the resources defined in the manifest, it will compare those resources with the same resources that are currently running in the cluster and output the differences to stdout. For example, if this was a deployment in the manifest here, diff will go and inspect that same deployment and compare it with the deployment defined in the manifest and let you know what's different. And so perhaps it's a container image or the number of replicas that are the differences, and those individual differences are what will be printed to stdout.

Lab Environment Overview
Before we kick off our first demonstration in this course, let's review the lab environment. We initially constructed this lab in the course, Kubernetes Installation and Configuration Fundamentals. Now our virtual machines here are going to be the same, they're going to be Ubuntu 18.0.4, VMware Fusion VMs, with 2 vCPUs, 2 GB of memory, and 100 GB of disk space with the swap disabled as required by the kubelet. For this lab. you can use any underlying hypervisor that you want. I just happen to be using VMware Fusion. Now to take DNS out of the equation for our lab environment, I have hostnames set for each server and specified in a host file on each of the virtual machines in the lab. And for the virtual machines that make up the lab, we're going to have one control plane node, c1‑cp1, and this is where we'll be driving all of our demonstrations from. We'll SSH into c1‑cp1, and use kubectl locally on that server. In addition to the control plane node, we're going to have three worker nodes in our lab, c1‑node1, c1‑node2, and c1‑node3. They're all going to be statically IP‑ed with the addresses specified here on the screen. Now if you need help constructing this lab, head over to my course, Kubernetes Installation and Configuration Fundamentals, to get you bootstrapped with this configuration. I also want to call out that c1‑cp1 used to be called c1‑master1 in this series of courses. We have since renamed it to c1‑cp1 to keep up with current terminology in Kubernetes.

Demo: API Server Discovery, Listing Resources, Using kubectl explain, and Creating Objects
So here we are in our first demo, and we're going to get things started with API Server Discovery. We're going to learn how to locate our API server and use the appropriate cluster context to log into our Kubernetes cluster. Then with that, we'll ask Kubernetes, Hey, Kubernetes, I need you to give me a listing of available API resources that you know about, basically the things that I can use to construct workload or objects in my Kubernetes cluster. Once we have that listing, we're going to use a tool, kubectl explain, to get a little bit further into the implementation details of the individual objects that we want to work with. And then once we have that information, we'll take that and define an object in YAML. We'll then send that manifest into our API server to create that workload in our Kubernetes cluster. Then to wrap it up, we'll use kubectl dry‑run to validate and generate some YAML manifests and then use kubectl diff to compare a manifest with a resource that's running in the cluster. So here we are logged into c1‑cp1, and let's get started with our first demo, API Discovery. Now the first thing that we're going to do is make sure that we're pointed at the correct cluster by setting our cluster context, and we can do that by first getting a listing of the configured cluster context on our local system here. We can do that with the command kubectl config get‑contexts. When I run this code, what this is going to do is output any of the cluster context that I have configured on my local kubeconfig file. In the output here, I have two cluster contexts defined. Let's walk through both of those, the first being CSCluster, which is the Azure Kubernetes service cluster that we created together in the course, Kubernetes Installation and Configuration Fundamentals. The second line of output we have is a kubernetes‑admin@kubernetes. That cluster context is the local kubeadm‑based cluster that we built together on virtual machines. In the left‑hand column there, we see an asterisk in the CURRENT column next to that cluster, meaning that this is our current cluster context. If we needed to change our cluster context, we can use the command here on line 11 to do just that‑‑kubectl config use‑context and then specify the context name, and here we have kubernetes‑admin@kubernetes. Let's go ahead and run that code together, and we can see the output here. If we needed to switch our cluster context, the output would say Switched to cluster context "kubernetes‑admin@kubernetes." Now that we know that we're pointed at the correct cluster, let's use the command kubectl cluster‑info to get some information about our API server. And in the output at the bottom, we can see the network location of that API server. So let's walk through that together too. Kubernetes control plane is running at https://172.16.94.10 on port 6443. So now we know that we're pointed at our local on‑premises Kubernetes cluster with that API server information there. And so now that we know we're pointed at the right cluster, let's use the command kubectl api‑resources to get a listing of the API resources available for us in this cluster. We'll go ahead and run this code together, and in the output, we'll get a listing of all the different API resources available to us to build and construct and configure workloads in our cluster. And so let's walk through this output at the bottom. On the left here, we have NAME, which is the name of the app resource. We have SHORTNAMES, which is an alias that we can use to address that same type of object but with a shorter name. We then see the APIVERSION, so everything on the screen here is a v1. Now we haven't touched on what a namespace is yet, but a namespace is a way to group resources inside of a Kubernetes cluster. And we're going to cover that in great detail in the next module. But here we have a column that describes if this particular API resource can be in a namespace or not. And then, finally, we have the KIND column. The kind is a string value representing the REST resource for this object. And so in the output, we see some things that we're familiar with from a workload standpoint. We see nodes and Pods, and if I go down a little bit further, we'll see things like deployments and replicasets. So API objects used to construct and configure workloads in our cluster. And so now that we know the different API resources available to us, maybe we need to dig a little bit further and get some more information about those different API resources to help us understand how we can build a workload with those particular API resource types. And we can use the command kubectl explain to help us with that. And so on line 24 here, I have kubectl explain pods. And what that's going to do is give me some detailed information about that particular resource, and this could be any resource that's available to us in our cluster, things like deployments, replicasets, and so on. But we're going to look at Pods more closely in this demo. And so when I run kubectl explain pods, what we're going to get is some deep‑dive information about that particular resource. And so here we see KIND is Pod, VERSION is v1. We also see a DESCRIPTION: Pod is a collection of containers that can be run on a host. So telling us exactly what this type of resource does. Now going down a little bit further in the FIELDS section, we see what it takes to construct this API resource in our cluster. So we see things like apiVersion, kind, metadata. And if I go down a little bit further, we'll see spec. And so with that then, we can start understanding what it takes to build the YAML manifest that describes this type of object that we want to deploy. Now I do want to call out that spec is a type object, and so what that means is we can go even further into that object with kubectl explain. And so let's look at how to do that. On line 28, I have kubectl explain pod.spec. Let's go ahead and run that code, and now what we have is the information to help us color in those details about what's needed for a PodSpec. And so if you're working with the workload object or any API resource, you can use this technique to kind of discover what it takes for all the various configuration elements that you might not have memorized. But you certainly could go look those up on the API references online on the web, but they're all available to you at the command line here at your fingertips. And so let's look at this output. So here we see KIND is Pod, VERSION is v1, and the RESOURCE is spec. In the DESCRIPTION, it says, The specification of the desired behavior for the Pod. And so this is where we're defining how a Pod is going to function. So in the FIELDS at the bottom, we'll see things that we're probably familiar with in a PodSpec, specifically containers. And so here we see containers is a type object, and it's also required. And so that's going to be something that we have to put in every single PodSpec that we write. And so let's break out of this output. And since that's an object, we can use kubectl explain to dive even further and get more documentation about that particular facet or field of the API resource. So kubectl explain pod.spec.containers, and now we get the description and the fields of pod.spec.containers. Let's go down to a field that might be familiar to us. Go down to image. And so here we see image is where we'd specify the container image that we want to launch. Here it says Docker image name. So let's go ahead and break out of this output now, get back to our terminal, and look at a Pod manifest that I already have written that is available to you in the course's downloads. And so now you can kind of make the connections between what we just saw in kubectl explain and what we see in a Pod manifest‑‑apiVersion: v1, kind: Pod, metadata: we give it a name, hello‑world. We then have the spec, the technical implementation details of how this Pod behaves. In this case, it's just going to run a container, one container named hello‑world, loading up the image from the Google container registry for our hello‑app application that we've been using in our demonstrations in this series of courses. And so jumping back to our driver script, let's go ahead and create that Pod with kubectl apply ‑f pod.yaml, and we go ahead and create that Pod. And so that technique of discovering API objects to understand how to construct them and then using that to help us construct the Pod manifest is going to be a common technique that you need in building your workloads inside of Kubernetes. Now before we move forward into the next demo, let's make sure that that Pod got created successfully with kubectl get pods. And at the bottom here, we can see in the output that that Pod was certainly created, and its status is up and running. And so with that, before we move forward to the next demo, let's delete that Pod with kubectl delete pod hello‑world.

Demo: Working with kubectl dry-run
Now moving forward, let's learn how we can use kubectl dry‑run for both a server‑side and client‑side validation and also to use it to help us generate syntactically correct YAML at the command line. And so the first thing we're going to do is we're going to use server‑side validation for a deployment manifest. And so let's check out how to do that with the code on Line 50. And so here we have a kubectl apply ‑f deployment.yaml and I specify ‑‑dry‑run=server. And so when I run this code, what that's going to do is take that deployment manifest and send it into the API server for validation, it's going to check both is it syntactically correct and can it actually create those resources in the cluster for me. And here in the output, we see deployment.apps/hello‑world created, but the difference here than for normal output as we see in parentheses (server dry run), and so that tells us that we can successfully create or change the resources defined in that manifest. If I go ahead and look at the deployments that I have available in my cluster with kubectl get deployments, we can see that well, we didn't actually create that deployment, it just validate it if it was syntactically correct and if it could create that in a cluster because we specified dry‑run=server. So let's move forward and try some client‑side validation. And so, very similarly we say kubectl apply ‑f deployment.yaml, so we're reusing that same manifest, but this time instead of saying ‑‑dry‑run=server, we're going to do client‑side validation and say ‑‑dry‑run=client, run that code there, and we can see similar output, but slightly different. We see deployment.apps/hello‑world created, and then just (dry run) in parentheses in contrast to what we saw a second ago where it said server dry run. And so with that, we saw both server and client‑side validation for correct manifests. Let's see what happens when I feed in a manifest with an error. And so on Line 62 here, I have kubectl apply ‑f and then I'm specifying a different manifest deployment‑error.yaml, and they're specifying ‑‑dry‑run=client. And so when I run that code and do client‑side validation, well, it's going to fail and we can see the error at the bottom. And so in the bottom here, we have error validating deployment‑error.yaml error validating the data. Scan a little bit further over on the right, we see unknown field replica in our deployments spec. And so, what that's telling us is well, that there is an error in that field in our deployment spec. And so, let's jump over to a deployment‑error.yaml and what we have here is on Line 8, well, I've incorrectly put the field in as a replica instead of replicas plural so that should have an s there on the end to make that syntactically correct, and that's exactly what the error told us in the output at the bottom here is that there was an error unknown field replica. And so, using client‑side validation with dry run is a good way to sift out configuration errors in your deployment manifests quickly. Now one of my favorite ways to use dry run is to combine it with imperative commands to generate syntactically correct YAML, and so let's check out how we can do that. And so first up, I want to show you that we can use kubectl create deployment nginx to create a deployment and specify in our image name and then adding the parameter dry‑run=client. And so when I run this code here, that's going to validate that well imperative configuration command. We can combine dry‑run=client with imperative commands and then also with an output modifier so that it can generate YAML for us and here we see the code to do just that on Line 70, so the same code as above, but we add in ‑o yaml. And so on Line 70, I have kubectl create deployment nginx and then we're specifying the image nginx dry‑run=client, and then ‑o yaml and then I'm going to pipe that into more so it doesn't run off the screen. And at the bottom here, you can see we've generated the YAML for a simple deployment, and we can use this either as a foundation for a more complex deployment or just to test things out to quickly generate manifests for deployments in our cluster. And so, I do want to point out though that that's not just for deployments, that could be for stateful sets, that could be services for whatever. Here, I have an example of a pod, and so we're going to use the same technique to generate a manifest for a bare pod, kubectl run pod and the name is going to be nginx‑pod for the image nginx and then dry‑run=client ‑o yaml pipe that into more, and now we have a manifest that describes that pod with that nginx image for us to use or to build upon if we need to do so. Now we can take this technique of dry‑run=client and ‑o yaml with that output modifier and use I/O redirection to store that generated YAML in a file and that's what we're doing just on Line 78 here, so kubectl create deployment nginx, nginx is the image again, and then specifying dry‑run=client ‑o yaml and then redirecting all of that output into a file named deployment‑generated.yaml. And so, let's run that code and then I'll use the more command to output that to console and here we can see that deployment manifest. Let's break out of this output at the bottom and we can use that manifest right away to ship that into the API server to actually create those resources and that's what we're doing here on Line 83, kubectl apply ‑f deployment‑generated.yaml, run that code, and at the bottom here, we can see deployment.apps/nginx is created. And so using that technique of kubectl create and then combining that with whatever resource it is that you want to create and then dry‑run=client and then the output modifier ‑o yaml, you can very quickly and easily generate syntactically correct manifest for the resources that you want to generate. And so let's clean up from this demo and use kubectl delete to delete the manifest that we just created, deployment‑generated.yaml, and that's going to delete our deployment as indicated in the output at the bottom.

Demo: Working with kubectl diff
Now for our final demo in this series of demos, we're going to look at working with kubectl diff. The first thing that I'm going to do is create a simple deployment with 4 replicas in our cluster, and here I have the code on line 94 to do just that, kubectl apply ‑f deployment.yaml. That's going to create a simple four replica deployment in our cluster. Looking at the deployment manifest for that, we can see that it is a deployment that has four replicas, and it's running the container image version 1.0, as we see on line 19 here, hello‑app with the tag of 1.0. Let's jump back to our driver script here. I now have a new deployment manifest that I have already written called deployment‑new.yaml, and in that manifest, I've made some changes. And let's use kubectl diff to help us understand what the differences are between the currently running object in our cluster and the deployment manifest that I want to deploy into the cluster. I want to see what the exact changes are between what's running and where we're about to go to with this new deployment manifest. And on line 98, we can do just that, kubectl diff ‑f deployment‑new.yaml, and I'll pipe that output into more. Let's skip past this metadata at the top here and jump down into the deployment and pod template spec at the bottom here, which is where the changes that are interesting to us are located. And so in the pod spec, we see replicas 4 and a minus sign, so that's the difference, and then plus is the change, replicas 5. So we went from 4 to 5 as indicated here in the output. Down to the bottom in the pod template spec, we see containers, we see the changed line with a ‑ sign. So image, there's the version 1.0 of our application, and then the line that we're adding or changing to right below that. So we see that the image is version 2.0 of our application. And so with kubectl diff, you can very quickly and easily compare resources that are currently running in a cluster with the resources defined in a manifest and highlight the differences between the two. And so that's what we did here with kubectl diff. Let's wrap up this demo, and use kubectl delete ‑ f deployment to clean up and to delete that deployment.

API Groups and API Versioning
So it's time for our closer look at API groups. API groups enable better organization of resources in the Kubernetes API. There are two high‑level organization methods for API groups in Kubernetes. The first is the core API, or legacy API group. These are the Kubernetes objects used to build the most fundamental workloads, and here you'll find things like Pods. Back when Kubernetes was first developed, there was no concept of API groups, and these objects were simply defined in the API. Officially called the core API group, you'll often see it referred to as the legacy group or legacy API group as well. Now, as Kubernetes developed, and the need for groups became more apparent, newer objects were placed into named API groups. A common example of a named API group is the storage API group. Since the Kubernetes API is a RESTful HTTP‑based API, resources in Kubernetes are referenced via a URL or URL path. For named API groups, the API group name becomes part of that URL path. That's not the case for the core API, or legacy API objects, which are just part of the base URL for their API version. We'll look more closely at URL paths later in this module. So now let's roll through some examples of resources in both the core and named API groups, and let's kick it off with the core API group. In here, you'll find things like Pods, Nodes, Namespaces, PersistentVolumes, and PersistentVolumeClaims. Again, these are the most fundamental elements in the Kubernetes API, and have been around since its inception. So now let's look at some named API groups and example objects from each group. The first API group that we're going to look at is the apps API group, and in here you'll find a common resource like the Deployment object. Then there's the storage.k8s.io named API group where you'll find the StorageClass. And finally, rbac.authorization.k8s.io, where you'll find things like the Role or ClusterRole objects used to assert some security or role‑based access control in our cluster. These are some examples of some very commonly used API objects. If you want to see a bigger list, then go ahead and check out this link here. Moving on in our deep dive into the API server, next up is api versioning, and the role that it plays in helping us build stable systems in our Kubernetes clusters. The Kubernetes API is versioned at the API level. This way we know what version of the API we're interacting with when we define resources in code. There could be multiple versions of the Kubernetes API on a server at any point in time, and this helps us by providing stability for our existing implementations. If we're using declarative deployments, and we're building systems and code using objects to model our system, like Kubernetes API objects, well, those objects might need to change over time, and Kubernetes provides us a mechanism to help us absorb that change when we're ready, and that's API versions. This allows us to specify which version of the API we want to interact with for those objects. API versioning enables both backwards compatibility and also forward change, so that we can specify the API version of the object that we want to work with in our code, and we've done this so far together when we've declared an API version in our YAML manifest. Now, as an API version develops, it moves between different phases of the Kubernetes development process, starting off in Alpha, moving on to Beta, and then onto Stable as objects and API versions become more tested and stable. Now, I do want to call out that there's not a direct relation between the API server version and the system‑level released versions of Kubernetes. At the time of this recording, Kubernetes is on system release 1.21. Newer versions of the API server and Kubernetes system software will be released, but they're not in lockstep with each other from a development standpoint, nor from a release standpoint. So let's look more closely at API versioning and those three phases of development that I introduced just a second ago. We're going to look more closely at Alpha, or experimental; Beta, or pre‑release; and then Stable, or general availability code. When working with the Alpha releases, you're going to find it's going to be versioned like this, where you see V1alpha1, or Alpha specifying that this is part of an Alpha release. This is going to be early‑release, bleeding edge stuff, brand new code coming out of the project team. It's going to be disabled by default in your API server, so you're going to have to go proactively enable this version of the API on your API server to consume objects from this API version. The code that's released in Alpha releases is for testing only. This is brand‑new stuff coming right out of the development lifecycle, and one of the things you're going to need to look out for when using Alpha release code is breaking changes. There's no guarantees that an object between versions of Alpha releases will look the same, so that API surface area might change between different versions of the API version in the Alpha phase of development. Now, as code moves along and gets more mature and more tested, it will move into the Beta, or pre‑release phase. Here you'll see a version like this, where you see V1beta1, indicating that it's part of the Beta release cycle. This is more thoroughly tested code and is considered safe, but it's still encouraged that you test this code in your systems before using it in production. There's some more stable guarantees around API object surface area, so different versions of the API server at the Beta release level have a less likely probability of changing between different versions. Now, one of the things that the project team encourages is feedback. They want to understand what your experiences are with these objects at this release level, and go over to GitHub and give them that feedback so that they can help make the objects that are released in particular API versions better and more production ready. Now, moving along through the lifecycle, we'll get up to Stable, or GA. This is going to be indicated with a versioning schema that looks like this. Here we see v1. The code at this level will have backwards compatibility between different versions. There are some policies around deprecation of objects and how long they have to live in an API version before they go away and become deprecated. This is considered production ready code, so when you're building your production systems, start here.

Demo: API Object Discovery - API Groups and Versions
Alright, time for a demo where we're going to look at API object discovery. Specifically, we're going to look at examining API groups and looking closely at specific API versions of objects available to us in our Kubernetes API. So just like in our previous demonstration, we're going to start off with some API discovery, but we're going to go down a different path this time focusing on API groups. And so, let's get a listing of all the API resources that are available to us on our API server and we can do that again with kubectl API resources. I'm going to pipe that output into more so that it doesn't run on the screen. And so, in the output at the bottom here, we can see a collection of resources that are available to us. And so in the first page of the output. we see that all of the resources are in the API version V1, this means they're in the core API group and all have an API version of V1. If I go down a little bit further in the output, we can see other resources that are part of different API groups, and in fact, different API versions and so let's zoom in on a couple of those. The first collection of API objects that I want to call out are the ones in the apps API group, so there we see daemon sets, deployments, replica sets, and stateful sets, and so those are workload objects that we use to well build applications in our Kubernetes clusters. Those resources are all on an API version of V1, and so here we see apps for the API group and V1 for the API version. Going down a little bit further in the output here, we can see that we have a resource that's not a V1 API version resource, and so let's look at one of those. There, we see cronjobs, which is in the API group batch and has an API version of v1beta1 because it's not quite graduated onto v1 yet. At some time in the future, you can expect cronjob will graduate on the v1, and so if you're trying to reproduce this demo and you see it's now on v1, that's okay. There is likely some other alpha or beta version resources available in the output here. So let's break out of this output here and move forward into the next part of our demo. Now, if I want to get a listing of the available resources in a specific API group, such as apps, I can use kubectl API resources to do that. When I specify the API groups parameter and I say api‑groups=apps, what that's going to do is give me a listing of the api‑resources that are in that API group on the server. And so this is a way for me to help discover and to navigate which API resources are available to me on my cluster that are part of specific API groups. Now let's say that I wanted to work with a specific version of an object, maybe I'm writing a manifest and I want to use an exact version of an object and while one of the ways that I can go about doing that when I'm writing a manifest as well, I'll specify the API version in the manifest, but how am I going to know the exact implementation details of that specific API version for that object and well we can use kubectl explain to help us do that. And so here on Line 13, what I'm doing is I'm specifying that I want an exact version of a deployment. So here we see, kubectl explain deployment, and for the parameter API version, I'm specifying apps/v1, and so this will explain that exact version of a deployment. Now, up until recently, deployments were actually in the apps/v1beta1 and then graduated onto apps/v1. And so if there was different versions of deployments available to us in our API server, we could specify that here at the command line, but as is of this recording, there is only one version of a deployment available to us in our API server. Now if I want to go and I want to look at all the supported API versions and groups that are on my API server, I can use the command, kubectl api‑versions to do that, and I'm going to pipe the output of this to sort and then to more to make it a little more readable for us. And so in the output here, we can see all of the different API groups available to us on our API server and all of the various versions of those that are currently available on our current API server. And so going to there, we can see lots of different objects in various stages of development. So there we see again batch/v1, batch/v1beta1.

Anatomy of an API Request: API Verbs and Special API Requests
So let's begin looking at the anatomy of an API request. When we're working at the command line with a tool like kubectl, we enter a command, and it'll convert that request from YAML into JSON. And it will then, based on the operation that we want to perform, submit that request into the API server. These requests will be based on RESTful API actions, things like GET, POST, and DELETE. Now since our API server is a RESTful HTTP web application, we'll need to submit our request and specify a URL resource location or API path. Encoded in this path will be our API group, whether it's core or named, the version of the API that we want to work with, and also the object type itself. Now when we submit our request, and if it's a valid request that changes the state of our system, the API server will commit that to the cluster store with the WRITE operation and send back a response with an HTTP response code that indicates what happened during that operation. If our request was a query for information, that'll be read from the cluster store, and that information will be sent back to the client as well with the appropriate response code. In this course, we're primarily going to focus on the API request, API paths, and the responses sent back from the server to the client. We'll look more closely at the cluster store in an upcoming course. The Kubernetes API is a client/server architecture where clients submit requests to the server. And primarily we've been using kubectl to do this work, but really any HTTP client that respects the protocol set forth by the API server and the API itself will be able to communicate with the API server and submit requests. So this gives us the ability, if needed, to build custom tooling to interact with your Kubernetes cluster. We're going to look at some examples in our upcoming demos where we'll do some investigation of our API server with the command‑line web browser curl. As long as we submit a well‑formed request with curl, we can get the API server to perform the operations requested. Now that leads us to, What does a valid API request look like? Well, we already know that our API server is an HTTP‑based RESTful API. What this means is we'll need to define requests in terms of HTTP verbs like GET, POST, and DELETE, and so on, and also specify a resource location, essentially a full path or a URL for the object that we want to interact with. You take that verb and you add in the resource location, and then you have a valid request that you could submit into your API server. Once you submit that request into the API server, the server will respond with the response code that indicates the outcome of your request. Was it successful? Was there an error? Did you fail to authenticate properly, and so on? Let's look at some of the more common API verbs available in the Kubernetes API. First up is GET. GET gets the data for a specified resource. Now we've used this verb many, many times when we did kubectl get and then specified some resource, like Pods or nodes or deployments. What happens behind the scenes is kubectl converts this into the necessary app request to submit this verb into the API server and retrieve that data for us. There's also a POST operation, which is used to create a resource, DELETE for deleting a resource, and PUT where we can create a resource if it doesn't exist or update an entire resource if it does exist in our cluster. And then, finally, PATCH if we need to modify a specified field of a particular resource when we compare this with PUT, which is going to update the whole resource or the whole object. In this case, PATCH will update an individual field or an individual element of a particular API resource. In addition to the API verbs that we just looked at, there are some special API requests that can be made of the API server that facilitate cluster operations and workload management in our Kubernetes cluster. And first up is LOG. LOG is an API request that retrieves logs from a container in a Pod, and it'll stream that output back to the requester. Most commonly, we'll use kubectl combined with the LOG parameter to request this information from a particular Pod. Then there's EXEC. We can use the EXEC verb to execute a command in a container inside of a Pod and get that output back to the requester. This is also commonly used at the command line with kubectl exec. And, finally, there's WATCH. WATCH is an API request that allows us to get change notifications on a resource with streaming output. And let's talk about what that really means behind the scenes. Each resource in Kubernetes has a resource version associated with it. And so when we instantiate a WATCH on an object or a resource in our cluster, when that version is changed on the watch, notifications will be sent to the client that are watching that particular version. Now a common use case for this is when you specify the ‑‑watch flag on a kubectl request so that you can get notifications on changes in state for objects. Things like deployment rollouts or Pod creations are common use cases for using watches, adding that ‑‑watch parameter to our kubectl command.

API Resource Location (API Paths) and API Response Codes
So, we've been focusing on the request, specifically the API verbs. Now, the other part of the equation to make a valid API request is specifying the API resource location, or the API paths. Now API paths are really thrown into two different buckets, like API groups. API paths are split across API groups. And so first we'll see the core API group, and then next we'll look at the named API group. And so in the core API group, we'll find that our URL path, or our API paths, the resource locations look like this. We'll see http://apiserver:port/, followed by the string API and then the version of the API that we want to work with, whether they be v1, v1beta1, and so on, and then slash the resource type that we want to work with, is it pods, is it persistent volumes, whatever it is. Now, if the resource that we're working with is in a namespace, we're going to get a slightly different URL representation. So here we see http://apiserver:port/, then it's the string API followed by the API version, again v1beta1 or v1, whatever it is. After that, we have the string namespaces followed by the actual namespace name. Inside of that namespace, we'll see the resource type, whether it be a pod or whatever it is, as part of the core API. And then if we're addressing an individual resource then it's going to be that resource's name. Now when we're looking at resources that are in an API group, like a named API group like storage or RBAC and so on, we're going to get a slightly different URL representation. Here we see from left to right, http://apiserver:port and then the string APIs, plural, followed by the group name. And that would be something like storage or app or whatever it is for the particular API group that we're working with. Next we'll see the API version. Again, that could be v1, v1beta1, and so on, followed by the resource type. If it's a namespaced resource, again we'll get a slightly different representation, but similar to what we worked with above here. We see http://apiserver:port, again followed by the string APIs, the API group name, the version of the API server that we're working with, and the string namespaces, followed by the namespace name, the resource type, and then the resource name. Now this might seem a little abstract, but don't worry I kind of wanted to get this in front of you. In the presentation portion of the course, we're going to do lots and lots of demos together where we'll use kubectl to make various requests of resources in our cluster. And so we'll look at different API paths and also the various verbs that we'll use in those requests. Once we've submitted an API request into the API server with the combination of API path and the verb or operation that we want to execute, then the API server's going to process that request that we made and then send us a response back based on what happened inside of the API server. So let's look at common API responses that we'll get back from the server. And we're going to group them into three classes, the 200 for success, 400 for client errors, and 500 for server errors. Now, if I submit a request and that request is processed and everything goes okay, well then we'll get back a response code of 200, which translates to OK. Now if I submitted a request with a POST operation into Kubernetes and it responds back with a 201 Created, that tells me that the resource that I requested was successfully created. Now there are some scenarios where the request that we're submitting is going to be processed asynchronously, meaning Kubernetes is going to go and deal with that in the background. And in those scenarios, we'll get back a 202 Accepted response telling us that the response was successfully accepted. Now let's jump into the client errors bucket here. In client errors we'll see commonly 401, or Unauthorized. Basically, I did not authenticate to the server correctly. There's also 403 Access Denied. We're going to see this in scenarios where the user has authenticated to the web server, but they don't have access to the API resource or API path that they're trying to interact with, where they can't use that particular operation on that particular API path. This is going to come from the security policies applied to both the user and the resource in the cluster. And then finally, there's 404, or resource Not Found. That's going to pop up when I try to interact with an API resource at a particular API path that's just not there. So very similar to what you see in regular web servers when you try to access a resource that's just not available on the web server. And then finally server errors, and those most commonly roll up into one error, 500 or Internal Server Error. Now, there are many more response codes available inside of Kubernetes. These are kind of the primary ones that we're going to see in everyday workings with our API server.

Anatomy of an API Request - A Closer Look
Now let's walk through the lifecycle of an API request from client to server, and the intermediate stages that happen in between those. And so first up is connection, then we go to authentication, then we look at authorization, and then finally, admission control. Now let's look at each one of these phases in much more detail. In the connection phase, we're basically trying to figure out, can we make a connection to the API server? And that's certainly going to be HTTP over TCP, but most commonly we'll see these connections as TLS‑encrypted HTTP sessions or HTTPS. Then we move into the authentication phase, we're basically logging in. We're trying to determine, are you a valid user? The user is going to be validated via an authentication plugin, which is modular, used to authenticate users via various authentication types such as certificates, passwords, tokens, and other various authentication plugins. If you're unable to authenticate to the server, you'll get a bad authentication request, and that's going to be visible in your HTTP response with a 401 error. Next up, in the authorization phase, we're trying to figure out can you, the user, perform the requested action on the resource that you're trying to operate on? In that case, it's going to be a combination of, can the user execute the API verb on the resource or API path that they want to interact with? In Kubernetes, actions are denied by default, and roles are established to allow users to access and perform actions on the object or resources that they have permissions to operate on. If you don't have permissions to perform the action that you want on the resource that you want to operate on, the API server will respond with a 403 error. And the final phase of our API request here is admission control. Admission control gives us administrative control over an API request with an admission controller. An admission controller is a piece of code that intercepts requests to the API server prior to persisting that object to the cluster store. When an object is being created, deleted or updated, that request is passed through the admission controller, which can modify or reject the request. An example of a modification is adding default values to an object, overriding object values with some configuration standards for our clusters or maybe asserting resource quotas or any configurable attribute of an object. And then the final part of admission control is validation. This is the part of admission control where the object being requested is validated using the validation routines for the objects that we want to work with. Basically, is this a well formed request for this particular API object? Now, all of these steps happen prior to our object being persisted to our cluster store, etcd.

Demo: Anatomy of an API Request
Now let's get into a demo and look at the anatomy of an API request. So we're going to look very deeply at what happens when we make requests from the client over to our API server. We're going to look at some special API requests, both watch, exec, and log, and look at authentication failures and missing resources, so we can see what to look for when things go wrong. Then we're going to wrap it up with a very close look at the API interactions between client and server when we create some objects, specifically deployments. All right, so here we are in VS Code, and we're going to kick off our demonstration starting with the anatomy of an API request. And the very first thing that we're going to do is create a Pod together, and I have some code here for that with kubectl apply ‑f pod.yaml, and there is just a simple Pod creation of the hello‑world Pod that we've used in previous demonstrations. So here we can see pod/hello‑world created. Let's go ahead and double check that, and use kubectl get pod hello‑world to make sure that our Pod is up and running, and it certainly is. So there we can see STATUS Running. Now, when working with kubectl, we can use the ‑v option to increase the verbosity of our request, and, well, what does that really mean? Kubectl is a client, and the API server is, well, our RESTful API server for our Kubernete cluster, and we can increase the verbosity or the output from the kubectl command, so we can get more insight into the interactions between our client and the server. And the first one that we're going to start off with is we're going to increase the verbosity with this ‑v parameter here to level 6, and that's going to give us a lot of information about the transaction between the client and server. And so let's go and do that together and review the output. So the code here you can see, I have kubectl get pod hello‑world ‑v 6. And so let's go ahead and run that code together. Now at the bottom here, we've got a couple lines of output, and we're going to go through each of those together. Now the first one is where it says config loaded from file, that's just reading in our kube‑config file for us to authenticate against the API server, or loading our certificates up, finding the API server, things like that. That next line there in the middle, we can see an API verb GET. That API verb is the operation that we want to perform against the API server, and that's the next part of this line of output here. We see https://172.16.94.10 on port 6443. That's the location of our API server. Now, the rest of that line there is the API path, and so we see it starts with /api, then our API version, v1, we see the string's namespaces, and then the namespace specifically is default, and so this Pod was created in the default namespace. After that, we see pods, or type of resource that we want to interact with, and then hello‑world, which is the individual Pod that we want to interact with. Now after that, we see the response code from the API server, we see 200 followed by an OK, and then the duration of the request, 9 milliseconds. And with this output, we can see the direct interactions between the client, which is kubectl, and the API server. This is the request made of the server and the response code back, right, the request consisting of the API verb, the API path, and also the response code. Now at the very bottom here, we can see the output that we're used to seeing when we use kubectl get pod where we have the name of the Pod, if it's up and running, and the number of restarts, and age, and so on. Now I'm going to scroll down a little bit in my code here and show you that we have some other verbosity levels that I want you guys to explore on your own, so you can see the information that's exchanged between kubectl or the client and the API server. And so as we increase the verbosity, we're going to get additional information and have some comments here in the code as to what you're getting as you increase the verbosity. So there we can see on line 12 and 13, when I increase the verbosity to 7, well, I'm going to get the same output of 6, but some additional information, in this case the HTTP request headers. So we'll see things like application type and the user agent that is specified in the request headers of the request sent to the server. And then as we increase to 8 and 9, gaining additional information with regards to the truncated response body, and then at level 9, the full response body. So you'll see the whole HTTP transaction and the data returned to kubectl in its non‑processed format. So let's go ahead and jump a little bit further ahead down to this batch of code here. And we're going to do some deeper‑dive API requests with the command line web browser cURL. Now, for me to use cURL against our API server, I'm going to need to authenticate that, and an easy way or a good way to authenticate against our API server is to use this command here, kubectl proxy. What kubectl proxy is going to do is make a connection from my local machine to the API server on a port, but what it's also going to do when it makes that connection is use my local kube config for kubectl to authenticate me against the API server. So now with that local connection, I can submit it to that local connection that was created, and I'm already authenticated to the API server, and so that's going to be relayed into our cluster API server, and I'm allowed to interact with that with tools like cURL. And so let's kind of walk through a demonstration so we can see that in action. So on line 23 here, I have kubectl proxy followed by an ampersand, which simply backgrounds this process, and so there we can see at the bottom here, starting a server on 127.0.0.1:8001. So that's my local proxy connection, and I'm going to make a request of that localhost address on that port, and that's going to be proxied or relayed over to the actual API server. And so let's go ahead and do that together on line 24 here. So from left to right, we see curl, which is our HTTP client, and we're going to request a particular URL. Now that URL is going to be the local kube‑proxy connection. So we see http://localhost:, or 127.0.0.1, on the port 8001. Now the rest of that API path is going to be the object that we just created together, and we got that API path when we used kubectl get pods ‑v 6, and we saw the API path in the output of that request. And so let's go ahead and walk through that API path together. We see /api, we see the API version, v1, /namespaces/default/pods/hello‑world, all the way out to that specific resource that we want to work with. And so this request is going to go and ask our API server for the JSON object at that API path. And so let's go ahead and run this code together. I'm going to pipe the output in ahead just to limit some of the output so I don't have to scroll so much because there is a lot of information coming back in this request. Now if we scroll up a little bit, we'll see some information that we're used to seeing. Here we see things like kind, and apiVersion, and metadata, right? This is the JSON representation of the object that we retrieved directly from the API server itself. And so using this technique, and we're going to build on this in later courses, this is a great troubleshooting technique when you need to interact with the objects to understand what's really going on with the state and maybe some information that's not exposed in the tooling that we have with kubectl. So let's go ahead and bring a prompt back up here, and bring our process into the foreground with fg, and then break out of that with Ctrl+C. So we just killed off our kubectl proxy.

Demo: Special API Requests - Watch, Exec, and Log
Now let's move on into our next demo. We're going to do some of those special API operations so that's going to bring that code into view here, and start off with a watch. What a watch is going to do is watch a particular resource based on the resource version of that particular object or collection of objects in our Kubernetes cluster. And so let's go ahead and kick off a watch on pods. And so, what we're going to do here is use the command kubectl and get pods ‑‑watch and I'm going to use the verbosity level ‑v 6, I'm going to put that in the background with an ampersand and what that's going to do is watch for any information on any pod operations in the default namespace. And when I initiated that command with the ‑v 6 parameter, it gave us the verbose output of the request and let's look at that request at the bottom. There are two lines I want to call out to you here. The first one we see a get operation against the API path to all of the pods in a default namespace. And so let's walk through that line together and kind of get a feel for what that looks like. We start off with the API server, so 172.16.94.10 again on port 6443. We see /api/v1/namespaces/default. We're going to operate in a default namespace. Now we're operating on the object or resource pots. Now we see limit 500 because it's going to limit the output that it retrieves from the cluster via the API server. We see 200 OK for our response code so we know that we were able to retrieve a list of pods. Now we initiated a watch as well, and so the standard output of the command would give us our normal kubectl get pods output, but since we initiated a watch, we're going to get additional information as those resources change. And let's look at that second API request here at the bottom. So here we see a get operation again against our API server. We see the API path is /api/v1/namespaces/default/pods, that same resource location that we've been working with, but we see an additional parameter there ?resourceVersion = 2691186, and then at the next line with a little line wrap there, we see &watch=true. So any time there is an operation on a pod in a default namespace, that resource version is going to change and the output is going to be streamed back to us from kubectl get pods and so we'll be able to see things like changes in state for these particular resources, in this case, pods. Now, let's go ahead and get a new line here at the bottom, and scroll back up to the top here and use netstat ‑plant | grep kubectl to get a list of the network connections associated with the kubectl process. You can see we have a persistent connection to the API server. so we see our local client which is actually still on the master. We see 172.16.94.10:42514, we see that as our local connection to the remote connection or the API server, which is resident at 172.16.94.10:6443, so we see an established TCP connection so as something happens on pods, that connection is already built, that output is going to be streamed from the API server back to our local kubectl process that's up and running with our watch, which we background it on Line 31. So let's go ahead and do something, let's go and delete a pod and see what happens. And so with kubectl delete pods, we're going to delete the hello‑world pod we've been working with. And so, here you can see at the bottom the streaming output as the state changes for the pods in the default namespace, in our case, the hello‑world pod. So we see starting off, we see hello‑world 1/1 Terminating, that's a state change. We initiated the terminating process. And then as it is watching and that state changes, we see it goes to 0/1 which means the container shutdown and then it takes a couple more samples, and then finally, the pod is shut down and deleted, but let's go ahead and bring our pod back with kubectl apply ‑f pod.yaml and watch the creation of a pod with our kubectl get pods watch operation. It's going to run that there and we can see as it changes through the different statuses to bring our pod up and running, it starts off with pending, goes into container creating, and then shifts over into the running state. Now that actually is completed, even though we didn't get a new line because of the way that the watch works and it doesn't grab a new line for us, so if we go ahead and simply hit an Enter key right there, we'll get our prompt back. So let's go ahead scroll up a little bit further and go ahead and kill off our watch, I think we're done with that for now, so bring that into the foreground and then use Ctrl+C to break out of that. The second special API request we're going to look at today is accessing logs, and so let's go ahead and do that together. We're going to go ahead and start off with just grabbing the logs without any increased verbosity. So we say kubectl logs hello‑world and that's going to pull the logs from the container running in our hello‑world pod. And so, we can see there is one line of output here, that's the only line in the log on that container that's up and running. So we see server listening on port 8080. Now if I run that command again and increase the verbosity to the level 6, we'll see the API request is a little bit different than just a regular kubectl logs. So let's walk through the two API transactions at the bottom here. So first up, we see a get against our API server all the way out to the API path for this particular pod. So we see that it comes back with a response code of 200 OK. And so that's checking to see if that pod is actually there, and if it finds that the pod is there, it makes a second request to grab the logs from that pod. And so in this case here, we can see a second API request with the get operation against that same resource, so we see /api/v1/namespaces/default/pods, then the pod name hello‑world, but there is an additional entry on the API path, this is /log and that's the API location to retrieve the log from that particular pod. You get a response back of 200 Ok, and at the very bottom there, we see the log output from that operation server listening on port 8080. Now I'm going to combine two techniques here, we're going to combine using kubectl proxy to make a direct request of an API resource using curl again, that command line web browser, and so let's go ahead do that together. So kubectl proxy with an ampersand on the end to background it. Our proxy is still listening on our localhost address at 8001. Now I'm going to make an API request of that same API path to retrieve the log directly from the API server, not necessarily using kubectl. We're using curl, the command line web browser. And so let's look at this command from left to right. We see curl, then the location of our local kube proxy, which is relaying that information or that request into our API server. So that's a localhost:8001 API path out to the resources /api/v1/namespaces/default/pods/hello‑world, and then finally, log. And so when we execute this code, we're going to make a request of the API server on that path and what do we get back? We get the actual log for that individual pod, so there we can see server listening on port 8080. And so with this authenticated kube proxy, we can retrieve any API resource on the API server. In this case, we went so far as to retrieve an individual containers log running in our hello‑world pod. Alright, so let's go ahead and foreground that kube proxy again and kill it off here and get back to our command line.

Demo: Authentication Failures and Missing Resources
All right, so let's clear our console and start off fresh and move forward into the next demo where we're going to look at some response codes to various events, things like authentication failures, missing resources, and also what happens when we create some objects. And so let's start off with an authentication failure. The first thing that I'm going to want you to do is to create a backup of your kubeconfig file, and that's what we're doing here on line 50. I'm copying our current kubeconfig file to a file named config.ORIG to save that as a backup because what we're going to do next is make an edit to that file to break our authentication from our kubectl client to our API server. And the way that we're going to do that is by editing the kubeconfig file, and then inside of here, we're going to search for kubernetes‑admin, and we're going to jump down to the section that defines the username. So here we see, at approximately line 29, we see name: kubernetes‑admin. And what I want you to do here is to throw a 1 on the end here so that it breaks the authentication to our cluster because this username doesn't exist in our cluster. So let's go ahead and save that out. And now let's execute a command and see what happens now that are kubeconfig file is broken. And so what we're going to do here on line 66 is try to execute a command against the API server, so kubectl get pods. And what we're going to do is we're going to see what happens when this occurs and look at the response code that we get from the API server. So to see the response code, I've added the ‑v 6 parameters to increase the verbosity. Let's go ahead and run this code together. And what we'll see at the bottom here is it asks us for a username because it couldn't find that user using certificate‑based authentication because Kubernetes‑admin1 doesn't exist. And so just enter any username here. It could be anything. I'll just enter my name and a fake password. And what we'll see is we're going to get some output at the bottom here. If we scroll up past all of the output here at the bottom to the actual response from the API server, what we'll find is that the response code that we get for kubectl get pods is 403 Forbidden. We failed to authenticate to the API server, and we were not able to retrieve a listing of Pods. And so that output there indicates that we were prevented from doing that, and we failed to authenticate. And so let's go ahead and put our kubeconfig file back. We'll copy our backup over to the correct location for the kubeconfig file, and that's what we're doing here on line 69, copying config.ORIG and overwriting the broken kubeconfig file. Before we move forward, let's be sure to test out access to the API server to make sure everything is back the way we want it to be. And with kubectl get pods there, you can see we were able to retrieve a listing of Pods for the Pods that are up and running in our cluster. There we see hello‑world Pod. So we know we were able to authenticate to the cluster. And so now let's check out what happens if we try to get a resource that doesn't exist. And so I have some code here on line 75, kubectl get pods nginx‑pod ‑v 6. And so that Pod doesn't exist in the cluster, but what I want to show you here is the response code that we get back when we try to ask for a resource that doesn't exist. So let's go ahead and run that code and check out the output at the bottom, scrolling back up to the top of the output here, we can see that our kubectl get pods nginx command did execute. And we can see the GET operation returned a 404 Not Found because that Pod was not found inside of the API server. Now moving forward, let's look at what happens when we create a deployment and look at the interactions between our kubectl client and the API server. And so on line 79 here, I have some code to create a deployment‑‑kubectl apply ‑f deployment.yaml ‑v 6. Let's check out the interactions between kubectl and the API server. And so in the output at the bottom here, we see the GET operation against the default namespace for the deployments named hello‑world, and the response code is 404. So the first thing that it does when we try to create a deployment is it checks to see if the deployment exists. If it doesn't exist, then it's going to go ahead and post the deployment to the API server. And so that's the next operation that we see. We see POST in the default namespace for deployments. It's going to apply the configuration that we want to deploy, and then the response code is 201 Created. And then the standard output at the bottom, we see deployment.apps/hello‑world created. And so that interaction of testing to see that it exists and then posting it to actually create the deployment is how kubectl interacts with the API server when you try to create resources. Let's see if our resource was successfully created with kubectl get deployments. There it is, there's our hello‑world deployment that's up and running. And so for the last part of our demo, let's check out what happens when we delete resources for the API server. So on line 85 here, I have kubectl delete deployment hello‑world ‑v 6. And then in the output at the bottom, we see the DELETE operation for the default namespace against the deployment/hello‑world. We get a 200 OK because that deployment was deleted. And then we have our normal output, deployment.apps hello‑world deleted. And then just to wrap up the demo, let's go ahead and delete that Pod that we created at the beginning of the demo with kubectl delete pod hello‑world.

Module Summary and What's Next
Here we are at the end of our module, and we covered oh so much information. We looked at the Kubernetes API and the API server, and also at working with Kubernetes objects, specifically defining objects, API groups, and API versioning. We also looked very closely with a deep dive into the anatomy of an API request, so we know exactly what's happening between our clients and the API server. Well, that's a wrap for this module. Please join me in the next module, Managing Objects with Labels, Annotations, and namespaces.

Managing Objects with Labels, Annotations, and Namespaces
Introduction, Course, and Module Overview
Hello, this is Anthony Nocentino with Centino Systems. Welcome to my course, Managing the Kubernetes API Server and Pods. This module is Managing Objects with Labels, Annotations, and Namespaces. So let's take a look at where we are in the course so far. We just wrapped up our deep dive into the Kubernetes API with the module Using the Kubernetes API. Now let's learn how we can use some techniques such as labels, annotations, and namespaces to organize resources and objects in our Kubernetes cluster in the module Managing Objects with Labels, Annotations, and Namespaces. In this module, we're going to discuss organizing objects in Kubernetes, and the techniques that we're going to use to organize objects are going to be namespaces, labels, and annotations. Once we have those principles behind us, we're going to learn how Kubernetes uses labels to manage critical system functions such as managing services, controlling deployments, and workload scheduling in our cluster.

Organizing Objects in Kubernetes
Now in this module, we're going to learn how to organize objects in Kubernetes. And the three primary methods with which we can organize objects in Kubernetes are going to be namespaces, labels, and annotations. Now we will learn throughout the module the appropriate place to use these different types of organizational methods. But from a high level standpoint, let's go ahead and touch each one of these right now. You'll want to use namespaces when you want to put a boundary around a resource or an object in terms of security, naming, or resource allocation. You'll want to use labels when you want to act on an object or groups of objects or influence internal Kubernetes operations. And then, finally, annotations are when you want to add just a little bit more additional information or metadata even about a particular object or resource.

Introducing and Working with Namespaces
Now, for the first organizational method in Kubernetes that we're going to look at, we're going to check out namespaces. What namespaces give you the ability to do is to subdivide a cluster and its resources, conceptually giving you a virtual cluster or virtual clusters to deploy objects into. You deploy objects into a namespace. The primary reasons why you want to use namespaces in your cluster is to give you some resource isolation or organizational boundaries inside the cluster. Now, some common grouping concepts behind using namespaces is multitenant clusters. Maybe you want to group on application environments such as prod, QA, and test or different application stacks that you're deploying into your cluster. Maybe you want to group on users or application owners or different teams, and things like that. It comes down to what are the business requirements for your organization, and how do you want to subdivide your cluster leveraging namespaces? Perhaps mixing environments in your cluster like prod, QA, and test isn't a good idea and you want to use separate clusters for that, but the opposing force to that is for a business standpoint, well, maybe you need to leverage all of the resources in your cluster and you do need to mix environments within your cluster. So those are some of the design decisions that you'll have to go through and look at the business requirements you're trying to achieve leveraging namespaces. Now, from a technical standpoint, we can leverage namespaces to assert some resource control upon namespaces, limiting to things like CPU, disk, RAM, number of Pods, and so on. So let's continue looking at namespaces and the benefits that they can provide for us in our Kubernetes cluster. First up is it's a security boundary for role‑based access control. We can limit who can access what inside of our cluster based on namespaces. We can leverage both role‑based access control and namespaces in different combinations that get users access or prevent access to resources in the cluster. We also can leverage namespaces as a naming boundary. I can have a resource in two separate namespaces have the same name, because a namespace is a boundary around naming. I do want to point out that a resource can only be in one namespace at a point in time. And then finally, I do want to drill this point home before we move forward, that Kubernetes namespaces is an organizational construct inside of our cluster. It has nothing to do with the concept of the Linux operating system namespace. Working with namespaces. When you want to work with namespaces, much like any other resource inside of Kubernetes, you have the ability to create, query, and delete a namespace. Leveraging namespaces, we can also perform operations on objects inside of a namespace. Perhaps I need to delete all of the Pods in the namespace collectively, and I can do that. And I can also delete the entire namespace and all of the resources within that namespace. Now, some objects in Kubernetes are namespaced, meaning they can be placed into a namespace, and some aren't. So let's look at some that are and some that aren't. Some of the resources that can be namespaced are Pods and Controllers and Services, right, workload that gets deployed into the cluster. And I like to think the things that aren't namespaced are, generally speaking, the physical things, things that we can touch, things like PersistentVolumes and Nodes. Now, conceptually, let's go ahead and look and see what a namespace looks like in our cluster. So in our cluster, let's say we went ahead and declared two namespaces, NS1 and NS2, and so these are two organizational units inside of our cluster, we have two namespaces for us to deploy resources into. And so let's go ahead and start that process. We're going to deploy some Pods. So here you can see we have two Pods with the same name, Pod1, deployed into each of the namespaces in our cluster. They can have the same name, because a namespace is a naming boundary. We can kind of continue that concept here, you can see Pod2 coming along. Again, normally that would be a naming conflict within a namespace, but since they're in two separate namespaces, we have that there. Now this kind of design pattern could be used to deploy for down‑level environments, multitenant clusters, and things like that. We can go and add other resource boundaries or security boundaries to these namespaces for administrative control over who can do what in our cluster. Now, continuing to look at namespaces, let's talk about some of the namespaces that are available by default for free when you create a Kubernetes cluster, and also how we can create our own namespaces. Now, some of the namespaces that we get out of the box in a Kubernetes cluster are the default namespace. The default namespace exists for when you deploy resources into the cluster and don't specify a namespace. This is the default namespace. There's the kube‑public namespace. This namespace is created automatically, and is readable by all users in the cluster. This is commonly used to store shared objects between namespaces for access across the whole cluster, so commonly used to store things like config maps. There's kube‑system. Inside of kube‑system, you'll find the system Pods, so things like the API server, etcd, the Controller Manager, and kube‑proxy, and so on. Now, in addition to these namespaces here, we have the ability to create user‑defined namespaces for us to be able to deploy a workload or resources into, and we can create a user‑defined namespace in one of two ways. We can do it imperatively at the command line with kubectl or declaratively in a manifest in YAML or JSON.

Creating Namespaces and Creating Objects in Namespaces
So let's discuss how we can create namespaces and also create objects in namespaces. And we're going to start off with creating the namespace declaratively in YAML. And so just like any other manifest, we start off with our API version, and it would declare the kind, in this case, a namespace. To give our namespace a name, we'll go under metadata, and we see name. In this case, the name of our namespace is going to be playgroundinyaml. Now to declare an object in a namespace, let's go ahead and start that process. So here you can see my API version is going to be apps/v1. The kind is going to be a deployment. And for me to deploy this deployment into a namespace, I specify the metadata and declare the namespace as the existing namespace already, in this case, playgroundinyaml. To do this imperatively at the command line, I can use kubectl create namespace and then just give my namespace the name, so in this case, playground1, creating a totally separate namespace in our cluster when compared with the example up top here. If I want to declare an object in the namespace, I can use this syntax. I say kubectl run. The resource in this case is going to be nginx. I'm going to feed it an image named nginx. And at the end there, you can see ‑‑namespace, and then the name of the namespace I want to deploy this object into‑‑playground1.

Demo: Working with Namespaces and Objects in Namespaces
Time for our first demo in this module, and we're going to do a lot of work. We're going to create a namespace, add some resources to a namespace, query a namespace, interact with some of the resources in a namespace, and then finally delete all of the resources in a namespace by deleting a namespace. So here we are in VS Code. Let's go ahead and get started with our first demo, and at the bottom here, you can see I have a connection open to c1‑master1. The first thing that we're going to do in our Kubernetes cluster in this namespace demonstration is get a listing of all of the namespaces that are available in our cluster, and to do that, we can use kubectl get namespaces, and that's going to list the three currently‑provisioned namespaces in our cluster. We have default, kube‑public, and kube‑system. Now I'd like to show you how to ask Kubernetes what resources can be in a namespace and what resources can't be in a namespace. And we've used this command so far in this course, kubectl api‑resources, and that lists all of the API resources that our API server knows about. And in this case, I'm going to add an additional flag here that says namespaced=true. What that's going to do is give me a list of all of the API resources or objects that can be part of a namespace. And so let's go ahead and run this code here, and I'll pipe that in ahead just to limit some of the output, just to get a sample of what is a namespace of an object that's available on our Kubernetes cluster. And so here you can see things like pods, and podtemplates, and endpoints, and things like that. On the right here, we can see a NAMESPACED column and the attributes for all of those is set to true. And so these are the resources that we can provision or deploy into a namespace. Now let's flip the coin a bit and go ahead and set that the false. So we're going to ask Kubernetes, kubectl api‑resources ‑‑namespaced=false, pipe that in ahead. And now let's get a listing of some of the objects that can't be part of a namespace. So in the output here, we can see various resources such as namespaces, nodes, and persistentvolumes. These are all resources that can't be part of a namespace. Now I do want to go ahead and pull some additional information about our namespaces, and we're going to use kubectl describe namespaces to give us that, and that's going to give us some deeper‑dive information about all of the namespaces that are up and running on our cluster. And let's walk through some of this output on the bottom here. So here we see kube‑system, right, the name of our namespace. There's no labels or annotations associated with it, and we can see its Status is Active. Namespaces actually do have status. When they're active, they're up and running, and provision, it could be in the state where you're deleting the namespace, and it's terminated and shutting down the resources in the namespace to be deleted, and then deleting the namespace. Now we also have some additional information at the bottom here. We see No resource quota and No resource limits, right? So there's no resource constraints around this particular namespace. If we scroll up in our output, we see the kube‑public namespace with very similar output, and also the default namespace. So let's say we wanted to pull that detailed information for an individual or named namespace. Here, we can do the same thing. We say kubectl describe namespaces, and just at the end there we specify the individual namespace that we want to operate on, holding back that describe data for that particular namespace, in this case, kube‑system. Now let's say we wanted to get a listing of all of the pods across all of the namespaces in our system, so regardless of where they are in what namespace they're provisioned into it. And so here we can do that with kubectl get pods ‑‑all‑namespaces. Let's go ahead and run this command here, and we can see all of the pods across all of the namespaces in our cluster. In this case, I only have pods running in kube‑system because we have no workload deployed into our cluster yet. Now that was just for pods. What if I wanted to ask Kubernetes to give me all of the resources across all of the namespaces? And so let's go and see how we can do that. We can say kubectl get all ‑‑all‑namespaces, and that's going to give me a listing of everything that's deployed no matter what resource type it is across all namespaces, so a useful debugging command when you have to get a big‑picture view of the world. And at the bottom here, we can see some of the output from the resources across all namespaces. You have replicasets, and deployments, and daemonsets, and services, and things like that, all of the system‑level functions that we've deployed when we built this cluster together in our previous course. So let's go ahead and keep moving along, and let's go ahead and learn how we can query a particular namespace or a named namespace. So far, we've been working with getting resources across all of the namespaces, and if I want to get the pods just for an individual namespace in our cluster, I specify it like this, I say kubectl get pods ‑‑namespace kube‑system where that kube‑system is the namespace I want to query. And again, we get our list of all of the pods in that individual namespace. Now let's go ahead and move forward and learn how we can create some namespaces together, and we're going to start off with doing this imperatively, or at the command line. I'm going to say kubectl create namespace, in this case I'm going to say playground1. Let's go ahead and run that code there, and you can see at the bottom we get output that says namespace/playground1 is created. Now I want to go ahead and take another shot at creating a namespace named playground1 as well, but notice that there's a capital P there. let's go and see what happens here. We get an error, The Namespace Playground1, with a capital P, is invalid. It fails a regular expression match for the naming convention for namespaces, it needs to be all lower case, alphanumeric, things like that. And so we get some good detail about what's expected in the error there. So in this case, it's not that it's a duplicate name, but it's going to throw an exception that it can't create a name with an uppercase letter. So let's go ahead and declaratively create a namespace now. I'm going to go ahead and look inside of this file here that I have. This will be available in the downloads for you. At the bottom here, inside of namespace.yaml, we can see the API version for the resource that we want to create is v1, kind is Namespace, metadata, and then the name of the namespace is going to be playgroundinyaml. And just like any other YAML file, let's go ahead and deploy that into our cluster, and here we say kubectl apply ‑f namespace.yaml, run that there, and we can see that our namespace, playgroundinyaml, was created. So if we go ahead and get a listing of all of the namespaces again with kubectl get namespaces, here you can see we have those two additional namespaces that we just created, playground1 and playgroundinyaml. Now let's go ahead and deploy a deployment into our playground1 namespace, and I have a deployment.yaml up here, let's go ahead and take a peek inside of that, and here we can go through kind of at the top‑level output here, we have the API version for the deployment that we want to deploy as apps/v1, we see the kind is Deployment, and in the metadata section here, we specify the name, and any labels associated with its resource, and then the namespace as an attribute of metadata, and we're going to assign it to the playground1 namespace. And so let's go ahead and jump back over to our demo script here, and send that deployment into our Kubernetes cluster, and we're going to do that with kubectl apply ‑f deployment.yaml. That's going to create our deployment hello‑world in the playground1 namespace. Now I want to do one more thing here to show you how to do that imperatively at the command line. So here, I specify the fact that I want to create a hello‑world pod. And so let's walk through this syntax here. So here we see kubectl run hello‑world‑pod, and we're going to specify the image as our hello‑world‑app that we've used several times so far, and we're going to use a pod generator just to create an individual pod. Now that last line there is the most important , ‑‑namespace playground1. So this resource is going to be at the command line, we're imperatively creating this resource and specifying the namespace that we want it to go into. So let's go ahead and use kubectl get pods to see where our workload is, and, well, there's no pods. Where are the pods? Well, we deployed them into a namespace, and when we use kubectl get pods, well, we're asking for the pods from the default namespace. And so let's go ahead and specify the namespace to which those pods live in. And to do that, we can use kubectl get pods ‑‑namespace, and then the namespace that we deployed all of that workload into, playground1. So we should see the four that were associated with the deployment that we created and the one individual pod that we created at the command line imperatively. Now if you get tired of typing ‑‑namespace, you can shorten it up to ‑n, and we'll get the same output here. So we just say kubectl get pods ‑n playground1, and get the same output if you get a little tired of typing ‑‑namespace all the time. Now if we want to work with a particular namespace and kind of get a feel for everything that's running inside of that namespace, well, earlier we specified kubectl get all across all namespaces, and we saw everything. Well, we can use that same concept against a particular namespace, and we can say kubectl get all in a named namespace, in this case playground1, let's go ahead and run that code there, and we can see the various resources in this particular namespace. So we have the five pods that we deployed, we have our deployment, and then we have the replica set that supports that particular deployment.

Demo: Performing Operations on Objects in Namespaces
Now let's go ahead and move forward and see how we can perform some operations on resources in a namespace. And so at line 61 here, I'm specifying kubectl delete Pods all in the namespace playground1. So this is going to attempt to delete all of the Pods in this particular resource. And so let's go and do that together and see what we get. So there we see the four Pods associated with a ReplicaSet and that one Pod that we deployed imperatively are being deleted. Now let's go and check the status of our deletion with kubectl get pods. I'm going to specify ‑n for the shortened version of the namespace parameter and then the namespace playground1. Let's go ahead and execute, oh hey, what happened here? I thought I was supposed to delete all of the Pods across the namespace. Well, I did. I deleted all the Pods. In fact, all the Pods got deleted, and then they were recreated by the ReplicaSet that supports that deployment. We deleted the Pods but not the deployment. The individual Hello World Pod that we created imperatively, well, that's gone. But Kubernetes did what it's supposed to do. It deployed four new Pods after we deleted, the Pods that were part of that ReplicaSet, which was part of our deployment. So if we want to take it all out, if you want to get rid of all the resources in a particular namespace, let's look at how we can do that because, in this case, I want to get rid of the Pods, I want to get rid of the deployment and the supporting ReplicaSet. So let's go ahead and specify kubectl delete namespaces playground1. Now what's going to happen is it's going to delay the deployment, the ReplicaSet, and all the Pods associated with it behind the scenes inside of that namespace. In fact, it's going to delete anything that is inside of that namespace. All right, so with that namespace deleted, let's go and delete the other namespace that we created together just to clean things up. And if I go ahead and ask Kubernetes to give me all the resources in the default namespace, I don't see anything there. And if I ask Kubernetes to get everything across all namespaces, this should just be the system level stuff. Let's go ahead and scan through it. Here we see all of the Pods as part of our kube‑system. We see the services, DaemonSets, deployments, and ReplicaSets associated with our cluster function. So all of that user workload that we created together is gone.

Introducing and Working with Labels and How Kubernetes Uses Labels
Now the second method that we're going to use to organize workload or objects in our Kubernetes cluster is labels. Labels are used to organize resources, such as pods or nodes, or honestly, any resource in Kubernetes. With labels we can leverage label selectors to select or query those objects, and what that's going to do is return a collection of objects that satisfy the search conditions that we provide in those label selectors. What this enables us to do is perform operations on those collections of resources, like pods. Perhaps I want to query a subset of pods and perform some operation on all those pods, like delete, or some other administrative operation. Now it's not just for us to perform operations on workload; labels are also used to influence the internal operations of Kubernetes, and we're going to look much more closely at that later on in this module. So exactly what is a label? Well, a label is a non‑hierarchical key/value pair. We assign a key, we give it a value, and that is then associated with a particular resource in our cluster. We can query on the key and retrieve that value. Now, objects in Kubernetes can have more than one label per resource. And what that gives us the ability to do is that enables us to build more complex representation of the state of our system and query on those attributes or query on those labels associated with the objects in our cluster. From a syntactical standpoint, keys can be 63 characters or less, and values have to be 253 characters or less. If we need to exceed that length in terms of value or data that we need to store in a label, there's another method in which we can do that, and we're going to look at that later on in this module. Now, let's discuss some of the operations that we can perform when using labels. Well, first off is we can create resources with labels already assigned, and we can do this both imperatively at the command line with kubectl or declaratively in a Manifest with YAML or JSON. That gives us the ability to create resources with labels already assigned. If we have something already deployed in our cluster, we can edit an existing resources' labels, and we can assign new labels or overwrite existing labels, if we need to. Now, let's look at what it takes to add and edit labels in resources in Kubernetes. And first off, let's look at how we do this declaratively in code in YAML. And so as we start off with any other YAML definition, we start off with the API version. In this case, we're going to work with a pod. Now, in the metadata section, we've gone ahead and used the name parameter many, many times before. In this case, name is going to be nginx‑pod. Here's where we'll go ahead and assign our labels. We specify labels, and then the key and the value for the label that we want to specify. In this case, the key being app, and the value being v1. We can specify multiple labels in the section as well. We can say tier equals PROD, assigning the tier key with the value POD. And then we go about filling out the rest of the Manifest, in this case, the spec section of our pod that we want to deploy. Now, that's declaratively in code. Let's look at how we do this imperatively at the command line with kubectl. We say kubectl label, then a resource type, in this case pod, then a resource name, nginx, and then specify the keys and the value, tier=PROD, app=v1. If we need to change a label, let's go ahead and look at the syntax for that. From left to right here, we see kubectl label pod nginx. We're going to change tier from PROD to tier to DEBUG, so we see tier=DEBUG, and then again app=v1. Now, we also need to add the ‑‑overwrite parameter to overwrite that label that already exists, changing it from PROD to DEBUG. Now, to delete a label, let's look at the syntax for that. We say kubectl label pod nginx, and then the label or the key that we want to remove, in this case, app followed by a minus sign, and that will remove that label, that individual label app from this particular resource. So we've talked so far a lot about the semantics of using labels in Kubernetes, but let's get a graphical representation as to why we would use labels in our Kubernetes cluster. So let's say we have a cluster up and running, and we've deployed some workload in it. Now, what if I needed to work with a subset of these pods or these workload in this particular cluster at a point in time? Well, I can leverage labels and assign labels to particular types of workload. Perhaps these three pods with this blue label here are part of one application, and these three pods over here with this magenta label are part of another application. I can use labels to write queries and select these subsets of pods to perform operations on them. So in Kubernetes, we can query using labels, and we're going to find some selectors to help us execute these queries. And so let's walk through some examples at the command line here. So we can use kubectl, and say kubectl get pods, and we've done this many, many times where that gives us a listing of all the pods. Now, if we specify ‑‑show labels, that's going to list all the labels associated with those pods, and honestly, this could be any resource. That could be nodes or services, whatever it is. If we wanted to find a selector and perform a query, here's how we do that. So kubectl get pods again, ‑‑selector, followed by the query that we want to execute. The selector being tier=prod. So this is the label where the key is tier and the value is prod. So, any pods that match this selector where the key is tier and the value is prod will come in the output list of kubectl get pods. Let's go a little bit further and look at another example. Kubectl get pods. If I get tired of typing ‑‑selector, I can switch to ‑l, and I can perform a more complex query. So here you see 'tier in (prod, qa)'. And so anywhere my label has a key of tier and a value of prod or qa will come in the output list of this kubectl get pods command because of what we defined in that selector and the labels that satisfy that query. Another example being kubectl get pods, and say ‑l. I can say 'tierr notin (prod, qa)'. That's going to get me any label that doesn't satisfy that query. And then, finally, this doesn't have to be just for pods, it can be for any resource in Kubernetes. So I can say kubectl get, whatever resource I want it to be, in this case, nodes, and specify the labels, either ‑‑show‑labels, or define a selector and build a query to get me that subset of those resources that I want to work with. So let's look at how Kubernetes uses labels. So far, we've looked at how we can use labels to get collections of objects or resources from Kubernetes at the command line using labels and selectors, but let's look at how Kubernetes uses this internally for some of the functionality that it provides to us. First up is Controllers and Services match Pods using selectors. This is how Kubernetes knows if this subset of pods belongs to this deployment and that ReplicaSet, or that's how Kubernetes knows that these pods are part of a particular service. They match on the label selectors so that they can figure out who belongs to which resource. In addition to controllers and services, Kubernetes also uses labels to influence the scheduling of pods onto specific nodes in our clusters. We see this commonly when we have special hardware where we have local SSDs or GPUs, and I want to say, hey Kubernetes, I need you to run this pod on this type of hardware in my cluster. And you don't really want to get in the business of pinning a particular pod to a particular node name because, well, that node name might change. But you can leverage labels to assign a label to a subset of nodes, or a particular node, but you can then in the future move that label to another resource or another node in your cluster, and so we get away from pinning it to particular nodes. And how this will all work? Behind the scenes, it uses label selectors. Now, in the next upcoming slides, we're going to go through each one of these scenarios specifically to get a visualization of how this all works. So let's go ahead and start that now.

Using Labels for Services, Deployments, and Scheduling
So let's see how Kubernetes uses labels and selectors to find out which pods are members of services in its cluster. So let's say we have a cluster up and running and we deploy a collection of pods and we front end that with a service, in this case, an HTTP service. We're going to go ahead and expose that to our clients, but how does Kubernetes know if the pods on the left here in our cluster are members of this HTTP service? Well, it does this with labels and selectors. In the definition of our service, we specify a selector that is used to query the pods to determine who are the valid endpoints for this particular service. And so in this case, let's say we have a selector, like A, and in our pods, well these pods have labels that match. So these pods will satisfy the selector for that particular service. All those endpoints for these individual pods will be registered as part of that service and that traffic will be distributed to them. Now let's say we come along and administratively we remove the label from one of these pods. What's going to happen is this individual pod is going to be deregistered as an endpoint from this service and workload is no longer going to be distributed to it. So we just look at services and how they use labels and selectors to determine who is a member of a service, let's look at controller operations, specifically deployments, and how we can use labels and selectors to determine which pods are part of the replica sets associated with deployments. So in a cluster here let's go ahead and say we instantiate a deployment and our deployment kicks off a replica set with the label R1. That's going to instantiate a replica set and create pods associated with that replica set. Now, each one of these resources will have a label and a selector for the lower level resource, so the deployment will have a selector that queries the replica set that makes that part of its active deployment, and then the replica set will have a selector associated with it that will then select the pods that are a member of that particular replica set. Now, let's say administratively we come along and we want to move between this first version of our application and a new version of our application. What happens here, is, the deployment switches and kicks off the creation of that second replica set in our cluster, and then based on our rollout strategy, pods will start moving between the two replica sets with the newer version of our application. So these pods here are created part of the second replica set, which is part of the current replica set in our deployment. Now, if we come along and we remove a label for a pod that's a member of this replica set, well, that pod will no longer satisfy the selector for that replica set and so it will be removed from the replica set. Now, our replica set will possibly recover and create a new pod if it's configured to do so. So let's look at the final way in which we can influence the internal operations of Kubernetes with scheduling pods to specific nodes. Now, let's say we have a couple nodes in our cluster, and in this case, this cluster has some hardware, which, some nodes have SSD and some notes have GPU and some nodes have both. And so let's go ahead and assign some labels to these nodes that represent that state, and so let's say the node on the left here has both SSD and GPU and the node on the right here has just SSD. When we define our pods, we can declare in the pod spec a node selector, and in that node selector, we can specify a query for a key and a value that we want this pod to be scheduled to. And so pods that need SSD can be assigned to nodes that have SSD, pods that need GPU can be assigned to nodes that have GPU, and so as our workload gets scheduled, you can see here that the node on the right gets just SSD and the node on the left there gets pods with labels that match on both SSD or GPU. Now let's walk through the YAML code needed to define deployments and services in terms of how do we know which pods are part of a deployment or a service, and so, when specifying a deployment in YAML, we start off with declaring the kind is deployment, then skip a few lines and jump down to the spec for this particular deployment. In the spec, we're going to define a selector. In the selector, we're going to define matchLabels where we're going to define the label or the key and the value for the pods that are going to be members of this deployment, or under the control of this deployment. Now, further along in our deployment, we define a pod template. In a pod template, we define metadata where we specify the labels for the pods that are created by this template, right? These are the ones that are going to get stamped out by the replica sets that support this deployment. Here we specify the labels as the same as the matchLabel at the top here. We say run: hello‑world. So this is the same label or key and the value that we need to match and the selector at the top here. So these are critical that they match. These pods that are created with this template will satisfy the selector and be part of this deployment. We go along and fill out the remainder of our deployment and push that code into the API server, and everything will be created as we need. Now from a service standpoint, when we specify the kind equals service and fill out the remainder of the definition for a service, in the spec for a service, we also define a selector. In that case again, we'll define the key and a value for the label that we want to satisfy as part of this service. So we say run equals hello‑world. These two need to match, right? The pods that are created by our deployment, if we're going to front end those by a service, the labels in the pod template, as those pods are created as part of the replica set, will need to match or satisfy the selector of the service on the right here, and then once we have that, we go ahead and fill out the remainder of the data that's required for our deployment.

Demo: Working with Labels - Creating, Querying, and Editing
All right, so here we are in a demo, starting it and off with working with labels. Then we'll learn how to interact with Pods by both name and label. And with that behind us, we'll learn how we can influence Kubernetes resource management with labels and selectors, specifically in services, deployments, and Pod scheduling. Here we are in VS Code with a session open to c1‑master1 at the bottom here, and we're going to start off our demonstration with working with labels by creating a collection of Pods with labels assigned to each Pod. And I have a YAML file here that I want to review with you so that you can go through and see how we created these individual Pods with the labels. And here you can see at the very top our apiVersion is v1, kind is Pod, and then in the metadata section we see the name of our Pod, in this case nginx‑pod‑1, and there we define a collection of labels. In this case, I have three labels, the keys and then the values specified, so we have app equals MyWebApp, deployment equals v1, and tier=prod. And so those three labels will be assigned to this resource. We have the rest of the Pod spec at the bottom here defining the container, the image, the ports, and so on. Here we have another definition for nginx‑pod‑2. Again, we see the apiVersion, the kind, and in the metadata section, the information about the labels that we want to assign to this Pod here. So here we see app equals MyWebApp, deployment equals v1.1, and tier equals prod, and then again the Pod spec at the bottom. Let's look at one more. I have nginx‑pod‑3. In this case, the labels are app equals MyWebApp, deployment equals v1.1, and tier equals qa, the rest of the Pod spec. And I have another Pod at the bottom here that, if you want to look more closely at you can on your own, but there is nginx‑pod‑4. So with that, let's go ahead and jump over to our demonstration again and create these Pods in our cluster. Let me go ahead and feed that configuration into our API server with kubectl apply ‑f, feed in our YAML file, and in the output at the bottom there we can see nginx‑pod‑1 through 4 successfully created. Now the first thing that we're going to do when working with resources with labels, or, in this case, we're going to be working with Pods, I'm going to ask kubectl to get me a list of the Pods, but I'm going to add the parameter ‑‑show‑labels, and that's going to give us this output here. We get our standard kubectl get pods output with regards to the name, if it's ready, the status, restarts, and age, but we get an additional column of data that gives us all of the labels for these Pods. And so there's all the various labels that we specified in our Pods definition in the YAML file. Now, in addition to kubectl get pods, we can also look at an individual Pod's labels by using kubectl describe. So at line 13 here we can see kubectl describe Pod nginx‑pod‑1, and I'm going to go ahead and pipe that into head just to limit some of the output. And here in the output for this particular resource we see the three labels in our output that we defined in our YAML file. So let's go ahead and move forward in our code, and we're going to work with querying labels using selectors. And so let's go ahead and start that process. On line 16 here we see we have kubectl get pods ‑‑selector, and then I'm specifying a single label, tier=prod. Let's go ahead and see what that gets us in the output here. And we can see in the output 1, 2, and 4 are the Pods that match on that label selector. If we go ahead and flip it and do kubectl get pods, specifying ‑‑selector tier=qa, we should get, there you go, Pod number 3, so that's our set of Pods. Three are in prod; one is in QA. If we get tired of typing ‑‑selector all the time, we can shorten it up with ‑l. Let's go ahead and run that same query as we did before, and there you can see for the tier=prod selector, we get 1, 2, and 4 back. Now, in addition to running our queries with selectors, we can also ask Kubernetes, kubectl, to also show the labels as well. So in this case, what we're doing here saying kubectl get pods ‑l, specifying the label selector as tier=prod, but also asking to output all of the labels associated with the resources being displayed by kubectl get pods. So we get a visual on the additional labels associated with the Pods that match on the selectors specified. Now, let's go ahead and look at how we can use a selector to look for multiple labels and build more complicated queries using a selector in kubectl get pods. And so at line 22 here, we have kubectl get pods ‑l. And in this case, I'm asking to match on two labels, tier=prod and app=MyWebApp, and I'm also adding ‑‑show‑labels so I can see the labels in this output. And there we can see it limits our set to Pods 1 and 2. So these are our Pods that match on app=MyWebApp and also match on tier=prod. So let's go ahead and move forward and look at another query that we can execute. In this case, we're going to add a not equals to the selector that we're working with. And so here we can see kubectl get pods ‑l tier=prod, app!=MyWebApp. And I'm also adding the ‑‑show‑labels flag at the end there. And there we see we just get nginx‑pod‑4 because that's going to match on not MyWebApp. In that case, if we look at the label output at the bottom, we see app=MyAdminApp, and this Pod is output because it's going to satisfy that query by not matching on the app!=MyWebApp but matching on tier=prod. We can also specify an in clause in our query. So here we see kubectl get pods ‑l tier in (prod,qa). And what this is going to hit on, or match on, is where tier is prod or qa, and both of those will be satisfied in this query. And so here we see all of the Pods are either in prod or qa. And so if I invert that query, I can say tier notin prod or qa, what we should get here is an empty set, and we do, because no resources matched on that particular query. Now let's look at how we can output a particular label in column format when working with kubectl get pods. Now look at the code here on line 28. We have kubectl get pods ‑l, but capital L, so ‑L, and then I specify a key that I want. Let's go ahead and run that code and see what the output gives us. So we still get the standard kubectl get pods output, but we added an additional column there with the key equals TIER and the value for each of the individual labels, so prod, prod, qa, and then prod for all of the Pods that are currently running in our cluster. We can combine that with additional labels. In this case I'm going to tier and app, and that's going to add an additional column output to the output of kubectl get pods, so there we have a column for tier and a column for app, specifying all of the values associated with those resources in the kubectl get pods output. So let's look at how we can work with labels and edit an existing label and adding a new label. And so first up, let's go ahead and edit an existing label. So on line 32 here we see kubectl label pod nginx‑pod‑1. So I'm going to change a label for an existing label on this individual Pod. And there we see tier=non‑prod and I added the ‑‑overwrite parameter. And so let's go ahead and execute that code there, and that's going to change the value for the key tier for this individual Pod for that label. I run that code there, and at the bottom we can see nginx‑pod‑1 is successfully labeled. If we do a show labels for that individual Pod, we'll now see that that label is updated from tier=prod to tier=non‑prod, just for nginx‑pod‑1. Now if I want to add an additional label to a resource, on line 36 we see kubectl label pod nginx‑pod‑1, and I specified the additional key and value. So in this case, the key is another and the value is Label. Go ahead and run that code, and we can see that we successfully labeled our pod‑1 at the bottom here with the output. If we ask kubectl get pods to show labels again for that individual Pod, on the right we can see in the LABELS section, we have another=Label as part of that output set there, in addition to all of the existing labels. If we want to get rid of the label that we just added, we go ahead and specify kubectl label pod nginx‑pod‑1, and then we specify the label or the key, another, followed by a minus sign. Let's execute that, and there we can see that it says pod‑1 labeled, which is a little weird. Maybe it should say something like pod‑1 label removed, but it successfully did the work that we asked. Because if we do show labels again, we can see we're back just to the three labels that we started with for app deployment and tier. So far, we've looked at adding a new label and removing an existing label on individual resources. What if we wanted to perform an action on a collection of Pods? And so let's look at what we're going to do next here on line 44. We're going to execute a command, kubectl label pod ‑‑all, and we're going to update the label with the key tier, and we're going to set that to non‑prod, adding the overwrite parameter. And what that's going to do is update all of the Pods in this namespace, or in our default namespace, since we're not specifying a namespace, setting the label to tier=non‑prod. And so let's go ahead and execute that code there. And we can see pod‑1 wasn't labeled, and pods 2, 3, and 4 were. Pods 2, 3, and 4 were set the prod. They were then updated to non‑prod, but we had previously updated pod‑1 to non‑prod earlier so we can see that's that specify as not labeled because there wasn't a change event on that individual resource. So if we go ahead and get a listing of all of our Pods and their subsequent labels, here we can see on the right everyone's been updated to tier=non‑prod. Now, moving forward in our demo, let's look at how we can perform an operation on a subset of Pods with a selector. And so here we're going to use kubectl delete pod, and we're going to specify ‑l to specify a selector, and the selector is going to be tier=prod. So anything that has the key tier and the value non‑prod in the label is going to be deleted when I execute this code. So let's going ahead and do that there, and we can see Pods 1, 2, 3, and 4 are now deleted. And if we get a listing of our Pods with kubectl get pods, we can see that there are no resources found in this default namespace that we're working with.

Demo: Deployments, ReplicaSets, Labels, and Selectors
So we just went through some basic label operations, adding, editing, and then performing operations on collections of resources with label selectors. Now let's shift gears and look at Kubernetes resource management and how Kubernetes uses labels to make decisions on resources in the cluster. And so let's go ahead and bring some new code in the view here, and the first thing we're going to do is create a deployment. So I'm going to go ahead and open up this code right here for a deployment‑label.yaml, and let's walk through the code on the screen here. So, we're declaring a standard deployment. We've done this many times in this course and previous courses. And let's go ahead and look at the spec for this deployment. Let's bring that into scope here. Now, in the spec, we see the selector on line 9 defining matchLabels on 10 where the label has a key of app and a value of hello‑world. Now the pod template for this individual deployment also specifies some metadata for the pods that will be created by this deployment, and the metadata for each of those pods is going to have a label of app=hello‑world. Now, let's go ahead and jump back over to our script here and deploy that deployment into our cluster. So there we see hello‑world created. Now we also have a service that we're going to associate with this deployment. So let's jump over and view this code in service.yaml. In this code, we see at line 10 in the spec for the service, we see a selector with the key of app and a value of hello‑world and that label being defined. And so that's going to be used to specify which pods are the endpoint of this service. And so let's jump back over to our script again and deploy that into our cluster with kubectl apply. And there our hello‑world service is created. So let's take a look at how labels and selectors are used in deployments, ReplicaSets, and pods to see how it all pieces together. And so the first thing that we're going to do is ask Kubernetes to give us some detailed information about our deployment, and we're going to do that with kubectl describe deployment hello‑world. So let's go ahead and run that code. And it kind of runs off the screen a little bit. We're going to scroll up in this output and look at some of the data in here. So here we see our selectors for this particular deployment are app=hello‑world. And if you go a little bit further in this detailed output, we see the pod template. There we see any pod that's going to be created by the ReplicaSet behind this deployment will have the label as app=hello‑world. That deployment created a ReplicaSet, and so let's look more closely at that ReplicaSet. So, kubectl describe replicaset hello‑world. And so let's look at the detailed information for the individual ReplicaSet for this deployment. So if we bring that into view here, we can see the name of this ReplicaSet is hello‑world‑5646fcc96b. Well, that's interesting. Where'd that value come from? Well, that's part of what's called the pod‑template‑hash, which is going to be part of the selector that helps define what are the pods that are members of this particular ReplicaSet. And so let's look at the selectors associated with this ReplicaSet. So we see app=hello‑world, and pod‑template‑hash, and the value there is 5646fcc96b. Now, there's also some labels associated with this resource, and there we see app=hello‑world, and also the pod‑template‑hash again. Now, the selector here with app and pod‑template‑hash is how Kubernetes, or how the ReplicaSet specifically, is going to know which pods match the selectors when it goes and looks at the set of pods that are running in our cluster. It's going to ask with a selector to see who matches on these keys and values. Right? App=hello‑world, and pod‑template‑hash= 564, so on. If we skew away from that and we get an answer from the cluster that says we're outside of the desired number of replicas, then our ReplicaSet is going to react accordingly. If we go down a little bit further in this output here, we see again the pod template specifying any of the pods that are created from this ReplicaSet are going to get these labels where app=hello‑world with the pod‑template‑hash specific to this ReplicaSet. So, let's go ahead and break out of this output here and look more closely at the pods that are up and running in our cluster, and we're going to ask Kubernetes to give us a listing of all the pods and showing the labels with the ‑‑show‑labels parameter. So, kubectl get pods ‑‑show‑labels, execute that there, and we can see that our four pods that we requested in our deployment, which kicked off the ReplicaSet, which then created all the pods, are up and running. And on the right there in the labels, we can see the app label and the pod‑template‑hash label, app=hello‑world, and pod‑template‑hash=564, so on. So we know we're in the desired state here. We have our four pods up and running because that's what we requested. Now let's go ahead and edit the pod‑template‑hash label for one of these pods and see how Kubernetes reacts. And so let me go ahead and bring that code into view here. And I'm going to grab a pod name, put it in my clipboard, and paste it right here in the code. Just grab pod off the top of the list there. And so let's walk through an update, the pod‑template‑hash, from the correct value of 5646fcc96b, and just set it to the value of debug. And so let's walk through that code here together on line 72. Kubectl label pod, and we're going to update this individual pod that we just copied and pasted the name into, and there we see we're adding pod‑template‑hash=DEBUG with the overwrite parameter. And so I'm going to go ahead and update that label now with this code. It's successfully labeled with the output indicated at the bottom. So now let's go ahead and ask Kubernetes for a listing of the pods with kubectl get pods, and also specify ‑‑show‑labels and see what it did. Well, here we can see we have a fifth pod up and running, right? Kubernetes reacted. It saw that when it ran the selector, it didn't find four pods, it found three pods because we updated one of those pods by changing the pod‑template‑hash. Right? The ReplicaSet is looking for the pods constantly in a loop saying, do you have four pods? Do you have four pods? You have three pods? Oh, here's a new pod, and created a new pod for us. And you can see that there in the hello‑world pod ending in pf5vf with an age of 14 seconds. Now, I do want to call out that the pod that we relabeled didn't really change state. Yes, it's outside of the selector for the ReplicaSet, but that pod is still up and running, and so this is a useful technique if we have to debug a running pod.

Demo: Services, Labels, Selectors, and Scheduling Pods to Nodes
So that's how deployment, replicas sets and pods kind of all pieced together using labels, right? This is the core of how Kubernetes keeps these resources in check and knows if we're in a desired state. Now let's move forward and look at how services use labels and selectors. And so we created that service at the top of this demonstration here with that service.yaml and so I should go ahead and grab kubectl get service to see if everything is good. Yeah, so we see hello‑world service up and running and we have a cluster IP. So, let's go ahead and move forward and bring some additional code into view here and on line 81, I'm asking for some additional information about our service. So kubectl describe service hello‑world, and let's go ahead and look at the detailed information that that gives us. So in here, we see the selector app=hello‑world, and so this individual service is running queries with the selector app=hello‑world and it's getting sets of pods back, right? And so at the bottom there, we see endpoints 192.168.1.105, 106, and so on, right? And so those are the pods that are satisfying the app=hello‑world selector and become registered endpoints of this service when it's up and running. So let's look a little more closely at the endpoints themselves and run kubectl describe endpoints hello‑world to get some more detailed information about the pods that have registered endpoints into this service. In the output here, we can see a set of IP addresses associated with the pods that are registered endpoints for this service. So there we see, 192.168.1.105, 192.168.1.106, and so on. But if we look at the number of registered endpoint IPs, we see that there's five available. Now, I wonder why there is five available if we defined a replica set of four. Well, let's go ahead and look more closely how this all pieces together. I'm going to go ahead and get a list of IP addresses for all the pods that are up and running in our cluster and so we kubectl get pod ‑o wide. We can see the IPs associated with the five pods that are actually up and running but our deployment that we created had a replica set with four pods in it, and we have five. Well, yeah, we created that additional pod when we assigned the pod‑template‑hash to DEBUG, right? It kicked out an additional pod. So let's go ahead and look at the labels for all the pods that are up and running right now. So here we see five that have app=hello‑world. So those are the five that match the label selector for the service. We see four that match for the pod‑template‑hash and one with a pod‑template‑hash of DEBUG. What we need to do now is update the label so that we can kick that one pod out of the selector for the service, right? And so the pod ending in kjvnl is matching because the selector app=hello‑world is finding a match for the label app=hello‑world for that pod, right? And so we want to remove that from load balancing as well. So let's go ahead and grab that pod's name again, put it in our clipboard, paste it into here and update that label. So let's go ahead and run that code to update the label and we can see that kjvnl was successfully labeled. So now, let's go ahead and get a listing of all of our pods and their labels again. So we can see that kjvnl now has app=DEBUG and pod‑template‑hash=DEBUG and this is the state that we'd want to be in if we wanted to remove this pod from operations in a cluster. First, we removed it from a replica set standpoint and the replica set created a new pod for us. But then we needed to remove it from load balancing by updating the label so that it didn't match on the selector for the service. So now if we go back and ask Kubernetes for the detailed information about the endpoints associated with the service, we say kubectl describe endpoints. Now we see that there's only four IPs associated with this particular service. So now that pod is really out of the scope of the service, it's out of the scope of the replica set, and so we can go look more closely at it and debug it if we need to. So that's a wrap for this demo. Let's go ahead and delete our deployment, just to kind of clean up after ourselves when we're done, delete our service, since we're done with that, and go ahead and paste in our pod name here and get rid of that as well. Now for the next demo that we're going to work on, we're going to look at scheduling a pod to a specific node in our cluster, and so let's go ahead and bring that code into view here. Now, we've been working a lot with pods and labeling pods and things like that, but labels can be associated with other resources. In this case, we're going to associate some labels with nodes, right? It's a different resource in our cluster but all resources in the cluster have the ability to get labels attached to them. And so let's go ahead and get a listing of all the nodes in our cluster and look at the labels that are associated with the nodes so far, and so we can see some labels associated with the nodes. We see things like the CPU architecture, we see the operating system is Linux and the host name is specified as a label in our cluster and we also see that the node role for the master is also specified as a label and we see that on c1‑master1. Now let's go ahead and associate some new user defined labels to nodes in our cluster and we can do that here on line 110, kubectl label node C1‑node2 and we're going to assign the label with the key of disk and the value of local_ssd. We're going to do a similar operation on c1‑node3 specifying the label where the key is hardware and the value is local_gpu. And at the bottom, we can see that both of those resources or nodes were successfully labeled. So moving forward, we can query our labels just to confirm that they were associated appropriately. So we see kubectl get node ‑L specifying the keys that we want to look at, disk and hardware, and in the output, we can see the values; local_ssd is associated with node2 and local_gpu is associated with node3. So let's go ahead and bring this code into view here. Now we're going to create three pods, and two of which are going to be using node selectors and one isn't. So let's go ahead and open up that code. We have PodsToNodes.yaml. Let's bring that into view here, and inside PodsToNodes.yaml, let's look at how we can specify where we want to run these pods. Now we start off just like we would with any other pod creation where we specify the API version, the kind, and so on. But if we look at the spec there, on line 11, we define a node selector. That node selector is then going to define a label with the key of disk and a value of ssd. Now this is what the scheduler is going to look at when it makes its scheduling decision when placing this pod onto a node that has a label that matches this node selector. If we move forward, we can see we have, on line 24 and 25, a node selector for this pod where the key is hardware and the value is local_gpu. And then we are defining just a regular pod here with no node selector, and we're going to see where that lands in our cluster. So let's go and jump back to our demo script and send this YAML file into our cluster to create those three pods and there we can see at the bottom, pod‑ssd created, pod‑gpu created and the regular nginx‑pod created. So let's go ahead and ask Kubernetes, well, let's figure out where you place this workload. Now, let's go ahead and bring into view the actual labels that we've assigned. So there we can see the labels with c1‑node2 as local_ssd and c1‑node3 as local_gpu, and if we use kubectl get pods ‑o wide, that's going to tell us which node a pod is running on. And so in this output here, let's walk through each one of these and see where they landed. All right, so nginx‑pod landed on c1‑node1. That could land anywhere in our cluster because we didn't specify a node selector. Now for nginx‑pod‑gpu, that landed on c1‑node3, and if we look in our output, just above, we see c1‑node3 certainly has the key of hardware and the value of local_gpu, so that matched and landed on the right node. Now, similarly nginx‑pod‑ssd, that landed on c1‑node2, and if we look in the output above where c1‑node2 has a key of disk and a value of local_ssd. And so this is how we're able to influence the scheduling of pods or workload into our cluster based on the attributes of the cluster. Not only could it be GPU or SSD, but it could be maybe fault domains in our data center or maybe geographically dispersed data centers depending on how we're distributing workload, and so a very good technique, and this gets us away from pinning things to individual node names, so we can update very easily where these things would live by changing the labels on the nodes and if we needed to move the workload to a different node in our cluster, really not binding it to an individual node name, which would be not really the best practice. So let's get and clean things up. I want to remove those labels from the nodes, and so here we see kubectl label node c1‑node2 disk followed by a minus sign and the same for node3, hardware followed by a minus sign. We're going to remove those labels. Let's go ahead and delete our workload because we're going to wrap up this demonstration.

Introducing and Working with Annotations
The third organizational method for objects in Kubernetes that we're going to look at today is annotations. Annotations are used to add additional information about your cluster resources directly to the metadata associated with that resource. Annotations are mostly used by people or tooling to help make decisions about what to do with a particular resource based on the annotation or the data stored in the metadata. We'll commonly find things like build, release, or image information attached in an annotation to a particular resource, and this becomes very easily accessible. In addition to these attributes, we also will find things like which team is responsible for a particular resource, or other metadata that helps us give additional information about the resource so that either a person or a tool can make a good decision on that resource. Now, you might be asking why? Like, why would I want to store this additional metadata or this annotation along with the individual resource deployed in a cluster? Well, what this does is it saves you from having to write integrations to retrieve information from external data sources and then merging it together with the information that you're getting from Kubernetes. It simply all lives in one place inside your cluster attached as an annotation to the resource that you want to operate on. Now, an annotation is a non‑hierarchical key/value pair, so in that sense it's very similar to a label, right? We have a key and we have a value. Now, a big difference between annotations and labels is that annotations can't be used to query or select resources, right? We can't query and write a selector to pull out a subset of pods or resources on an annotation. We have to use labels to do that. The data that's stored in annotations is used for other purposes. It's used by people or tooling to help make decisions on what to do with a particular resource if we needed to perform an operation on it or help identify or gain more information about the resource that we're working with. Now, similar to labels, annotations, the keys can be up to 63 characters in length. But very different from labels is that an annotation can have a value that can be up to 256 kB in length, so a lot of information can be stored in a value of an annotation. So when it comes to adding and editing annotations to resources in Kubernetes, let's look at how to do that both declaratively and imperatively, and so we'll start off with a declarative implementation. At the top here, we start off by declaring our resource just like we would any other resource, so apiVersion, kind. In this case, we're going to work with a pod. Well, we can attach an annotation to any type of resource inside of Kubernetes. Then we go ahead and specify the metadata. Here we'll specify the name, in this case, nginx‑pod. Now here's where things get a little bit different. We specify the annotation and then the key and the value pair, the key being owner and the value being Anthony. And then we go about specifying the remainder of the information for this particular resource that we want to deploy. Now imperatively at the command line, it's going to look like this, kubectl annotate, then the resource type pod, and then the resource name, nginx‑pod. Then we specify the key and the value pair, so here we see owner=Anthony. If I want to make an edit to that annotation, here's what the syntax looks like for that. So, kubectl annotate pod nginx‑pod. I'm changing the value for the key owner from Anthony to NotAnthony, and then for this to overwrite the existing value in that annotation, we'll have to specify ‑‑overwrite. Now, let's take some time to review these organizational methods that we talked about for organizing objects in Kubernetes. And first up is namespaces, you'll want to use a namespace when you want to put a boundary around resources, security, or naming inside of your cluster. Basically, subdividing your cluster into virtual clusters and then grouping objects based on those concepts. Then we looked at labels. We're going to want to use labels when you want to act on a collection of objects in your cluster. This gives you the ability to query and retrieve collections of objects and perform some actions on those. We can also use labels to influence the internal operations of Kubernetes, whether it be from controllers, services, or scheduling. And then finally, we looked at annotations. You're going to want to use annotations whenever you want to add additional information directly to a resource, and that information will live with that resource inside of the cluster.

Module Summary and What's Next
Well, here we are at the end of this module, and let's review some of the key topics that we discussed. We talked about organizing objects in Kubernetes and the three primary ways that we can do that, with namespaces, labels, and annotations. Then we looked at how Kubernetes uses labels, specifically in the scenarios with services, deployments, and scheduling. Well, with that, that's a wrap for this module. Join me in the next module, Running and Managing Pods.

Running and Managing Pods
Introduction, Course, and Module Overview
Hello, this is Anthony Nocentino with Centino Systems. Welcome to my course, Managing the Kubernetes API Server and Pods. This module is Running and Managing Pods. So far in this course, we took a deep dive into the Kubernetes API, and then we also looked at how to manage objects in our Kubernetes clusters with labels, annotations, and namespaces. Now, let's move the conversation towards running and managing Pods in our cluster. Now, it's time to get into that fundamental workload element and learn how we can run and manage Pods. In this module, we're going to start the conversation off with understanding pods, and why do we need this abstraction of a Pod around our container‑based application? Then we'll look at the interoperation between controllers like deployments, and ReplicaSets, and Pods themselves and learn why we need such a construct. Next, we'll look at multi‑container Pods where we have multiple containers resident inside of a single Pod and why we would use something like that in our container‑based application deployments. And then we'll wrap up the conversation with managing Pod health with probes, where we'll give Kubernetes a little more information about the health of our application so that it can make good decisions on how to react in certain scenarios with regards to our applications that we're deploying in Pods.

Understanding Pods
So what exactly is a Pod? Well, a Pod is a wrapper around your container‑based application. It's the thing that will deploy into our Kubernetes cluster, and it's the job of Kubernetes to figure out how to run that workload in the cluster with this abstraction that is a Pod. Most commonly, there will be a single container per Pod, but there are many application scenarios where you'll deploy multiple containers within a Pod and then deploy that workload into your cluster. Now, in addition to the container‑based application, a Pod also has resources, the resources associated with the execution environment that is your application. So things like storage, networking, and application configurations with constructs like environment variables, that's all wrapped up in this thing that's called a Pod that we deploy into our cluster. The Pod is the unit of scheduling in Kubernetes. In fact, it's the responsibility of the scheduler to figure out where to allocate Pods into the cluster, onto the nodes based on the resources that are available. But when a Pod is up and running in our cluster, it's simply a process that's running somewhere consuming resources in our cluster. The Pod is also the unit of deployment, so it defines the application configuration and the resources associated with the Pod, such as storage and networking. So why do we need Pods in Kubernetes? Pods provide a higher‑level abstraction over our container‑based applications for manageability reasons. We've learned so far about scheduling and how Kubernetes uses that to deploy our applications into our cluster, but Pods also provide an abstraction around the execution environment for our container‑based applications in terms of configuration and resources such as networking and storage.

How Pods Manage Containers
So how do Pods manage containers? There's three primary ways that this happens, and first up is single container Pods. This is where we have a single application container wrapped up in a Pod and deployed into our cluster. This is the most common deployment pattern for getting container‑based workloads up in Pods inside of Kubernetes. Now, there are some scenarios where we'll have multi‑container Pods, where there's more than one container wrapped up in a Pod and deployed into our cluster. And we'll find these deployment scenarios are for tightly coupled applications that have some sort of producer/consumer relationship. And then next, there are init containers. Init containers are containers that run before the main application container is started in a Pod, usually used to set something up for the application container before it starts. We're going to look at each of these deployment scenarios more closely in the upcoming slides.

Introducing and Working with Single Container Pods and Controllers
Single container Pods, as we mentioned before, are the most common deployment scenario. And generally speaking, when you have a single container Pod, you're going to have a single process executing inside of that container. And so what this translates to is a Pod running a container running a single process on a node in our cluster. And this often leads us to easier application scaling because we're minimizing dependencies between our applications by running just a simple process inside of a container. This allows us to scale our application out by adding more Pods, which leads to more containers, which leads to more processes, really driving home and fulfilling that microservices philosophy. Let's look at the relationship between controllers and Pods. Controllers have the responsibility of keeping your applications or your Pods in the desired state. Controllers have the responsibility of starting and stopping Pods based on whatever configuration you assert upon your cluster. If you tell Kubernetes you want four replicas of a ReplicaSet, it's going to deploy four Pods in your cluster for you. Now in addition to keeping you in the desired state, controllers can also be used for application scaling. If you have the resources available in your cluster and you want to grow from 4 to 40 to 400 Pods, controllers can make that happen for you very easily based on the configuration that you assert upon a controller. If you tell it you want to increase the number of Pods in your application, it will do so for you. In addition to application scaling, controllers can also facilitate for application recovery, again, keeping things in a desired state. So if for whatever reason a Pod goes offline or becomes unavailable, Kubernetes can sense this and will recover and make sure that you have the correct number of Pods up and running in your cluster. Now, I want to make a key point here is that you don't want to run bare or naked Pods in Kubernetes, generally speaking. What a bare‑naked Pod is is a Pod that's deployed and not under the control of a controller, such as a deployment, a ReplicaSet, or a DaemonSet, or other type of controller that's available in Kubernetes. And the reason why you don't want to run a bare or naked Pod is that they won't be recreated for you in the event of a failure. If you have a Pod that's up and running on the node and that node dies, well that Pod won't be recreated for you somewhere else in the cluster. It's simply going to go away.

Introducing and Working Static Pods
Now that we've introduced Pods started by controllers and bare Pods, let's introduce one more way to start Pods in our cluster, and that's static Pods. Static Pods are Pods that are managed by the kubelet on specific nodes. To create a static Pod, you'll create a static Pod manifest, which is just a YAML manifest describing a Pod, and then you'll save that manifest into a location on the file system that the kubelet is watching. That file system location is called the staticPodPath, and by default, that location is /etc/kubernetes/manifests. So you write a Pod manifest and save it into this location, and the kubelet on that node will read the manifest and start up the Pod on that node. We learned a little bit about static Pods and static Pod manifests in the first course in a series of Kubernetes installation and configuration fundamentals where we learned that this is how the control plane Pods are started when the control plane node starts up. Those static Pod manifests were created by the kubeadm bootstrapping process. Static Pods can be used to start up any Pod, not just control plane Pods. The staticPodPath watched by the kubelet is configurable, so if needed, you could change that setting by editing your kubelet's configuration. The kubelet's configuration is in /var/lib/kubelet/config.yaml. The staticPodPath is watched by the kubelet, so if you reboot a node, any manifests in the staticPodPath will be processed when the kubelet starts up, and those Pods will be started. Further, if you make a change to a Pod definition in a manifest, that will be affected on the Pod's deploy. For example, if you update a container image in the manifest, this will cause the kubelet to restart the Pod with the new container image. If a Pod fails, it'll be restarted by the kubelet. And finally, when you've removed the static Pod manifest from the staticPodPath, the Pod will be terminated and deleted. The static Pods that are created using this technique are controlled by the kubelet, not the API server. But the kubelet will create what's called a mirror Pod for each static Pod. This means you'll be able to see the Pods from the API server with commands like kubectl get pods, but you will not be able to control the Pods via the API server. You must interact with it by managing the static Pod manifests in the staticPodPath.

Working with Pods - kubectl exec, logs, and port-forward
So now let's look at how we can work with some Pods. We've already covered in great detail how to deploy bare Pods and Pods under certain types of controllers into our clusters. So we're going to skip past that, but if you need a review of that, check out Kubernetes installation and configuration fundamentals or the other parts of this course where we covered the creation of Pods in some of the other demonstrations. For this, we're going to look at how we can interoperate with Pods and ask our Pods for additional information when it comes to troubleshooting and other techniques for access. And so, let's say we have a Pod that's up and running in our cluster, and administratively, we're sitting at our console. We need to interact with this in some way. Let's say I want to execute a process or something like that on that Pod to just interrogate it more closely. Let's say we wanted to start up a bash shell on the container running inside of that Pod, and so here's how we'd go ahead and do that. So kubectl exec ‑it, then the Pod name, in this case, POD1. If it's a single‑container Pod, we don't have to specify the container name, but if it's a multi‑container Pod, we should, and we would do that with ‑‑container and then specifying the container name. And then we follow that up by ‑‑ /bin/bash, and that's going to be the process that we're going to execute on that container in our application. And now let's look at what happens behind the scenes when we execute this command. When we execute kubectl exec, it's going to open a connection up to the API server. Then that API server is going to open up a connection to the kubelet on the node that's running that Pod and attach a process to the container running inside of that Pod, and that's going to allow us to execute this program, and the output that we generate will be streamed back across that same socket to our kubectl command running on our localhost. Now similarly, this is the same process that happens when we do kubectl logs and we go ahead and we want to get the log from that remote Pod. So again, the same flow. We open up a connection to the API server, the API server connects to the kubelet, reads that log information out, and streams it back to our kubectl process. Now I want to explore another scenario that we haven't covered so far in this course. What if I need to talk to that application that's running on that Pod and interrogate it to see if it's running the application correctly that we need to interoperate with? And now, since that Pod might be on a Pod network, it's not really accessible to me if I'm running remotely maybe in a cloud scenario where I'm on‑prem and my Pod is up in the cloud somewhere and I dont have direct network access, but I can reach the API server. Well, we can just kubectl to help us and access that container‑based application, and this is how we can go about doing that, kubectl port‑forward. We specify the resource you want to port forward to, in this case, a pod, and then the Pod name, POD1. We specify a local port, which is going to be instantiated on the local system that we're running kubectl on, and then we specify a colon, followed by the CONTAINERPORT, which is the container‑based application. In this case, in this example, it would be port 80. We send our traffic into that local port, and it pops out the other end inside of our Pod on the port that we requested with our container port.

Demo: Running Bare Pods and Pods in Controllers
So here we are in our first demo for this module, we're going to start off with running pods. We'll look at bare pods and also creating pods in a deployment. Then, once we have some pods up and running, we'll look at how we can use port‑forward to access a pod's application when we don't have direct access to the network that the pods are running on. And to wrap things up, we'll create some static pods. Alright, so here we are in VS Code with the session open to c1‑master1 on the bottom here, and the first thing that we're going to do is we're going to start up a kubectl events with the ‑‑watch parameter and background that, because I want to be able to show you the events that occur when I create some pods and delete some pods, and also create deployments and scale it up and scale it down, and we'll see what Kubernetes does behind the scenes to make that happen for us. And so, let's go ahead and start off by creating a bare pod. Inside pod.yaml I have the basic pod definition to create a bare pod for us, and let's go ahead and push that into the API server and see what happens. So in the bottom here we see Scheduled Pod Successfully assigned default/hello‑world‑pod to c1‑node3. The default there is we deployed this pod into the default namespace. The next line we see Pulled Pod Container image and then the name of the container image, and it says already present on machine. That means that this container image is already present on that machine from a previous time when I had to pull that container image from the container registry when I started this workload in other demos. If that container image wasn't already there, it would be the responsibility of the container runtime to go ahead and pull that container image from its container registry. The next line we see Created Pod Created container, and then Started Pod Started container, and so now, our single pod is up and running and our application should be up and running as well. Now let's do something similar with the deployment. Inside this deployment.yaml, I have a simple single replica deployment that runs our hello‑ world application, so let's go ahead and run that and watch the events that occur when we do this. So here we see ScalingReplicaSet Deployment Scaled up replica set hello‑world, and then the pod template hash, so, ‑5646fcc96b, and at the end there we see scaled it to 1, and it kind of wraps onto the new line there. Now, we also see SuccessfulCreate ReplicaSet, Created pod: hello‑world being our deployment name, then the pod template hash, and then a unique identifier for that individual pod, which is dk4mm. The next line we see that that pod is successfully assigned to c1‑node1, and then we go through that process again that we did earlier when we created the bare pod where on that node it checks to see if the container image is there, it is, so it didn't have to pull it, then it goes and creates the container and starts the container, and so that deployment is up and running on c1‑node1 now. So let's go ahead and see what happens when we scale our replica from one to two. We can do that with this command here, kubectl scale deployment hello‑world ‑‑replicas=2, run that code, and we'll see a very similar process that we saw when we created the first replica in our replica set. So here we see ScalingReplicaSet Deployment Scaled up replica set hello‑world, then our pod template hash to. Then we see SuccessfulC ReplicaSet Created the pod, then we get a new pod, hello‑world, followed by the pod template hash, and then a new identifier 8pgfl. Next line shows us that it's scheduled to c1‑node2, looks for the container image, which is already there, creates the container and starts the container. It's a very similar process to go from 1 to 2 as it was to go from 0 to 1. Now, let's go ahead and scale our deployment down from two replicas to one, run that code right here, and we can see Kubernetes events tells us ScalingReplicaSet event has occurred, for the Deployment, Scaled down the replica set to one. We see it deletes the pod in the next line there, and then it goes ahead and tells that node to kill that container and shut our application down. So now if we do kubectl get pods, we should see two pods, one associated with our deployment and one in that bare pod that we deployed initially. So if we keep moving forward, let's go ahead and bring this code into view here, and for this one, let's go ahead and we're going to instantiate or execute a process on a container in a pod. And in this case, the process is going to be /bin/sh to give us a shell onto that remote container running in the pod on that node. So, let's go ahead and use this one here, and we'll go ahead and copy and paste this pod name that ends in dk4mm and paste it here into this line of code. Let's go ahead and walk through this line of code. So we're going to do kubectl ‑v 6, we're going to increase the verbosity so we can see the API transaction between kubectl and the API server. We'll have exec, because that's the operation we're going to perform, ‑it for integrated terminal, and then the pod name ending in dk4mm. We have a ‑‑ /bin/sh, which is the process that we want to run on our container in that pod. Let's go ahead and do that, and we see here at the bottom we have a hashmark or a prompt available to us at the very bottom, and then let's go ahead and look at the API requests that occurred when we did this. The first API request checks to see if the pod's there. So we see a GET against the full API path to that individual pod and we get a 200 OK. Then we see a POST operation against that pod as well, but we have some additional attributes associated with the API transaction, so in that operation that we posted to that resource, we see exec?command= and then %2F, which is a forward slash, bin/sh and container=hello‑world, and so on. So this is how we instantiate the process on that remote container with kubectl exec. Let's go ahead at the bottom here and we can do a quick hostname to see that this certainly is the pod that we're connected to because the hostname is the pod's name. If we do a ps, on the container we can see our hello‑app is PID 1, our shell is PID 9, and this ps that we just executed is PID 16. Now, let's go ahead and exit out of this container and get back onto c1‑master1 with the exit command. And so that's our container‑based application. It's simply a process running in a container, running in a pod on a node in our cluster.

Demo: Running Pods and Using kubectl port-forward to access a Pod's Application
And so we just looked at the process listing inside the container, let's look at what it looks like outside the container at the node level. To do that, I need to know where that container‑based application is running, and we can ask Kubernetes for that information with kubectl get pods ‑o wide, and we can see that our pod is running on c1‑node1. And so let's go ahead and open up an ssh connection into c1‑node, and with that, now I have a connection onto c1‑node1, and if I do a process listing, I'm going to see all the processes that are executing on c1‑node1. So let's go ahead and run that here and filter for the process name for our container‑based application, and there we see hello‑app in the process list with a PID of 63151. Our container‑based application is just a process running on a node in our cluster, and it's the job of Kubernetes to figure out where to run that workload, make sure the execution environment is set properly, and also adding any additional resources, such as networking, or disk, or whatever it is we need to run our application in that pod. And so in the end though, it's just a process running in a container in our cluster. So, let's go ahead and exit out of this and get back onto c1‑master1. Go ahead and move forward in our code a little bit, and now I want to be able to access our pod's application directly. We didn't front end this application with the service and let's say that we weren't on the pod network at all, and so we didn't have any network reachability to the pod that's executing inside of our cluster. We can use kubectl port‑forward to actually access the application running inside that pod, and so let's go ahead and do that here. In this command, line 33, we say kubectl port‑forward, and I'm going to go ahead and put our pod name right here, and then we're going to use a local port of 80:, and the container port of 88, and so that's going to go ahead and open up a connection from my local system on port 80 all the way through to the container port on port 88. So let's go ahead and run that code here and see, uh‑oh, that's, oh, I know what happened, let's see what happened here. So we executed kubectl port‑forward against our pod and we specified port 80, which in Linux is a privileged port. So anything below 1024 we would need to run this command as root to be able to get access to the privileged port. In this case, the better solution than running this with pseudo or with privileged access is to make a modification to the local port, which we're executing on, I just so happen to have that code right here. So here we see kubectl port‑forward, let's go ahead and paste in the pod name, and I'm going to go ahead and run that application on 8080 locally, which is a non‑privileged port, and then relay that to the container port on 8080, and I'm going to throw an ampersand on the end there to background this task. So here we can see at the bottom, we're forwarding from 8080 to 8080. Now with that, I should be able to use curl to access our container‑based hello world application running on 8080 in the container port by making a request of http://localhost:8080, which is my local port here. I run this code here, and I certainly see I get the output from our container‑based application, we see Hello, World!, version 1.0.0, and then the host name, which is also the pod name, hello‑world, there's our pod template hash followed by the pod ID, which is dk4mm. So let's go ahead and kill our port‑forward session with an fg here, and kill it off with a Ctrl+C, and the final two lines of code here are to clean up after ourselves, so let's go ahead and delete our deployment and delete our pods, and it looks like I still have that watch running, so let's go ahead and bring that into the foreground and kill that off.

Demo: Working with Static Pods
Now for our final demo in this series of demos, let's look at static pods. And so what we're going to do here on line 55 is create a pod manifest using kubectl dry‑run combined with the ‑o yaml parameter to generate a syntactically correct pod manifest very quickly. And so on line 55, we have kubectl run hello‑world, which is going to be the name of the pod, and then we're specifying the image to be hello‑app with a tag of 2.0 from the Google Container Registry, our sample hello‑world application that we have been using. We're going to combine that with the dry‑run=client parameter and ‑o yaml, and then specify the container port as 8080. And so when we run this code here, what we'll get at the bottom is our pod manifest. And so I'm going to go ahead and take all of that code and throw that into my clipboard, because what we're going to do now is take this code over to c1‑node1 and create a static pod manifest. And so let's start that process. I'm going to SSH into c1‑node1, and before we create the static pod manifest, I want to show you where you can look in the kubelets configuration to determine where your static pod path is. And so let's take a peek at the kubelets configuration file, which lives in /var/lib/kubelet/config.yaml. So here it will cat that out to the console, and in the output at the bottom, we can see staticPodPath: /etc/kubernetes/manifests. So that's where we need to create our static pod manifest, and so let's start that process. On line 65, I have sudo vi /etc/kubernetes/manifests, which is our static pod path, and I'm going to give our static pod manifest a name of mypod.yaml. So let's go ahead and open up an editor to that, and switch over into insert mode, and paste in the pod manifest that we just created together at the command line. Jump up to the top of the file, and here we can see that this is a pod manifest. Let's save this file out, and take a quick look at a directory listing in the static pod path to see if our file was there, and there is my pod.yaml. So let's jump out of c1‑node1 and get back on the c1‑cp1, and now if I do a kubectl get pods ‑o wide, we'll see that I have a pod up and running. Now if we look at the name of the pod, we can see it's in the format of pod name and then a dash and then a node name. So we have hello‑world‑c1‑node1. Now, even though we see this pod in the output of kubectl get pods, it's not an actual pod in the API server, it's a mirror pod. And so if we try to delete the pod, like we're going to do here on line 76, well, it's not really going to delete the pod, it's just going to delete the entry for the mirror pod in the API server. And so when I run kubectl delete get pods hello‑world‑c1‑node1, it'll tell us that the pod is deleted. I'll come back a few moments later, and we'll see that the mirror pod gets recreated. So there we see hello‑world‑c1‑node1 is back up and running again. This actually had no impact on the running lifecycle of the pod on c1‑node1‑, just the mirror pod. Now, if I jump back on to c1‑node1, and if I delete the static pod manifest from the static pod path, the kubelet is going to sense that that file is no longer there, and then it's going to terminate and delete the pod. Now let's hop back out on to c1‑node1, and if I do a kubectl get pods now, we can see that there are no resources available in the default namespace. Our static pod has been terminated and deleted.

Introducing and Working with Multi-container Pods
So far in this series of courses, we've focused exclusively on single‑container Pods. Let's look at multi‑container Pods, how they work, and why you would use them. The primary reason why you'd use a multi‑container Pod is if you had very tightly‑coupled applications. And the reason is because those processes in those multiple containers will be scheduled together onto the same node because the unit of scheduling in Kubernetes is the Pod, so those containers will be deployed and instantiated on a node in a Pod. Now the key reason why you'd use a multi‑container Pod is there's likely some requirement on a shared resource between the containers running inside of that Pod. And usually, what this looks like is one of the containers is generating data while the other one is consuming data. It's some sort of producer‑consumer relationship. Now, this isn't an exhaustive discussion on design patterns for multi‑container Pods. In fact, there will be some course work on that in the future. But this is just the big picture idea when you'd want to use multi‑container Pods, and this goes into your application architecture and design. Now, I do want to call out is that we don't want to use multi‑container Pods to influence scheduling of Pods in our cluster. We use other techniques for that. In fact, we learned that technique back in the module, Managing Objects with Labels, Annotations, and Namespaces. We can influence the scheduler using labels on our nodes and node selectors in our Pod definitions. The YAML code for the manifest for multi‑container Pods is very similar to what you worked with when working with single‑container Pods, and so let's walk through the code together. Just like any other resource in Kubernetes, we start off with our API version and specify the kind, in this case, the Pod. We're going to specify some metadata and give it a name. Our name's going to be multicontainer‑pod. Now in the Pod spec, you'll notice before that the word containers was plural because we can list multiple containers in this part of the Pod spec. And it's also a requirement that the containers have unique names. So here we see name, nginx, we specify the image, nginx. and the ports for the containerPort and so on for that particular container in this Pod. If we need to fill any additional information about that container, we'd specify it here, and then we'd go and specify the next container in the Pod. In this case, its name will be alpine, and the image will be alpine. Now when working with customers or talking to conference attendees, I get a common question about designing multi‑container Pods, and so let's go ahead and review a common anti‑pattern for multi‑container Pods. This comes up very frequently in both of those scenarios. When deploying multi‑container Pods in Kubernetes, what you don't want to do is something like this, pair the web server and the database server inside of a Pod. On the surface, this might make sense. They're co‑resident. They can very efficiently exchange data. But in operations, this isn't the right way to get things done. And we're going to look at this from two different perspectives, the first off being recovery. What are my recovery options to bring this Pod back online if I have a stateful database server paired with a stateless web server? What type of controller would I use? Would I want that controller to just kill this Pod and recreate it? I need to take that into account in my Pod design. Now we're not going to focus too much on controllers in this course. We're going to pick that conversation up in a later course, and there's ways that we can deploy database servers and stateful applications into Kubernetes. But when we pair these two things together, it really limits our ability to wrap a controller around a Pod. Now the second issue with this anti‑pattern for multi‑container Pods is this really limits scalability. Now, the unit of scaling in Kubernetes is the Pod, and so I'd have to scale these two containers together, as I added additional Pods. And, well, web servers and database servers certainly have two very different scaling patterns when one is stateless and one is stateful. Within a Pod, we talked about how there are shared resources, specifically in terms of networking and storage. And so let's take a little time to look at these both more closely. And let's start this conversation with networking. Containers in a Pod share the same Linux operating system level namespace. This isn't the Kubernetes namespace that we introduced previously in this course. Since they share the same network namespace, to be able to communicate to each other, these two containers must communicate to each other over the loopback interface inside of their container. If these containers need to talk to other Pods on the network or other network resources, they would simply reach out over IP like they would to open up any other network communication. Since they do share the same network namespace, we need to be mindful of application port conflicts. And so these two container‑based applications, when they open up network ports, they need to be unique network ports within this Pod due to the shared network namespace. Now let's look at shared resources inside of a Pod from a storage perspective. Each container image within a Pod in a multi‑container Pod scenario will have its own file system. When we define volumes or persistent volume claims, they're going to be defined at the Pod level, which means these are shared resources amongst the containers in a Pod. Now this shared storage resource can be mounted into the containers' file system, which makes this an accessible resource to any container that mounts it within this Pod. This becomes a common way for containers to exchange data, and we're going to see this in action in an upcoming demo.

Demo: Running Multi-container Pods and Sharing Data Between Containers in a Pod
Alright, so in this demo we're going to look at running multi‑container Pods and how to share data between containers in the Pod at the storage level. Alright, so here we are with VS Code open again, and our code loaded up on the top here, and we have a terminal session to c1‑master1 at the bottom. The first thing that we're going to do is look at this multicontainer‑pod definition that I built, so multicontainer‑pod.yaml. So it starts off looking like any other Pod definition. We have our API version, we have the kind set to Pod, and the metadata giving it a name, which is multicontainer‑pod. If we go down a little bit further, though, we see this is where things get different. So in the Pod spec under containers, we're specifying two containers. So at line 7, we have name: producer and at line 14 we have name: consumer, and those are the two container‑based applications that will be resident inside of this Pod. Now the first container, producer, is going to run a container image ubuntu, and our second container, consumer, is going to run a container image named nginx. So you can imagine a scenario here where the ubuntu container is going to be producing some information in its container‑based application and our nginx, or our consumer container, is going to be reading that information and displaying it via its web application, that is nginx. If we look at the producer container on lines 9 and 10, we're instantiating a bash shell, and then running a while loop on line 10 that's basically going to echo the hostname and the date on this particular container, and write that information out to var/log/index.html. Now, let's go ahead and jump to the bottom of this definition. Here we see volumes on line 21, and we're specifying a volume with the name of webcontent and of type emptyDir. EmptyDir is a volume type where data is written locally on the node. We can use this Pod‑level resource to expose this volume to both of the containers in our Pod. We're going to use this as our shared volume, so these two containers in this Pod can easily exchange data. But I do want to call out, know that when this Pod goes away, the data written to emptyDir will go away as well. There is no persistency here. This could be any type of persistent volume inside of our Kubernetes storage universe if we wanted to, but for this demonstration, it works well to use emptyDir mounted into each container‑based application. And so let's look at how we mount that volume into our container, so let's go ahead and bring that code into view here. In the producer container on line 11, we declare volumeMounts. We're going to mount the volume name, web content, which we declared below on line 21, and we're going to mount that into the mountPath var/log, as you can see here on line 13. So anything that happens inside our container which reads or writes from that mounted volume in the file system at var/log, is going to perform that action on the volume, web content, which is that emptyDir‑backed volume. In the consumer container, we have volumeMounts, and again, we're going to use the same name, webcontent, and a mountPath of usr/share/nginx/html. That's the default location in which nginx reads from. And since we're writing out index.html file in the producer container, when we go access the web application in our nginx container, it's going to read that index.html and display that information when it's requested from the web server. So let's jump back over to our code and go ahead and deploy this application, and we do that with kubectl apply, and we're going to feed in multicontainer‑pod.yaml. And there we can see multicontainer‑pod.yaml was created. And so let's go ahead and bring this additional code into view, and I'm going to open up a kubectl exec to multicontainer‑pod, and open up a shell with bin/sh. Now, when we don't specify a specific container, kubectl exec will default to the first container in the container configuration. So there we can see in the first line of output there it says defaulting container name to producer, right? That's the producer of the information, and that's our ubuntu container that's generating our web application data. If I do a directory listing on var/log, which is where we mounted that shared volume, we'll see we have an index.html that was created just a few seconds ago. I'll go ahead and type in date here. We can see that we're just a little bit off of that from a few seconds ago on that date and time. If I look inside of the file, it's got a tail at the end of that file, and let's see what's inside of there. We can see what we're doing is we're writing the pod name followed by the date in a loop every 10 seconds. So let's go ahead and exit our shell on that container and get back to c1‑master1. And let's go ahead and bring this code into view on line 19 here, and go ahead and log into the consumer container. So from left to right, let's look at the code to do so, kubectl exec ‑it. There's our Pod name, multicontainer‑pod, and I'm specifying the container that I want to log into, which is ‑‑container, followed by the container name consumer, then ‑‑bin/sh to open up a shell. Now I have a shell on the consumer container. Now this is an nginx web application, and we mounted our shared volume into usr/share/nginx, so let's get a directory listing inside of that directory. And here you can see index.html, and we can see that the index.html was created just a few seconds ago as well. If we use tail to read the contents at the bottom of this index.html file, we can see it's still being updated every 10 seconds, so it's the same exact file between the two containers. Let's go ahead and exit out of the shell on our container and get back to c1‑master1. We're going to use that port‑forwarding technique to access this container‑based application so we can see that nginx content. This is deployed in a bare Pod, and we didn't front end it with a service, so we'll use this technique to access our container‑based application. So the syntax for this is going to be kubectl port‑forward, multicontainer‑pod. Our local port will be 8080 and the container port will be 80. We'll put an ampersand on the end there to background that task. And if we try to access that container‑based application with curl http://localhost:8080, we'll see the index.html file, which is constantly being updated every 10 seconds with new content.

Introducing and Working with Init Containers
Now, let's take a look at a variation of a multi‑container Pod, a Pod with init containers. Init containers are containers that will run before the main application container in a Pod is started. Init containers are commonly used to run tools or utilities to set up an environment for the main application container in a Pod. An init container must run to completion before starting the main application container, meaning the init container will perform some sort of task, and then when that application successfully finishes its execution, it's going to change the container state to completed, and then the main application container is started. You can have multiple init containers in a Pod, each of which are run sequentially, and each init container is run in the order that they're specified inside of the Pod spec. And then when all init containers have successfully completed, the main application container can start up and the Pod reports its status as ready. If one of the init containers fails, it will stop executing and not run the remaining init containers, nor will it start up the main application container. And so it's important to note that all of the init containers need to run to successful completion for the main application container to start up and for the Pod to report its status as ready. When an init container fails, the container restartPolicy applies. The container restartPolicy allows you to control what the container does in the event of a failure of the application running inside of that container, and by default, the kubelet restarts failed containers. There are other container restart policy settings, which we'll explore later on in this module. When building applications that require init containers, there are some common patterns, and so let's look at a few. And first up, we've already introduced that you can use init containers to run tools or utilities to set up the execution environment for your main application container, the primary benefit here being the tools don't have to be part of your main application container image. Now this leads us to our second pattern, separation of duties. Using init containers, the init containers can run potentially sensitive operations at higher privilege levels. This means your main application container can run at a least privilege level for its main functions. And then finally, init containers can be used to block container startup, meaning you can use an init container to ensure some sort of environment settings are in place before starting up your main application. Let's look at what it takes to create a Pod that uses init containers, and just like any other Pod definition, we'll start off with an API version and a kind, which is Pod. And now we're going to skip down into the Pod spec, which is where we'll define our init containers. Each init container will have a name and an image that we want to run. And so here you see the name is init‑ service and the image is ubuntu. And then you'll define a command that you want to run. So here in our example we're going to launch a shell that's going to print the standard out that we're waiting for the service to start, and then sleep for 2 seconds, simulating a task that needs to run to completion. Then we'll use that same pattern again if you want to define another init container, and so here we see name: init‑databases, image: ubuntu, and then a command that says waiting for database and then sleeps for 2 seconds. And then finally is the main application container. And so in this example here we can see that the name is app‑container and it's going to run the image nginx for the main application container. And so when this Pod is created, the init containers are processed one after the other, waiting for each to successfully complete. And so init‑service will echo out, wait for 2 seconds, and then init‑database runs, echoes out, and then waits for 2 seconds, and then finally, our main application container will start up. If any of these init containers fails, it will not proceed to the next init container, nor will it proceed on to start up the main application container in the Pod.

Demo: Working with Init Containers
Alright, let's get into a demo and see init containers in action. Let's get started with Init‑Containers. Now, the first thing that we're going to do is launch kubectl get pods with a watch, so that we can watch the progress of our init containers, and then also the main container start in the Pod that we're deploying here. And so let's go ahead and do that with the code on line 7, kubectl get pods ‑‑watch, and then we're going to throw that process in the background with an ampersand. And so, what we'll do next is to create a Pod with two init containers, and so let's take a peek at the YAML manifest to do just that. Now, in the code here, this is the code that we walked through in the presentation portion of the course, and so what we'll have is two init containers. So here we see our initContainers, init‑service and then init‑database. Once those two finish in sequence, then our application container, app‑container, will start up and run our nginx image. So let's jump back over to our driver script here and deploy that manifest to the API server with kubectl apply ‑f init‑containers.yaml, and we'll run that code there to start the creation of that Pod. And so in the output at the bottom we can see the various state changes of our Pod as it goes through the startup process running each of the init containers and then starting up the main application container, and so let's walk through some of that output at the bottom. Now the first status we see there for our init‑containers Pod is Pending. That means the Pod is getting scheduled, and created, and starting up. Now, on the next line we see the status change from Pending to Init:0/2, and what that indicates is that we're at the part of the process that's starting the first init container, and so there we see 0 of 2 init containers has completed. So right at that point in time for each of those iterations where we see 0 of 2, that first init container is running. Once it completes, we see the status change from Init:0/2 to Init:1/2. One init container has completed. And then once the second init container finishes, the status switches to PodInitializing. Now that main application container is starting up. Then we see the status switch to Running. And then in the Ready column we see 1/1. That means our one application container is up and running inside of our Pod. And so with that, let's go ahead and break out of our watch, bring that into the foreground and kill that task, and we'll clear our console to start off fresh. Now we're going to take a look at this init container from another angle. We're going to use kubectl describe pods to help us look at some detailed information about the pod that ran some init containers, and so let's do that. On line 18, I have kubectl describe pods. Which Pod do I want to describe? That's going to be init‑containers, and then we'll pipe that output into more. And so we have the standard output here for kubectl describe pods, and so we see the name is init‑containers, and we see the current status of the Pod is Running, and then the Pod IP. Going down in the output here, we have a field for Init Containers, and then under that we'll have each init container defined. And so the first one we see is init‑service, which was the first init container that we ran. Looking at the output here, if we scroll down a little bit, we see the State is Terminated and the Reason is Completed. This init container ran to successful completion and then terminated. Let's go down a little bit further. We see the second init container, init‑database. It too is terminated because it completed, and we can see that in the output, State: Terminated, Reason: Completed. If we keep going down in the output, we then have our main application container in the Containers field, there is the app‑container. Its state is running, because, well, the other two init containers finished their processes in sequence successfully, and then it started up the main application container, which is nginx, which is a daemon, and so that's going to continue to run, so the State there is Running. Let's take a peek at some more information about our Pod with init‑containers. If we go down to Events, we'll kind of see that same flow right. In the Events here, we see it's successfully scheduled to c1‑node2. It pulled the image for ubuntu. It then created the container init‑service, started the container init‑service for that first init container. Then we see the same pattern again for the second init container, init‑database. And then finally, our main application container, the image gets pulled, the container is created, and then the container is started. And so with that, let's go ahead and clean up from this demo and delete the Pod with init containers with kubectl delete ‑f, and then we'll specify the manifest that we want to delete, init‑containers.yaml.

Pod Lifecycle, Stopping/Terminating Pods, and Persistency of Pods
So far in this module, we've seen little bits and pieces of Pod lifecycle, especially when we looked at in the demonstrations using kubectl events. And so let's take some time to look more closely at Pod lifecycle now. At the highest level, Pod lifecycle is going to pass through three phases: creation, running, and termination. Now let's look at more closely how a Pod transitions between these three phases. When working with Pods, they're going to be created in one of two ways, administratively or by a controller. So administratively at the command line, we'll tell Kubernetes to do something, create a Pod, or we'll create a controller which has the responsibility of creating Pods for us. Once our Pods are created, they transition into the running phase and are scheduled onto nodes, and get our Pods up and running in our cluster. As things go along in the lifecycle of a Pod, eventually they'll transition into the termination phase, and in the termination phase, there's a pretty big list of reasons why a Pod would shut down or terminate. First up, is the process finished, or crashed, right? We just either finished our execution and the process terminated, or something bad happened inside of our container‑based application, and it crashed and brought the Pod down. Another reason could be that the Pod is deleted, whether it be by a controller or administratively by a user at the command line. The next reason is a Pod could be evicted due to lack of resources on a node. In certain scenarios, if a node senses that it's running out of resources, it can elect for a particular Pod that's running on that node to be evicted, and it will be terminated and then recreated and rescheduled somewhere else in the cluster. We can also see terminations due to node failures or maintenance. If a node goes away, the workload that was on that node, well, that won't be there either. And so that will have to be recreated somewhere else and rescheduled in the cluster, or for maintenance reasons, perhaps as an administrator, we select to take a node offline for maintenance; that workload will have to move to somewhere else in the cluster based on the restart patterns of the controllers for those particular Pods. Now, I do want to call out that no Pod is ever re‑deployed. Pods are recreated onto the other resources in the cluster if they need to be rescheduled and recreated after a termination. We're going to look at the concepts behind Pod persistency a little bit later in this module. As we've learned so far in this course, Pods manage containers, which are just processes running inside of our cluster. And so when a Pod needs to be shut down, there is a process that happens, and the following is what occurs. Let's say we have a Pod that's up and running in our cluster, and a user or a controller sends a command to delete that Pod. The first thing that's going to happen is the API server is updated with a grace period timer, which by default is 30 seconds. The grace period timer is the time Kubernetes is allowing for the application to shut down gracefully on its own before it has to intervene. At this point in time, Pod status will show up as terminating when listed in client commands like kubectl. Next, the kubelet on the node that the Pod is running on sees that the Pod has been marked as terminating. It then sends a SIGTERM to the processes in the containers and those processes begin to terminate. The Pod is then removed from the endpoint list for any services that it may be associated with and is also removed from any controllers that it's associated with, and then the Pod is deleted. If for whatever reason, the processes in the containers in the Pods are still up and running when the grace period expires, those processes are going to be killed with a SIGKILL signal. Once those processes are killed, then that specific Pod resource is then removed from the API server and etcd is updated. The grace period timer is a configurable attribute. If you need to specify a grace period time for your application, you can do it in one of two ways, both imperatively at the command line and declaratively in code, and let's talk about both of those. The syntax to set a grace period timer for a Pod at the command line imperatively is kubectl delete Pod, specifying the Pod name, and then the parameter grace period, and then giving that a number of seconds. There are some scenarios where you might need to force the deletion of a Pod. A forced deletion immediately deletes a Pod from the API server and etcd. You'll need to do this in scenarios where you can't get the application processes to terminate, but you need to reuse that Pod name to get your application back online. A forced deletion looks like this: kubectl delete pod <name> ‑‑grace‑period=0 ‑‑force. Now it's important to note that you will have to go and clean up those non‑terminating processes still, but you get to reuse that Pod name to get your application back online. On the declarative side of things, recently Pod termination grace period has been added to the Pod spec and can be configured per Pod if needed by your application, and so it's possible that your app does take a bit of time to shut down. And in those scenarios, you'll want to increase Pod termination grace period from the default value of 30 seconds. And on the other hand, if you need to shorten Pod termination grace period, you can lower the value from the default value. The field in the Pod spec that you want to look for is terminationGracePeriodSeconds. Now, let's look at the persistency of Pods. We've said several times so far in this course and in this series of courses that Pods are never re‑deployed, they're recreated. If a Pod has stopped, either administratively or by a controller, when a new Pod is created, it's a brand‑new Pod. There's no state transition between the previous execution and the current one that is currently being recreated. This means that at Pod creation, the Pod goes back to the original container image that's part of the pod's definition. So that leaves us with the question, how do we deal with things like application configuration or application _____, stateful applications? If we're always going back to that initial container image state, how can we assert some level of persistency, configuration, or state between Pod lifecycles? Well, we have to decouple both state and configuration from the Pod, and Kubernetes gives us some constructs to do just that. So some of the core ways that Kubernetes gives us the ability to have persistency between Pod executions or Pod lifecycle is that configuration is managed externally to the Pod. So when we define our Pod manifests, we'll use things like secrets, which are stored in the cluster, or config maps, which are stored in the cluster as well. Both of these give us the ability to assert some level of application configuration onto our container‑based applications. We can use secrets to feed in things like passwords and other key values into our applications for things like connection strings, or we can use config maps to assert more complicated configuration constructs onto our container‑based applications. A very common technique for application configuration is passing environment variables into the containers at the Pod level. What this gives us the ability to do is at runtime, our applications will look for these environment variables, and then configure themselves appropriately based on the contents. Now, data persistency is managed externally to the Pod as well. We touched on this very slightly when we looked at the demonstration for multi‑container Pods. We learned that we can have a volume externally to our container‑based applications and then mount it inside of our containers. Well, that same construct can be used for data persistency. In that case, we used emptyDir. Well, there's other types of volumes that we can use that provide persistent volume access, things like PersistentVolumes and PersistentVolumeClaims which reference other types of storage, whether it be cloud‑based storage or physical storage in your data center. Now, both of these topics, both configuration management and data persistency are very, very deep topics. In fact, we'll have modules on each of these in upcoming courses, but I just wanted to cover that here within the construct of Pod lifecycle.

Introducing and Working with Container Restart Policy
Inside of our pods, we have an additional layer of application resiliency, and that's the container restart policy. A container in a pod can restart independent of the pod. In fact, if our application crashes, or we initiate a restart on our own inside of our application, the pod itself really is just going to stay there for us and our container can restart. This is defined as part of the pod's spec. We tell Kubernetes in the pod's spec how we want our containers to react in the event of a container shutdown. Now, recall, the pod is the environment that our container executes or runs in. So, things like data persistency and configuration will still be available to our container‑based applications when they restart in the pod. Now, I do want to point out that our pods aren't going to be rescheduled on another node during container restarts, right? The pod is going to stay up and online during this period of time. The container restart is going to be initiated by the Kubelet on the node that that pod is running on. So our workload is going to stay where it is; it's just going to restart that container inside of the pod. Now, if our application is failing, and it's failing over and over and over again, Kubernetes protects itself and our applications with a restart exponential backoff, starting at 10 seconds, 20 seconds, 40 seconds, and capping itself at 5 minutes, and so it will slowly back off the time between restarts. After 10 minutes of successful runtime, this backoff loop is reset to 0. Container restart policy has three different configurations, and let's walk through each of those together right now. First up is Always, which is the default. The container restart policy always will restart all containers inside of a pod if they stop running. OnFailure will restart the containers on non‑graceful termination, basically non‑zero exit codes. And then Never, which means container restarts will never occur inside of a pod. Now let's look at what it takes to create a pod definition with the container restart policy. And just like any other pod definition, we start off with the API version in kind and some metadata. In this case, we're going to call this nginx‑pod. Then, in the pod spec, we'll still define our containers, so in this case, nginx, for the name and also the image. Now, here's where things get a little bit different. We define a restart policy at the pod level, not the container level. In this case, we're specifying the restartPolicy of OnFailure. So if something goes wrong inside of this container with the non‑zero exit code, it will restart the containers in this pod. In this case, there's only one container. If we don't specify a restart policy as part of our pod definition, the container restartPolicy is Always, which will restart the containers, regardless of how they stop or shut down.

Demo: Pod Lifecycle and Container Restart Policy
Alright, so here we are in a demo, let's look at pod lifecycle, killing a container process, and seeing how it reacts with the container restart policy. Now in VS Code here, we have a session open to c1‑master1, and let's walk through some pod lifecycle events, killing a container process, and the container restart policy. And so like we have before, I'm going to kick off a kubectl get events, and I'm going to throw in the ‑‑watch parameter, and with an ampersand to background that task, because I want to show you what goes on when we work with some pods in Kubernetes and that pod lifecycle. And so let's go ahead and run that code, and we have some events from our last execution, so let's go ahead and run a clear command to clean that up to give us a nice, fresh console. The first thing that we're going to do together is create a pod, and I'm going to do that with kubectl apply ‑f pod.yaml. Inside of there is just our basic pod definition, and we see, we go through that workflow that we described in the pod lifecycle where we schedule the pod, and we go ahead and pull up the container, create the container, and start up the container. Let's go ahead and bring this next code into view. Now I'm going to go ahead and launch a shell into that container, because I want to see what's going on inside of that particular container‑based application, so kubectl exec ‑it, we're going to attach it to hello‑world‑pod, and the process we're going to execute is /bin/sh to give us a shell. Now inside of here, we have our process listing. So we see our Hello World app is up and running at PID 1, our shell on PID 9, and this ps process on PID 16. Now if we get out of there, and now we're going to make some fun stuff happen. Let's go ahead and execute a different command. We've used kubectl exec a lot to attach a shell. Well, we can use kubectl exec to launch an arbitrary process inside of a container if the binary is available to us inside that container. And so let's walk through what I have here on line 18. I say kubectl exec ‑it, there's our pod name, hello‑world‑pod, and the process that I'm going to run is /usr/bin/killall, and the parameter for that is hello‑app. So I'm going to go ahead and tell the container to kill that process inside that container. So let's go and do that together. Run this code. Now we see right away our kubectl get events watch shows us that, well, it created a new container and started the containers, right? So that's that container restart policy coming into play there. In that YAML file for pod.yaml, I didn't define a container restart policy. It defaults to Always, and it protects our application and keeps it up and running. And so let's look more closely at where we can see these other restart events in some other areas in our Kubernetes cluster. And so let's go ahead and grab kubectl get pods, and while you may have seen this Restarts column before in the output of kubectl get pods, well, guess what? That's exactly what that is. Usually we see this as 0, because no restarts have occurred, but in this case, we killed our application with killall, the container restart policy kicked in, restarted that container, and there we see one restart for that individual pod. If we wanted to look at this even more closely, we can use kubectl describe, and so let's go ahead and grab that kubectl describe output for our hello‑world‑pod. Now at the bottom in the events here, for the last three events, we see x2 or two times, over the last 2 minutes and 44 seconds. That means these three events at the bottom here executed more than once, in this case two times, over the last 2 minutes and 44 seconds, and the last events were 69 seconds ago, and those events were checking to see if the container image is there, creating the container, and starting the container, once for the initial pod creation and container starting, and then once again when we killed that application and the container restart policy kicked in. If we go up a little bit further on our output, we'll see some additional information underneath containers. So if you scroll up, in here you see containers, and then let's go ahead and scroll down a little bit further. We see that the state is currently running, and that it started a few minutes ago for this particular execution, so this one is at 9:55. The original container execution is in the last state. We see that original container execution is terminated, and the reason was error with an exit code of 2. That container started at 9:53 and then was killed by me at 9:55. So that 9:55 and 17 seconds is when I executed that killall command. Now up at the top there at 9:55 and 18 seconds is when this iteration of the container restarted. We see that our container Ready is set to True, so this container is up and running and ready, and the Restart Count was incremented to 1. And so we have some pretty good, detailed information here into the lifecycle of our containers. So let's go ahead and get a console on the bottom again here, and delete that pod. We're going to move on and work with some additional workloads here. So delete our pod, and once that's finished, we'll go ahead and kill our watch, we'll bring it into the foreground and throw a Ctrl+C at the console to get a terminal back. Now if you are working with pod definitions, you might forget where things live and things like that. Just a quick reminder here that you can use kubectl explain to get this kind of information, and in this case, kubectl explain pods.spec.restartPolicy. I run that there, and I can see the various settings for the restart policy and the output at the bottom there: Always, OnFailure, or Never in that the fact that it defaults to Always. So a good reminder of where we can go ahead and retrieve that information from. Let's go ahead and create some pods with specific restart policies, and I have some YAML over here to describe that. And so we have two pods being created; one is going to be hello‑world‑onfailure‑pod, and on line 9 there, we see the restartPolicy is specified as OnFailure. The second pod in this YAML file that we're going to create is hello‑world‑never‑pod, and on line 19 there, you can see the restart policy is going to be Never. And so we're going to go through, deploy these pods, and see how they react when we kill our application. So let's go ahead and jump back over to our demo, and send that code into our API server with kubectl apply ‑f pod‑restart‑policy.yaml, and at the bottom here, we can see both of our pods were created successfully. Just a quick check with kubectl get pods, and make sure everything is okay. We see hello‑world‑never‑pod and hello‑world‑onfailure‑pod; both are ready, so we see one of one, the status is running, and no restarts have occurred. So on line 45 here, let's go ahead and use that killall command again inside the container to execute the killall command against our hello‑app inside the pod, hello‑world‑never‑pod, and see how our restart policy reacts in this scenario. We'll go ahead and run this code, and do kubectl get pods. We can see that the hello‑world‑never‑pod status changes from Running to Error, right? Something went wrong. We can see in the Ready column that 0 of 1 containers is running, because we used killall to kill our application, and their container restart policy is set to Never. In this case, well, it certainly reacted the way that we wanted it to. And so let's go ahead and look at kubectl describe for this particular pod. And at the bottom there, we don't see any restart events this time, just the initialization events for the initial startup of this pod and its container. If we scroll up and look at our containers, again, so here we see Containers, and scroll down a little bit further, and we can look and see that our state is Terminated, our reason is Error, our exit code is 2, and basically our pod is not up and running. We also see that ready is set to False this time, and no restart count. So we didn't get our application back because we defined the restart policy as Never for this particular application. So let's go ahead and get our terminal back at the bottom and bring this code up. Now let's go ahead and do the same thing with our other pod, where we specify the container restart policies on failure. So we're going to send the killall command into our hello‑world‑onfailure‑pod to kill off the hello‑app again. Let's go ahead and run that. If we do kubectl get pods, this time, we'll see that our restart incremented to 1, and our status is running, so it recovered appropriately. We killed the process, the kubelet realized that that occurred, and then it kicked off a new creation of that container, set us back into the running status, and we see the increment that restarts to 1. Now let's go ahead and do that again. I'm going to kill our app one more time, and now I'm going to do a get pods again, and here we see we're in the error status for our onfailure pod. So let's think about what's occurring here. I killed the app, and then I killed it again, and what's happening is, well, that's the backoff loop asserting some control over the restart policy. So if we wait just a little bit, and we do kubectl get pods again, we should see running now. And so we just backed off for a second, restarted the container, and now we see restarts incremented to 2, and our status is set to 1. If you want to look at the detailed information behind what occurred there, we can use kubectl describe pod for our hello‑world‑onfailure‑pod, run that code there, and at the bottom, we can see that there have been three initializations of this container‑based application, the one in the initial pod deployment, the second one when we killed the process, and the third one when we killed the process again. So everything should be good to go here. Yes, that's a wrap for this demo. Let's go ahead and clean up our workload by deleting both of our pods there, and keep moving forward.

Defining Pod Health: livenessProbes, readinessProbes and startupProbes
We just looked at container restart policy and how it's used to bring containers back online when they crash or something goes wrong. We can go a little bit further and help Kubernetes understand the health of our applications better. Let's discuss defining Pod health. In Kubernetes, a Pod is considered ready when all of the containers in that Pod are up and running. But maybe we need to give Kubernetes a little bit better of an understanding about what really defines a healthy and ready application in our container‑based apps that are running in Pods. We can add some additional intelligence to our Pod's state and health with container probes. With container probes, Kubernetes can know when the application in the Pod is up and responding properly before changing the Pod's state to ready. There are three types of container probes that we're going to look at today, livenessProbes, ReadinessProbes, and startupProbes. Let's get started. The first container probe that we're going to look at today is the livenessProbe. A livenessProbe continuously runs a diagnostic check on a container in the Pod, and this is a per container setting. And so if you are running multiple container Pods, you'll define a livenessProbe or a check for each container in your Pod, because it's possible that your containers in a Pod could have different definitions of health because they're different applications. Now, here's the key thing about livenessProbes. On failure, the kubelet will restart the container according to the container restart policy for that Pod. Using a livenessProbe, gives Kubernetes a better understanding of our application and how to react when things go wrong. And so, for example, if your application crashes and is no longer responding properly, Kubernetes will restart the container with the hopes of returning your application to service. You should, of course, figure out why your application crashed, but a restart is likely to get your container‑based application back into service quickly. The second type of container probe that we're going to look at today is the readinessProbe. ReadinessProbes continuously run diagnostic checks against the containers in your Pod during the life of the Pod to determine if your Pods are ready to receive traffic from Kubernetes services. ReadinessProbes can also help deployment rollouts ensure that there are a set number of Pods online and functioning to support your applications while rolling out new versions of Pods, and we'll dive into this in great detail in the course, Managing Controllers and Deployments. ReadinessProbes are per container settings, and so we'll define readinessProbes, if needed, for each container within a Pod. With the readinessProbe defined, on Pod start up, your application won't receive traffic from a service load balancer until the readinessProbe reports success, and on failure, the endpoint controller will remove the failed Pod's IP from the service endpoints for the service, so that it no longer receives traffic from the service load balancer. When using readinessProbes, on failure, the container isn't restarted. The Pod IP is removed from load balancing, and so that's how it differs from the livenessProbe, where a livenessProbe potentially restarts the container on failure, based on the container restart policy. ReadinessProbes are used to protect applications that temporarily can't respond to a request, and on failure, it removes the Pod endpoint from a service so that users won't see any application errors, since the traffic will be load balancing to the remaining pods that are up, running, and healthy with passing readinessProbes. The third type of container probe that we're going to look at today is the startupProbe. StartupProbes run diagnostic checks against the containers in your Pod during Pod startup to determine if the application containers in the Pod are up and ready for traffic. StartupProbes are a per container setting, and so we'll define them, if needed, for each container within a Pod. On Pod startup, when using a startupProbe, once the startupProbe succeeds, the Pod is considered ready, and so traffic will be load balanced to it or a deployment rollout can continue. If there are also readinessProbes and livenessProbes defined, they are disabled until the startupProbe succeeds. When that startupProbe succeeds, the liveness and readinessProbes are then enabled to help determine the Pod's liveness and readiness as they normally would. If no livenessProbes or readinessProbes are defined, then the Pod is considered ready at the point at which the startupProbe succeeds. On startupProbe failure, the kubelet will restart the container according to the container restart policy for the Pod. Now, you want to use startupProbes when you have applications that have long startup times. Before startupProbes existed in Kubernetes, people used a combination of liveness and readinessProbes to wait for slow application startup, and this led to more complex container probe configurations. Also, if the readinessProbes or livenessProbes fail too many times, it could cause the Pod to prematurely report as not ready and unnecessarily be restarted. The startupProbe helps solve these problems, since the startupProbe and livenessProbe and readinessProbes can each have different configurations defining what's healthy during Pod startup, and also what's healthy during when a Pod is up and running, and so you'll likely use them together to determine Pod startup success and runtime health. So far in this module, we've defined what container probes are, but we haven't defined what those diagnostic checks are that we could use in our container probes, and so let's go ahead and do that right now. Let's look at the types of diagnostic checks available for our container probes. There's three primary diagnostic checks available for container probes, and those are Exec, tcpSocket, and httpGet. Now, let's look at each one of these individually. The first one, Exec, measures a process exit code from an arbitrary process executed inside of our container‑based application. And so we simply execute a command in the container, and then look at the exit code and make a determination if our application is healthy or not. The second is tcpSocket. This is a test to see if we can successfully open a TCP socket on a container that we want to run the diagnostic check against. And then finally, httpGet . HttpGet will execute a check against the URL on the container, and looks at the return code for that request. If it's greater than or equal to 200 and less than 400, it's a successful check. Now for each of these diagnostic checks, there are three potential outcomes for our container probes. First up is success, that the container passed that check. Second is failure, that the container failed the check. And then the final state it is unknown. This is a case where the diagnostic check itself failed in its execution, and no action was taken against the container.

Configuring and Defining Container Probes
Kubernetes gives us the ability to configure our container probes so that we can fine tune the readiness, liveness, and startup checks to be as accurate as possible to help us get a better representation of our application health. And so let's look at some of the ways that we can configure our container probes. The first configuration point is initialDelaySeconds. This is the number of seconds after the container has started up before it will start executing the container probes against the container. This can be useful if you need some additional time for your applications to start up before initializing the container probes. The default here is 0 seconds. Then there's periodSeconds. This is the probe interval or how frequently the container probe will run against a particular container. Next up is timeoutSeconds. Once a container probe has started executing against a container, this is how long the probe will wait before giving up and declaring failure. Then there's failureThreshold. This is the number of container checks that need to fail before the probe reports failure. Default here is 3. And then finally, successThreshold. After the failure of a container probe check, this is the number of probes that have to be successful before the application is considered healthy again. The default here is 1. Now let's look at the actual code to define a liveness and readinessProbe. We'll go through one example of each. Now let's say we're working at defining a Pod, and we're in the Pod spec. And we're in the containers section, and we're going about our business defining our container‑based application in terms of the name, an image, and whatever it takes else to construct that. In the containers section is where we'll define our liveness or readinessProbe. And let's start off with our first example here, which is going to be a livenessProbe. And I'm going to select for this particular livenessProbe a tcpSocket diagnostic check. For a tcpSocket diagnostic check, we'd have to declare a port. In this case, it's going to be 8080, just for our example. If I needed to add some additional configuration about this individual livenessProbe, here's where we'd go ahead and do that. So for this particular one, I'm going to have initialDelaySeconds set to 15, and the check interval or the periodSeconds set to 20 seconds. And so that's how we go about defining a livenessProbe in the container inside of a Pod spec. Now similarly, a readinessProbe would be established in the same area of our Pod's YAML inside of the container definition. And so for a readinessProbe, again, maybe you wanted to define a tcpSocket. We're going to define it on port 8080. It'll have an initial delay of 5 seconds and a periodSeconds of 10. And this is what it would look like to define both a liveness and readinessProbe in YAML as part of our Pod spec in the container definition. Now I do want to call out since we are looking at the code right here together that we can leverage both livenessProbes and readinessProbes together inside of a Pod spec for checking the health of a container. They're not mutually exclusive, and both could be leveraged to really extend and check the health of our container‑based applications. Now, let's check out the implementation details of a startup probe in YAML. And so inside of the Pod spec, inside of containers, we'll define a startupProbe. And similar to our livenessProbe that we just looked at, the startupProbe here is going to use a tcpSocket check, and we're going to tell it to check port 8080 and give it an initial delay of 10 seconds and a period of 5 seconds. And this startupProbe here, what it's going to do is when the Pod starts up, it'll start checking the container after a 10‑second initial delay, executing its checks every 5 seconds against TCP port 8080. Now I want to call out that we can leverage both livenessProbes and readinessProbes together with startupProbes inside of the Pod spec for checking the health of a container. They're not mutually exclusive, and each type of container probe can be leveraged to really extend and check the health of our container‑based applications. If you are using liveness and readinessProbes in your Pod spec in conjunction with a startupProbe, they're going to be disabled until that startupProbe is successful.

Demo: Implementing Container Probes - livenessProbes and readinessProbes
Well, here we are in our demo, let's go ahead and look at how we can implement container probes. We're going to look at examples for livenessProbes, readinessProbes, and startupProbes. Here we are in VS Code, I have a session open to c1‑master1, and let's get started working with container probes. The very first thing that we're going to do here is kick off a kubectl get events with the ‑‑watch parameter, so we can watch how our containers react when livenessProbes or readinessProbes succeed and fail, and so let's go ahead and run this code here, get this cleared off to get us a fresh console, and get started. And so I've defined in this container‑probes.yaml file a deployment that has a pod template for a hello‑world container. That hello‑world container has two container probes defined, both a livenessProbe and a readinessProbe, and so both of those will work in conjunction to check the application health of this particular container‑based application. Our Hello World application lives on the container port 8080 as specified on line 19 there, and then on lines 20 and 25, we can see where we start the definitions of our liveness and readinessProbes. So, the livenessProbe that starts on line 20 is a tcpSocket probe, it's going to check for the health of port 8081. Now I'm intentionally using 8081 here and not matching to container port because I want to show you what a failed check looks like and then we're going to loop back and correct it, and show you what happens when the state changes and our application comes back online. On line 23, we configure an initialDelaySeconds of 10, and a periodSeconds of 5, that's our check control. Line 25 is where we begin our readinessProbe. In this case, I'm going to use an httpGet probe and check the URL at the path, just the base URL or the forward slash there on line 27, and again we're going to define a port here of 8081, we're telling our readinessProbe that our web application lives on 8081 when it actually lives on 8080, but again, I want to show you what a failure looks like, we'll correct it, and then we will get our application back up and running. Here, we'll set two parameters for the readinessProbe, initialDelaySeconds of 10 and periodSeconds of 5, and so let's jump back over to our demo and feed that into our API server with kubectl apply ‑f container‑probes.yaml. Run that code there and let's see what happens. So, just like we've seen before in other demos, since this is a deployment, it's going to scale that deployment's replica set from 0‑1, and bring our application up and online. After about 10 seconds, because of our initialDelaySeconds settings of 10 that we set in our container probe definition, we're going to start seeing, well, here we go, the readinessProbe and livenessProbes fail, and so let's go ahead and stop that before it runs off the screen too quickly. So bring that into the foreground, and go ahead and break out of the check, and so let's walk through this output together. So, in the output below from our kubectl get events, we can see the failed probe attempts of our liveness and readinessProbes and so let's walk through this output together and see what Kubernetes does when these checks start failing. And so, first off, we see three livenessProbes, Liveness probe failed: dial tcp, then the pod's IP on 8081: connect: connection refused, right? Our livenessProbe can't hit that tcpSocket on 8081, because, well, the application's actually running on 8080. We see a second livenessProbe check, a third livenessProbe check, and then Kubernetes reacts and kills the container, because of the failed livenessProbe. And so here we see, container will be killed and recreated, and then it goes through the process of killing the container, pulling the image, creating the container, and starting it up. In this output here, we also see two readinessProbe failures, and so that's trying to touch that web application and doing the HTTP Get on http://192.168.2.143:8081, again on the base URL, which we defined in our readiness check, and that's failing because we're pointing it to the wrong port. Now, let's go ahead and look at a couple other areas where we can see this information, and then we'll loop back and make the appropriate correction for this. So, when we do a kubectl get pods, we're used to seeing output that looks like this, but we're also used to seeing ready where it's 1/1 or the number of ready containers over the number of containers in the pod, in this case, there's one container, no containers are ready, so we see 0/1. We also see that we're in a CrashLoopBackOff because the livenessProbe is constantly killing the container and restarting it, trying to bring it back online. So here we can see five restarts in the last few minutes. So, we already kind of know what's wrong, but let's go ahead and keep moving forward and look at how we can look more closely at the runtime configuration and some of the events associated with this particular pod, and so I'm going to use kubectl describe pods to dig in a little bit further into what's going on with this particular container‑based application inside of our pod. So in the events, we can see lots of the events that we've seen so far at the bottom here. We can see Readiness probe failed, we can see Liveness probe failed, and the fact that it's looping over, creating our container and killing it over and over again. But if we go up in this output, we can go ahead and get some insight as to what's going on inside of here. If we go to our container definition, so we'll go all the way up to here, there's our container definition for our hello‑world container, then we'll loop back down, we can see the Liveness and Readiness probes configuration. So we see Liveness and we see tcp‑socket :8081, then we see the configuration, the delay=10s timeout=1s, and so on. We see the Readiness probe as an http‑get probe on http:// :8081/. Then we see its configuration in terms of delay and timeout, and so on for the attributes that we either configured in our YAML or are the default for that particular container probe. We see Ready: False, Restart Count: 5, so we know this thing is in bad shape, looking at what's going on in this output. We see the State as Waiting, the reason is because we're in the CrashLoopBackOff, and the Last State was terminated. Again, that livenessProbe just constantly restarting that container. So let's go ahead and fix that up. We'll jump over to our container‑probes.yaml definition, we'll switch it over to the correct ports, on line 22 we'll change the tcpSocket from 8081 to 8080, and we'll change the readinessProbe for the HTTP Get, we'll change its configuration to stop looking at 8081 and to start looking at 8080. Let's go ahead and save that out, jump back over to our script here, send that into our API server, get a new console at the bottom first, and then we'll send it into our API server, it configures our application. Now we're going to have to wait a little bit because of that CrashLoopBackOff for our application to come back online. Alright, so, before we go ahead and check to see if our application's online, let's go ahead and confirm that our configuration is correct for the container probe. So we do a kubectl describe pods, we'll scroll up to our readiness and livenessProbe configuration, there we see tcp‑socket :8080 and an http‑get :8080, so we're pointing at the right container port for our application, we can see the port in the container is 8080. So let's go ahead and bring up a new console and check the status of our application, do a kubectl get pods, we can see that we're now up and running with 1/1. I do want to point out that in this demo, we updated the deployment, which created a new replica set and that created a new pod, and that's what we see here, a new pod as part of that new replica set, with the readinessProbes and livenessProbes succeeding. So we're all done here, let's go ahead and clean up this demo with kubectl delete for our deployment and get moving forward.

Demo: Implementing Container Probes - startupProbes
For our final demonstration, let's look at startup probes. The first thing that we're going to do is start up a watch on kubectl get events, and so here on line 50 we have the code to do just that. Kubectl get events ‑‑watch and we're going to background that task with an ampersand. Let's go ahead and clear our console to get rid of those old entries. Now what we're going to do next is we're going to deploy a deployment with a startup probe, and so let's go ahead and check out the code to do just that. So in the course downloads, you'll find container‑probes‑startup.yaml. And in here what we have is the deployment that will have defined a startup probe in. So inside of the Pod template spec and inside of the hello‑world container, you'll find here on line 20 a startupProbe. Now, this startupProbe is of type tcpSocket, and we see that there on line 21. The port that this startupProbe is going to check is port 8081. I'm intentionally injecting an error here so that we can watch the startupProbe in action and see what happens when it fails. The containerPort for our application is on port 8080. Now, in addition to the base configuration of the tcpSocket and the port, I'm also configuring a startupProbe with an initialDelaySeconds of 10, a periodSeconds of 5, and a failureThreshold of 1. In addition to the startupProbe, this deployment also has a livenessProbe and a readinessProbe configured. And so the livenessProbe is going to check tcpSocket port 8080, and the readinessProbe is going to do an httpGet against the base URL with the path of just the forward slash, and then the port is 8080. Now jumping back over to our driver script, let's go ahead and roll this code out with kubectl apply ‑f, and then specifying container‑probes‑startup.yaml. So there we run that code, and then the output at the bottom, what we can see is that deployment rollout, creating a Pod, then also creating the container. And so there we see Created container hello‑world, Started container hello‑world, and now in the output we see Startup probe failed because our startupProbe is pointing at the wrong port. In the output, we can see 8081 and that the connection was refused. Our application is actually running on port 8080. And so in a few moments here, what's going to happen since the startupProbe is failing, the container restart policy is going to kick in, and the container will be restarted by the kubelet. And after a few moments in the output, we can see that the container hello‑world failed startup probe and will be restarted. And then we see the process of creating and starting the container again. Now after that container restarts, the startup probe is still going to fail because we have an error in our configuration. So let's break out of this output at the bottom here and bring our kubectl events ‑‑watch into the foreground and kill that off. Before we fix that problem, let's do a kubectl get pods and inspect the output there. So there we see our Pod, and that status is running and it restarts as 1, and we saw that 1 restart in the output of kubectl get events. Additionally, I want to point out that we see ready 0/1. And so what that means is 0 of 1 containers defined inside of this Pod are up and ready because our startup probe is failing. And so let's go ahead and correct that issue. We'll jump back over to container‑probes‑startup.yaml and make the modification on the startupProbe, then change the tcpSocket check from port 8081 to 8080. So we'll save that out, jump back over to our driver script, and then roll that code out with kubectl apply ‑f container‑probes‑startup.yaml. And so if I do a kubectl get pods now, what we'll see is the old deployment is going away, and that should be terminated here in a second, I'll check the status in a moment. But during that time, that failed container restarted once more, so we see that incrementing from 1 to 2. Now, I also want to point out that the other Pod here, ending in gqg4k, that's part of our new fixed configuration, isn't up and running yet either because of the initial delay seconds of 10. So there we see it's not ready yet. If I do kubectl get pods once more, that status has changed. We can see that the previous Pod ending in 4469r is now terminating, so that'll shut down in a moment. And then our new Pod that's in our corrected deployment is now ready and we see that 1/1 containers in that Pod is reporting as ready. And so that is passing its startup probes and also passing those liveness and readiness probes that are configured in the Pod templet spec. And so that's a wrap for this demo, let's go ahead and delete our deployment.

Module Summary and Thank You
Well, here we are at the end of our module, and we covered a lot of content. We looked at understanding Pods, what a Pod really is and what it looks like inside of our cluster. We then looked at the relationship between controllers and Pods. We then took some time to look at multi‑container Pods and how they interoperate and exchange data. We also covered managing Pod health with container probes so that we can help Kubernetes get a better understanding of our container‑based applications' health. And so here we are at the end of our course, and I really hope you enjoyed listening to this as we continue along in your Kubernetes studies. We've covered a lot of ground so far together. We did a deep dive into the Kubernetes API and the API server. We looked at some of the ways that you can manage objects with labels, annotations, and namespaces and how Kubernetes uses some of those constructs to manage its internals as well. Then we spent some time looking closely at Pods, what they're made of, and how we can help Kubernetes get a better understanding of our workloads. It's truly been a pleasure recording this course for you, and I thank you so much for listening and most importantly, learning with me. I hope you enjoyed the course, and join me again soon here, at Pluralsight.






